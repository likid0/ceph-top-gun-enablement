<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Untitled :: Ceph Top Gun Enablement</title>
    <link rel="canonical" href="https://likid0.github.io/ceph-top-gun-enablement/training/ceph/cephfs_intro.html">
    <meta name="generator" content="Antora 3.0.1">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">
          <img src="../../_/img/header_logo_reverse.svg" height="48px" alt="Ceph">
      </a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Get Help</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage" target="_blank">Ceph Documentation</a>
            <a class="navbar-item" href="https://bugzilla.redhat.com/describecomponents.cgi?product=Red%20Hat%20OpenShift%20Container%20Storage" target="_blank">Browse Bugs</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Improve Guides</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/red-hat-storage/ocs-training/issues/new/choose" target="_blank">Open Issue</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">More Infos</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://www.redhat.com/en/blog/channel/red-hat-storage" target="_blank">Our Blog</a>
            <a class="navbar-item" href="https://www.youtube.com/channel/UCoyG8VyvB-XUxQl1mD3T3Gw" target="_blank">Youtube</a>
            <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">OCS Technology</a>
          </div>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="training" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Ceph Top-Gun Enablement</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="opentlc_lab_env.html">Opentlc Lab Env</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RadosGW</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_intro.html">RGW Introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_arch_deep_dive.html">RGW Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ha.html">RGW High Availability</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ssl.html">RGW &amp; Ingress with SSL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_users_quotas.html">RGW Users &amp; Quotas</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_auth.html">RGW Auth &amp; Authz</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_object_versioning.html">RGW S3 Object Versioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_placement_and_storage_classes.html">RGW Placement &amp; Storage Classes</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_life_cycle_management.html">RGW Life Cycle Management</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_policy.html">RGW S3 Bucket Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_introduction.html">RGW Secure Token Service</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_bucket_role_policy.html">RGW Bucket vs Role Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_multisite.html">RGW Multisite Replication</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_cloudsync.html">RGW Object Cloud Transition</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_presignedurl.html">RGW presigned URL</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">CephFS Shared FileSystem</span>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="cephfs_intro.html">CephFS introduction &amp; Deployment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Troubleshooting</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-nearfull-osds.html">Troubleshooting nearfull OSDs</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Stretched</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="rhcs-stretched-deploy.html">Ceph Stretch Mode</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Ceph Top-Gun Enablement</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Ceph Top-Gun Enablement</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Ceph Top-Gun Enablement</a></li>
    <li>CephFS Shared FileSystem</li>
    <li><a href="cephfs_intro.html">CephFS introduction &amp; Deployment</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="file:///antora/training/modules/ceph/pages/cephfs_intro.adoc">Edit this Page</a></div>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<div class="sect1">
<h2 id="_cephfs_introduction"><a class="anchor" href="#_cephfs_introduction"></a>Cephfs Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Ceph File System, or CephFS, is a POSIX-compliant file system built on top of Ceph’s distributed object store, RADOS. CephFS endeavors to provide a state-of-the-art, multi-use, highly available, and performant file store for a variety of applications, including traditional use-cases like shared home directories, HPC scratch space, and distributed workflow shared storage.</p>
</div>
<div class="paragraph">
<p>CephFS achieves these goals through the use of some novel architectural choices. Notably, file metadata is stored in a separate RADOS pool from file data and served via a resizable cluster of Metadata Servers, or MDS, which may scale to support higher throughput metadata workloads. Clients of the file system have direct access to RADOS for reading and writing file data blocks. For this reason, workloads may linearly scale with the size of the underlying RADOS object store; that is, there is no gateway or broker mediating data I/O for clients.</p>
</div>
<div class="paragraph">
<p>Access to data is coordinated through the cluster of MDS which serve as authorities for the state of the distributed metadata cache cooperatively maintained by clients and MDS. Mutations to metadata are aggregated by each MDS into a series of efficient writes to a journal on RADOS; no metadata state is stored locally by the MDS. This model allows for coherent and rapid collaboration between clients within the context of a POSIX file system.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/cephfs-architecture.svg" alt="cephfs" width="740" height="580">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_mds_the_metadata_server"><a class="anchor" href="#_mds_the_metadata_server"></a>MDS the metadata server</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Metadata Server (MDS) manages metadata for CephFS clients. This daemon provides
information that CephFS clients need to access RADOS objects, such as providing file locations
within the file-system tree. MDS manages the directory hierarchy and stores file metadata, such
as the owner, time stamps, and permission modes, in a RADOS cluster. MDS is also responsible for
access caching and managing client caches to maintain cache coherence.</p>
</div>
<div class="paragraph">
<p>While the data for inodes in a Ceph file system is stored in RADOS and accessed by the clients directly, inode metadata and directory information is managed by the Ceph metadata server (MDS). The MDS’s act as mediator for all metadata related activity, storing the resulting information in a separate RADOS pool from the file data.</p>
</div>
<div class="paragraph">
<p>CephFS clients can request that the MDS fetch or change inode metadata on its
behalf, but an MDS can also grant the client capabilities (aka caps) for each
inode</p>
</div>
<div class="paragraph">
<p>Since the cache is distributed, the MDS must take great care to ensure that no client holds capabilities that may conflict with other clients’ capabilities, or operations that it does itself. This allows cephfs clients to rely on much greater cache coherence than a filesystem like NFS, where the client may cache data and metadata beyond the point where it has changed on the server.</p>
</div>
<div class="paragraph">
<p>When a client needs to query/change inode metadata or perform an operation on a directory, it has two options. It can make a request to the MDS directly, or serve the information out of its cache. With CephFS, the latter is only possible if the client has the necessary caps.</p>
</div>
<div class="paragraph">
<p>Clients can send simple requests to the MDS to query or request changes to certain metadata. The replies to these requests may also grant the client a certain set of caps for the inode, allowing it to perform subsequent requests without consulting the MDS.</p>
</div>
<div class="paragraph">
<p>Clients can also request caps directly from the MDS, which is necessary in order to read or write file data.</p>
</div>
<div class="paragraph">
<p>MDS daemons operate in two modes: active and standby. An active MDS manages the metadata
on the CephFS file system. A standby MDS serves as a backup, and switches to the active mode
if the active MDS becomes unresponsive. CephFS shared file systems require an active MDS
service. You should deploy at least one standby MDS in your cluster to ensure high availability.
CephFS clients first contact a MON to authenticate and retrieve the cluster map. Then, the client
queries an active MDS for file metadata. The client uses the metadata to access the objects that
comprise the requested file or directory by communicating directly with the OSDs.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_deploy_cephfs"><a class="anchor" href="#_deploy_cephfs"></a>Deploy CephFS</h2>
<div class="sectionbody">
<div class="paragraph">
<p>One or more MDS daemons are required to use the CephFS file system. These are created automatically if the newer <code>ceph fs volume</code> interface is used to create a new file system.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a CephFS volume, specifying <code>fs_name</code> as the name, and a comma-separated list of host names as the placement:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">$root@ceph-mon01 ~]# ceph fs volume create fs_name --placement=ceph-mon01.example.com,proxy02.example.com</code></pre>
</div>
</div>
</li>
<li>
<p>Confirm that the CephFS volume was created:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[root@ceph-mon01 ~]# ceph fs volume ls</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Sample Output</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">[
    {
        "name": "fs_name"
    }
]</code></pre>
</div>
</div>
</li>
<li>
<p>Create a YAML file called <code>mds.yaml</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">service_type: mds
service_id: fs_name
placement:
  count: 2</code></pre>
</div>
</div>
</li>
<li>
<p>As the root user on the <code>ceph-mon01</code> server, apply the specification to manually deploy the MDS daemons, using the YAML file that you created:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[root@ceph-mon01 ~]# ceph orch apply -i mds.yaml</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Sample Output</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-texinfo hljs" data-lang="texinfo">Scheduled mds.fs_name update...</code></pre>
</div>
</div>
</li>
<li>
<p>List the services that are running on the new installation and verify that the <code>mds.fs_name</code> service is created:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[ceph: root@ceph-mon01 /]# ceph orch ls | grep mds</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Sample Output</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-texinfo hljs" data-lang="texinfo">NAME                       PORTS  RUNNING  REFRESHED  AGE  PLACEMENT
mds.fs_name                           2/2  3m ago     3m   count:2</code></pre>
</div>
</div>
</li>
<li>
<p>View the <code>mds</code> daemon processes that are running:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[ceph: root@ceph-mon01 /]# ceph orch ps | grep mds</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Sample Output</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-texinfo hljs" data-lang="texinfo">NAME                               HOST                    PORTS        STATUS         REFRESHED  AGE  VERSION  IMAGE ID      CONTAINER ID
mds.fs_name.ceph-mon01.vnuima      ceph-mon01.example.com               running (19s)  13s ago    19s  16.2.4   8d91d370c2b8  c91ca8508916
mds.fs_name.proxy02.txydml         proxy02.example.com                  running (17s)  15s ago    17s  16.2.4   8d91d370c2b8  d4c2cd362001</code></pre>
</div>
</div>
</li>
<li>
<p>Verify that <code>mds</code> is available and up:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[root@ceph-mon01 ~]# ceph -s
cluster:
	id:     7d4ee168-d9b9-11eb-bc7e-2cc260754989
	health: HEALTH_WARN
					nodeep-scrub flag(s) set

services:
	mon: 3 daemons, quorum ceph-mon01.example.com,ceph-mon02,ceph-mon03 (age 22m)
	mgr: ceph-mon01.example.com.cntwzr(active, since 22m), standbys: ceph-mon02.pxyuuu
	mds: 1/1 daemons up, 1 standby
	osd: 3 osds: 3 up (since 22m), 3 in (since 13h)
			 flags nodeep-scrub

data:
	volumes: 1/1 healthy
	pools:   4 pools, 129 pgs
	objects: 34 objects, 4.1 MiB
	usage:   25 MiB used, 30 GiB / 30 GiB avail
	pgs:     129 active+clean

[root@ceph-mon01 ~]# ceph fs status
cephfs - 0 clients
======
RANK  STATE              MDS                ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active  cephfs.ceph-node03.ifnlti  Reqs:    0 /s    10     13     12      0
       POOL           TYPE     USED  AVAIL
cephfs.cephfs.meta  metadata  96.0k  9609M
cephfs.cephfs.data    data       0   9609M
     STANDBY MDS
cephfs.proxy01.udpgpo
MDS version: ceph version 16.2.8-85.el8cp (0bdc6db9a80af40dd496b05674a938d406a9f6f5) pacific (stable)

[root@ceph-mon01 ~]# ceph fs ls
name: cephfs, metadata pool: cephfs.cephfs.meta, data pools: [cephfs.cephfs.data ]</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
      <img src="../../_/img/header_logo.svg" alt="Ceph">
  </a>
</footer>
<script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
