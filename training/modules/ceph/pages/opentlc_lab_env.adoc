== Red Hat Ceph Storage Lab Introduction

.Goals
* Provision the lab environment
* Become familiar with the lab environment
* Review server access considerations
* Install Red Hat^(R)^ Ceph^(R)^ Storage using the `cephadm` utility

:numbered:

== Provision Lab Environment

=== Provision Environment

The lab environment consists of 13 virtual servers, one of which is a workstation/bastion host. In this section, you provision the lab environment, which sets up the components required for the lab.

The lab environment is a playground provided with instructions on how to set up some of the scenarios. The lab is a guide and does not provide step-by-step instructions, nor does it provide details on implementing best practices. For topics or scenarios that are not covered in this lab, or commands that do not provide the expected output, refer to Red Hat's documentation.

For example, the lab provides information on how to configure four MONs. A best practice in a production environment is to configure an odd number of MONs, such as three or five. You have the option in this lab to use the `cephadm` command line tool to configure only three MONs.

. Navigate to the link:https://labs.opentlc.com[OPENTLC lab portal^] and log in with your username and password.
+
[NOTE]
If you do not remember your username or password, go to the link:https://www.opentlc.com/pwm[OPENTLC Account Management page^] to obtain a username reminder or reset your password.

. Navigate to *Services -> Catalogs -> All Services -> OPENTLC Cloud Infrastructure Labs*.

. On the left side of the screen, select *Ceph 5 Playground*.

. On the right, click *Order*.

. On the next page, check the box to confirm that you understand the runtime and expiration dates, then click *Submit* on the bottom right to order your environment.

. After a few minutes, check your email account for a message from Red Hat confirming that provisioning started.
+
[WARNING]
====
Do not select *App Control -> Start* after ordering the lab. The lab environment is already building, and selecting *Start* may corrupt it or cause other complications.
====
+
[TIP]
====
You can select *Status* to view the status of your environment.
====

=== Set Up Registry Prerequisites

In this section, you verify that you can access `registry.redhat.io` and retrieve your username and password token.

. Navigate to link:https://access.redhat.com/terms-based-registry/\#/accounts[https://access.redhat.com/terms-based-registry/#/accounts^].

. Select your account name, and note your username and password from the *Token Information* tab:
+
image::images/Registry-Login-01.png[]

== Explore Lab Environment and Server List

Due to the nature of OpenStack^(R)^ and storage, the lab environment for this course is larger and more complex than most. Elements such as Red Hat Ceph Storage, high availability, shared storage, and others require several servers and enhanced networking. The Ravello lab environment that is provisioned for each student provides all of these requirements.

[NOTE]
The lab environment is cloud-based, so you can access it over the Internet from anywhere. However, do not expect the performance to match a dedicated environment.
+
[NOTE]
====
You are able to access any system in your lab environment by using the Novello virtual console, the link to which is provided in the email message you received with information about accessing your lab environment.
====
+
[WARNING]
====
While you have root access to all of the systems, including the workstation node, and you are able to make any configuration changes necessary, do not enable root login from SSH to the workstation node. The default password is not secure and your machine may be compromised and subsequently deleted without warning.
====

. Verify that your environment is ready for use by connecting to your administration host through SSH using the details provided in the external provisioning email, which includes your username and password:
+
.Example Command
[source,sh]
-----
# ssh username@workstation-67a5.dynamic.opentlc.com
-----
* It may take up to 30 minutes for your lab environment to be ready. If you cannot connect to your administration host, continue to try periodically until your connection succeeds.
+
[NOTE]
====
* If you are using Windows, use a terminal program such as PuTTY to connect instead of the `ssh` command shown.

* To learn more about SSH and keys, read link:https://www.opentlc.com/ssh.html[Setting Up an SSH Key Pair^].
====

== Prepare for Red Hat Ceph Storage Installation

=== Prepare the workstation

Once you have logged in via SSH to your opentlc workstation, and used sudo to
scalate into
root,  we are going to
run a playbook to get the pre-reqs for the labs in place.

Clone the cephadm automation repo, you will need to authenticate to get access,
in this example we are using a private key to authenticate against github.

----
GIT_SSH_COMMAND="ssh -i key.rsa" git clone git@github.com:likid0/cephadmdeploy.git
----

We then run the client/workstation playbook.

----
# ansible-playbook cephadmdeploy/ansible-cephadm-deploy/prepare-client.yaml -i workstation,
----

=== Prepare Ceph nodes for deployment

Once the client playbook finishes we can start preparing our ceph clusters, we
have 8 servers available, four per ceph cluster.

*Ceph Cluster1:* ceph-node01,ceph-node02,ceph-node03,proxy01

*Ceph Cluster2:* ceph-mon01,ceph-mon02,ceph-mon03,proxy02

The previous client playbook created 2 inventory files for us.

----
# cat /root/cluster1
ceph-node01.example.com
ceph-node02.example.com
ceph-node03.example.com
proxy01.example.com

[admin]
ceph-node01.example.com

[osds]
ceph-node01.example.com
ceph-node02.example.com
ceph-node03.example.com

[mgmt]
proxy01.example.com

[client]
workstation.example.com
----

----
# cat /root/cluster2
ceph-mon01.example.com
ceph-mon02.example.com
ceph-mon03.example.com
proxy02.example.com

[admin]
ceph-mon01.example.com

[osds]
ceph-mon01.example.com
ceph-mon02.example.com
ceph-mon03.example.com

[mgmt]
proxy02.example.com

[client]
workstation.example.com
----

To get all the pre-requisites ready to deploy our ceph clusters we can run the
`cephadmdeploy/ansible-cephadm-deploy/deploy-cephadm.yaml` for each of our
clusters.

[NOTE]
====
This playbook doesn't deploy ceph it just prepares the pre-reqs needed, repos,
dns name resolution, etc
====

For that first we need to configure a group_vars file

----
cat << EOF > cephadmdeploy/ansible-cephadm-deploy/group_vars/all.yaml
update_cluster_os: true
dedicated_observability: true
reg_password: 'REG_PASS'
reg_username: email@email.com
rhcs_subscription_username: email@redhat.com
rhcs_subscription_password: 'SUB_PASS'
hosts_add_ansible_managed_hosts: false
dnsmasq_upstream_servers_ip: 150.239.16.12
EOF
----

[NOTE]
====
If `update_cluster_os: true` is set to true a full OS upgrade will take place
to the latest RHEL 8 minor version.
====

For each cluster we run:

----
# ansible-playbook -i /root/cluster1 cephadmdeploy/ansible-cephadm-deploy/deploy-cephadm.yaml
----

Once the playbook finishes you are ready to start the ceph deployment, if
needed several scripts have been created in our admin host
`ceph-node01.example.com`

----
# ssh ceph-node01.example.com ls *.sh
cephadm-ansible-run.sh   <--- runs Cephadm-ansible preflight playbook
cluster-install.sh       <--- Deploys the cephcluster with cephadm
cluster-postinstall.sh   <--- Configures Ceph post deploy
cluster-wait-until-installed.sh  <--- waits until ceph is healthy
zap-disks.sh <--- Zaps Disks
----

The cluster-install.sh script is using for the cephadm bootstrap a spec file, you can edit and modify
this file to your needs:

----
# ls -l /root/cluster-spec.yaml
-rw-r--r-- 1 root root 1240 Dec 27 16:13 /root/cluster-spec.yaml
----

For a full automated deployment we can run:

----
# bash cephadm-ansible-run.sh && bash zap-disks.sh && bash cluster-install.sh &&
bash cluster-postinstall.sh
----
