= RadosGW & Ingress SSL 

:toc:
:toclevels: 3
:icons: font
:source-highlighter: pygments
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:


== Introduction

The first ingredient we need to configure SSL endpoint is the "SSL Certificate" itself, which must be obtained through an official certification authority, or CA. It’s the CA’s responsibility to confirm the certificate’s identity, as well as assert its authenticity. For demonstration purposes, we’ll use a self-sign certificate. Acquiring an SSL certificate through an authorised CA is recommended for a production environment.

== Configuring SSL for the RGW daemon

To configure SSL in our RGW daemon, we will use an RGW spec definition.

[NOTE]
====
Some parameters, such as the network to be used by RGW instances or the SSL certificate content, can only be defined by using the service specification file.
====


We have pre-created a self-signed certificate that you can use as your RGW
instance certificate. It's available in the admin node of the cluster, for
example, `ceph-node01.example.com` in the `/root/certificates` folder

----
# ls -l /root/certificates/
total 8
-rw-r--r-- 1 root root 34 Jan  2 06:24 certificate.key
-rw-r--r-- 1 root root 34 Jan  2 06:24 certificate.pem
----

[TIP]
====
If any modifications are needed to the certificates, like a different SAN or just
creating a new certificate, the CA cert/key is available on the `workstation.example.com`
machine on the same path `/root/certificates/`
====

The Certificate has a SAN for ceph-node02, so we are going to configure the RGW
service to run on `node ceph-node02.example.com`

----
# openssl x509 -in /root/certificates/certificate.pem -noout -text | grep -A1 'Subject Alternative Name'
            X509v3 Subject Alternative Name:
                DNS:example.com, DNS:s3zone1.example.com, DNS:s3zone2.example.com, DNS:s3.example.com, DNS:ceph-node02.example.com
----

We are going to add a label called `rgws` to host `ceph-node02.example.com` so
we later can use the label in the RGW spec placement section.

----
# ceph orch host label add ceph-node02 rgws
Added label rgws to host ceph-node02

# ceph orch host ls
HOST        ADDR           LABELS              STATUS
ceph-node01  192.168.56.64  _admin osd mon mgr
ceph-node02  192.168.56.65  osd mon rgw rgws
ceph-node03  192.168.56.66  osd mds mon
proxy01     192.168.56.25  mgmt
4 hosts in cluster
----

Here is a cephadm sample spec file, where we create a new RGW service with SSL
using port 8443, for placement we are using labels, it will select a host with
the label rgws set, we also need to copy and paste the certificate and key into the
spec file.

----
$ cat <<EOF >> /root/rgw-ssl.spec
service_type: rgw
service_id: objectgw
service_name: rgw.objectgw
placement:
  count: 1
  label: "rgws"
spec:
  rgw_realm: default
  rgw_zone: default
  ssl: true
  rgw_frontend_port: 8443
  rgw_frontend_type: beast
  rgw_frontend_ssl_certificate: |
     -----BEGIN CERTIFICATE-----
$( cat /root/certificates/certificate.pem | grep -v CERTIFICATE | awk '{$1="     "$1}1' )
     -----END CERTIFICATE-----
     -----BEGIN RSA PRIVATE KEY-----
$( cat /root/certificates/certificate.key | grep -v PRIVATE | awk '{$1="     "$1}1' )
     -----END RSA PRIVATE KEY-----
EOF
----

Once we have the spec file ready we can apply the configuration

----
# ceph orch apply -i /root/rgw-ssl.spec
Scheduled rgw.objectgw update...
# ceph orch ps | grep rgw
rgw.objectgw.ceph-node02.xqvwoe  ceph-node02  *:8443       running (18m)     8m ago  18m    60.0M        -  16.2.8-85.el8cp  b2c997ff1898  7435e5359df8
----

Quick test with curl to check that we can access the RGW endpoint using
HTTPS/SSL.

----
# curl https://ceph-node02.example.com:8443
<?xml version="1.0" encoding="UTF-8"?><ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><Owner><ID>anonymous</ID><DisplayName></DisplayName></Owner><Buckets></Buckets></ListAllMyBucketsResult>
----

== Configuring SSL for the Ingress Service

When using the cephadm Ingress service can configure the SSL termination at
the HAproxy level. 



When doing the SSL termination at the HAproxy/Load Balancer level, we need to
disable SSL in the RGW daemons. In the previous exercise, we enabled SSL on our
RGW service, so the first step will be to disable SSL from our RGW service.

We are going to continue using RGW specification files. we are modifying the
following things in our spec file:

- count: 2 , we need at least 2 RGW daemons running for the Ingress service to
  work
- label: rgw , we are setting the label for placement to rgw instead of rgws
- ssl: false, we disable ssl, this is the default, but just leaving there for
  clarity
- rgw_frontend_port: using port 8080 instead of 8043

----
# cat <<EOF >> /root/rgw-nossl.spec
service_type: rgw
service_id: objectgw
service_name: rgw.objectgw
placement:
  count: 2
  label: "rgw"
spec:
  rgw_realm: default
  rgw_zone: default
  ssl: false
  rgw_frontend_port: 8080
  rgw_frontend_type: beast
EOF
----

We add the rgw label to node proxy01, so the second instance of our RGW service
gets deployed in node proxy01

----
# ceph orch host label add proxy01 rgw
Added label rgw to host proxy01
----

We now apply the spec file, so the configuration changes take place.

----
# ceph orch apply -i /root/rgw-nossl.spec
Scheduled rgw.objectgw update...

# ceph orch ps | grep rgw
rgw.objectgw.ceph-node02.seoxpk  ceph-node02  *:8080       running (5m)     2m ago   5m    54.1M        -  16.2.8-85.el8cp  b2c997ff1898  be29114c01ce
rgw.objectgw.proxy01.dcsrvq     proxy01     *:8080       running (2m)     2m ago   2m    78.1M        -  16.2.8-85.el8cp  b2c997ff1898  73385e654861
----

Now lets create the ingress service spec file, we use port 443 and IP
192.168.56.100 for the frontend, haproxy and keepalive will run on hosts
ceph-node02 & ceph-node03

----
 cat << EOF >  rgw-ingress.yaml
service_type: ingress
service_id: rgw.default
placement:
  hosts:
    - ceph-node02
    - ceph-node03
spec:
  backend_service: rgw.objectgw
  virtual_ip: 192.168.56.100/24
  frontend_port: 443
  monitor_port:  1967
  ssl_cert: |
     -----BEGIN CERTIFICATE-----
$( cat /root/certificates/certificate.pem | grep -v CERTIFICATE | awk '{$1="     "$1}1' )
     -----END CERTIFICATE-----
     -----BEGIN RSA PRIVATE KEY-----
$( cat /root/certificates/certificate.key | grep -v PRIVATE | awk '{$1="     "$1}1' )
     -----END RSA PRIVATE KEY-----
EOF
----

We can now apply the spec file

----
# ceph orch apply -i rgw-ingress.yaml
Scheduled ingress.rgw.default update...
----

[NOTE]
====
It could be that you get one haproxy instance in a failed state. This is
because of resource contention in our lab nodes. The problem gets fixed with a
restart of the haproxy daemon

----
# ceph orch ps  | grep rgw.default
haproxy.rgw.default.ceph-node02.kueked     ceph-node02  *:443,1967   error             5m ago   6m        -        -  <unknown>        <unknown>     <unknown>
haproxy.rgw.default.ceph-node03.uwzado     ceph-node03  *:443,1967   running (5m)      4m ago   6m    9239k        -  2.2.19-7ea3822   6b6ff8a83cd7  eeec4109ddd9
keepalived.rgw.default.ceph-node02.qwacfu  ceph-node02               running (5m)      5m ago   5m    18.0M        -  2.1.5            f68c62a66d49  12a59ba5f81f
keepalived.rgw.default.ceph-node03.exqenp  ceph-node03               running (5m)      4m ago   5m    18.0M        -  2.1.5            f68c62a66d49  55a00e06bf28
# ceph orch daemon restart haproxy.rgw.default.ceph-node02.kueked
Scheduled to restart haproxy.rgw.default.ceph-node02.kueked on host 'ceph-node02'
# ceph orch ps  | grep rgw.default
haproxy.rgw.default.ceph-node02.kueked     ceph-node02  *:443,1967   running (43s)    38s ago  11m    8732k        -  2.2.19-7ea3822   6b6ff8a83cd7  d90de802e811
haproxy.rgw.default.ceph-node03.uwzado     ceph-node03  *:443,1967   running (10m)    10m ago  11m    9239k        -  2.2.19-7ea3822   6b6ff8a83cd7  eeec4109ddd9
keepalived.rgw.default.ceph-node02.qwacfu  ceph-node02               running (10m)    38s ago  10m    18.0M        -  2.1.5            f68c62a66d49  12a59ba5f81f
keepalived.rgw.default.ceph-node03.exqenp  ceph-node03               running (11m)    10m ago  11m    18.0M        -  2.1.5            f68c62a66d49  55a00e06bf28
----
====


We can check that SSL is being terminated at the haproxy level on the config
file the frontend bind sectopm has a ssl cert entry pointing to our certificate
`/var/lib/haproxy/haproxy.pem`

[NOTE]
====
Comunications from HAproxy to the RGW instances is in clear text through HTTP.
====

----
# cat /var/lib/ceph/6e282f42-8a83-11ed-909f-2cc260754989/haproxy.rgw.default.ceph-node02.kueked/haproxy/haproxy.cfg
# This file is generated by cephadm.
...

frontend frontend
    bind 192.168.56.100:443 ssl crt /var/lib/haproxy/haproxy.pem
    default_backend backend

backend backend
    option forwardfor
    balance static-rr
    option httpchk HEAD / HTTP/1.0
    server rgw.objectgw.ceph-node02.seoxpk 192.168.56.65:8080 check weight 100
    server rgw.objectgw.proxy01.dcsrvq 192.168.56.25:8080 check weight 100
----

Let's test with curl command against the VIP that has an FQDN of
`https://s3zone1.example.com`

----
# curl https://s3zone1.example.com
<?xml version="1.0" encoding="UTF-8"?><ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><Owner><ID>anonymous</ID><DisplayName></DisplayName></Owner><Buckets></Buckets></ListAllMyBucketsResult>
----

