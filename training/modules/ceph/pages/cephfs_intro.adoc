== Cephfs Introduction

The Ceph File System, or CephFS, is a POSIX-compliant file system built on top of Ceph’s distributed object store, RADOS. CephFS endeavors to provide a state-of-the-art, multi-use, highly available, and performant file store for a variety of applications, including traditional use-cases like shared home directories, HPC scratch space, and distributed workflow shared storage.

CephFS achieves these goals through the use of some novel architectural choices. Notably, file metadata is stored in a separate RADOS pool from file data and served via a resizable cluster of Metadata Servers, or MDS, which may scale to support higher throughput metadata workloads. Clients of the file system have direct access to RADOS for reading and writing file data blocks. For this reason, workloads may linearly scale with the size of the underlying RADOS object store; that is, there is no gateway or broker mediating data I/O for clients.

Access to data is coordinated through the cluster of MDS which serve as authorities for the state of the distributed metadata cache cooperatively maintained by clients and MDS. Mutations to metadata are aggregated by each MDS into a series of efficient writes to a journal on RADOS; no metadata state is stored locally by the MDS. This model allows for coherent and rapid collaboration between clients within the context of a POSIX file system.

image::cephfs-architecture.svg[cephfs,740,580]



== MDS the metadata server

The Metadata Server (MDS) manages metadata for CephFS clients. This daemon provides
information that CephFS clients need to access RADOS objects, such as providing file locations
within the file-system tree. MDS manages the directory hierarchy and stores file metadata, such
as the owner, time stamps, and permission modes, in a RADOS cluster. MDS is also responsible for
access caching and managing client caches to maintain cache coherence.

While the data for inodes in a Ceph file system is stored in RADOS and accessed by the clients directly, inode metadata and directory information is managed by the Ceph metadata server (MDS). The MDS’s act as mediator for all metadata related activity, storing the resulting information in a separate RADOS pool from the file data.

CephFS clients can request that the MDS fetch or change inode metadata on its
behalf, but an MDS can also grant the client capabilities (aka caps) for each
inode

Since the cache is distributed, the MDS must take great care to ensure that no client holds capabilities that may conflict with other clients’ capabilities, or operations that it does itself. This allows cephfs clients to rely on much greater cache coherence than a filesystem like NFS, where the client may cache data and metadata beyond the point where it has changed on the server.

When a client needs to query/change inode metadata or perform an operation on a directory, it has two options. It can make a request to the MDS directly, or serve the information out of its cache. With CephFS, the latter is only possible if the client has the necessary caps.

Clients can send simple requests to the MDS to query or request changes to certain metadata. The replies to these requests may also grant the client a certain set of caps for the inode, allowing it to perform subsequent requests without consulting the MDS.

Clients can also request caps directly from the MDS, which is necessary in order to read or write file data.


MDS daemons operate in two modes: active and standby. An active MDS manages the metadata
on the CephFS file system. A standby MDS serves as a backup, and switches to the active mode
if the active MDS becomes unresponsive. CephFS shared file systems require an active MDS
service. You should deploy at least one standby MDS in your cluster to ensure high availability.
CephFS clients first contact a MON to authenticate and retrieve the cluster map. Then, the client
queries an active MDS for file metadata. The client uses the metadata to access the objects that
comprise the requested file or directory by communicating directly with the OSDs.


== Deploy CephFS

One or more MDS daemons are required to use the CephFS file system. These are created automatically if the newer `ceph fs volume` interface is used to create a new file system.

. Create a CephFS volume, specifying `fs_name` as the name, and a comma-separated list of host names as the placement:
+
[source,sh]
----
$root@ceph-mon01 ~]# ceph fs volume create fs_name --placement=ceph-mon01.example.com,proxy02.example.com
----

. Confirm that the CephFS volume was created:
+
[source,sh]
----
[root@ceph-mon01 ~]# ceph fs volume ls
----
+
.Sample Output
[source,json]
----
[
    {
        "name": "fs_name"
    }
]
----

. Create a YAML file called `mds.yaml`:
+
[source,yaml]
----
service_type: mds
service_id: fs_name
placement:
  count: 2
----

. As the root user on the `ceph-mon01` server, apply the specification to manually deploy the MDS daemons, using the YAML file that you created:
+
[source,sh]
----
[root@ceph-mon01 ~]# ceph orch apply -i mds.yaml
----
+
.Sample Output
[source,texinfo]
----
Scheduled mds.fs_name update...
----

. List the services that are running on the new installation and verify that the `mds.fs_name` service is created:
+
[source,sh]
-----
[ceph: root@ceph-mon01 /]# ceph orch ls | grep mds
-----
+
.Sample Output
[source,texinfo]
-----
NAME                       PORTS  RUNNING  REFRESHED  AGE  PLACEMENT
mds.fs_name                           2/2  3m ago     3m   count:2
-----

. View the `mds` daemon processes that are running:
+
[source,sh]
-----
[ceph: root@ceph-mon01 /]# ceph orch ps | grep mds
-----
+
.Sample Output
[source,texinfo]
-----
NAME                               HOST                    PORTS        STATUS         REFRESHED  AGE  VERSION  IMAGE ID      CONTAINER ID
mds.fs_name.ceph-mon01.vnuima      ceph-mon01.example.com               running (19s)  13s ago    19s  16.2.4   8d91d370c2b8  c91ca8508916
mds.fs_name.proxy02.txydml         proxy02.example.com                  running (17s)  15s ago    17s  16.2.4   8d91d370c2b8  d4c2cd362001
-----

. Verify that `mds` is available and up:
+
[source,sh]
-----
[root@ceph-mon01 ~]# ceph -s
cluster:
	id:     7d4ee168-d9b9-11eb-bc7e-2cc260754989
	health: HEALTH_WARN
					nodeep-scrub flag(s) set

services:
	mon: 3 daemons, quorum ceph-mon01.example.com,ceph-mon02,ceph-mon03 (age 22m)
	mgr: ceph-mon01.example.com.cntwzr(active, since 22m), standbys: ceph-mon02.pxyuuu
	mds: 1/1 daemons up, 1 standby
	osd: 3 osds: 3 up (since 22m), 3 in (since 13h)
			 flags nodeep-scrub

data:
	volumes: 1/1 healthy
	pools:   4 pools, 129 pgs
	objects: 34 objects, 4.1 MiB
	usage:   25 MiB used, 30 GiB / 30 GiB avail
	pgs:     129 active+clean

[root@ceph-mon01 ~]# ceph fs status
cephfs - 0 clients
======
RANK  STATE              MDS                ACTIVITY     DNS    INOS   DIRS   CAPS  
 0    active  cephfs.ceph-node03.ifnlti  Reqs:    0 /s    10     13     12      0   
       POOL           TYPE     USED  AVAIL  
cephfs.cephfs.meta  metadata  96.0k  9609M  
cephfs.cephfs.data    data       0   9609M  
     STANDBY MDS       
cephfs.proxy01.udpgpo  
MDS version: ceph version 16.2.8-85.el8cp (0bdc6db9a80af40dd496b05674a938d406a9f6f5) pacific (stable)

[root@ceph-mon01 ~]# ceph fs ls
name: cephfs, metadata pool: cephfs.cephfs.meta, data pools: [cephfs.cephfs.data ]
-----

