= Cephfs Introduction

//++++
//<link rel="stylesheet"  href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/3.1.0/css/font-awesome.min.css">
//++++
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:sectnums:
:sectnumlevels: 6
:toc: left
:toclevels: 4


== Introduction

The Ceph File System, or CephFS, is a POSIX-compliant file system built on top of Ceph’s distributed object store, RADOS. CephFS endeavors to provide a state-of-the-art, multi-use, highly available, and performant file store for a variety of applications, including traditional use-cases like shared home directories, HPC scratch space, and distributed workflow shared storage.

CephFS achieves these goals through the use of some novel architectural choices. Notably, file metadata is stored in a separate RADOS pool from file data and served via a resizable cluster of Metadata Servers, or MDS, which may scale to support higher throughput metadata workloads. Clients of the file system have direct access to RADOS for reading and writing file data blocks. For this reason, workloads may linearly scale with the size of the underlying RADOS object store; that is, there is no gateway or broker mediating data I/O for clients.

Access to data is coordinated through the cluster of MDS which serve as authorities for the state of the distributed metadata cache cooperatively maintained by clients and MDS. Mutations to metadata are aggregated by each MDS into a series of efficient writes to a journal on RADOS; no metadata state is stored locally by the MDS. This model allows for coherent and rapid collaboration between clients within the context of a POSIX file system.

image::cephfs-architecture.svg[cephfs,740,580]



== MDS the metadata server

The Metadata Server (MDS) manages metadata for CephFS clients. This daemon provides
information that CephFS clients need to access RADOS objects, such as providing file locations
within the file-system tree. MDS manages the directory hierarchy and stores file metadata, such
as the owner, time stamps, and permission modes, in a RADOS cluster. MDS is also responsible for
access caching and managing client caches to maintain cache coherence.

While the data for inodes in a Ceph file system is stored in RADOS and accessed by the clients directly, inode metadata and directory information is managed by the Ceph metadata server (MDS). The MDS’s act as mediator for all metadata related activity, storing the resulting information in a separate RADOS pool from the file data.

CephFS clients can request that the MDS fetch or change inode metadata on its
behalf, but an MDS can also grant the client capabilities (aka caps) for each
inode

Since the cache is distributed, the MDS must take great care to ensure that no client holds capabilities that may conflict with other clients’ capabilities, or operations that it does itself. This allows cephfs clients to rely on much greater cache coherence than a filesystem like NFS, where the client may cache data and metadata beyond the point where it has changed on the server.

When a client needs to query/change inode metadata or perform an operation on a directory, it has two options. It can make a request to the MDS directly, or serve the information out of its cache. With CephFS, the latter is only possible if the client has the necessary caps.

Clients can send simple requests to the MDS to query or request changes to certain metadata. The replies to these requests may also grant the client a certain set of caps for the inode, allowing it to perform subsequent requests without consulting the MDS.

Clients can also request caps directly from the MDS, which is necessary in order to read or write file data.


MDS daemons operate in two modes: active and standby. An active MDS manages the metadata
on the CephFS file system. A standby MDS serves as a backup, and switches to the active mode
if the active MDS becomes unresponsive. CephFS shared file systems require an active MDS
service. You should deploy at least one standby MDS in your cluster to ensure high availability.
CephFS clients first contact a MON to authenticate and retrieve the cluster map. Then, the client
queries an active MDS for file metadata. The client uses the metadata to access the objects that
comprise the requested file or directory by communicating directly with the OSDs.


== Deploy CephFS

One or more MDS daemons are required to use the CephFS file system. These are created automatically if the newer `ceph fs volume` interface is used to create a new file system.

[TIP]
====
For more control over the deploying process, you can do a 3 step proccess:
- Manually create the pools that are associated to the CephFS
- Start the MDS service on the hosts
- Create the CephFS file system.
In our example we are going to do the 1 command process to get everything ready
with help of the `ceph fs volume` interface
====

. Create a CephFS volume, specifying `fs_name` as the name, and a comma-separated list of host names as the placement:
+
[source,sh]
----
# ceph fs volume create fs_name --placement=ceph-node01,proxy01
Volume created successfully (no MDS daemons created)
----

. Confirm that the CephFS volume was created:
+
[source,sh]
----
# ceph fs volume ls
[
    {
        "name": "fs_name"
    }
]

# ceph osd lspools | grep cephfs
6 cephfs.fs_name.meta
7 cephfs.fs_name.data
----

. Create a YAML file called `mds.spec.yaml`:
+
[source,yaml]
----
cat <<EOF > mds.spec.yaml
service_type: mds
service_id: fs_name
placement:
  count: 2
  hosts:
  - ceph-node01
  - proxy01
EOF
----

. As the root user on the `ceph-node01` server, apply the specification to manually deploy the MDS daemons, using the YAML file that you created:
+
[source,sh]
----
# ceph orch apply -i mds.spec.yaml
Scheduled mds.fs_name update...
----

. List the services that are running on the new installation and verify that the `mds.fs_name` service is created:
+
[source,sh]
-----
# ceph orch ls | grep mds
NAME                       PORTS  RUNNING  REFRESHED  AGE  PLACEMENT
mds.fs_name                           2/2  3m ago     3m   count:2
-----

. View the `mds` daemon processes that are running:
+
[source,sh]
-----
# ceph orch ps | grep mds
NAME                               HOST                    PORTS        STATUS         REFRESHED  AGE  VERSION  IMAGE ID      CONTAINER ID
mds.fs_name.ceph-node01.vnuima      ceph-node01               running (19s)  13s ago    19s  16.2.4   8d91d370c2b8  c91ca8508916
mds.fs_name.proxy01.txydml         proxy01                running (17s)  15s ago    17s  16.2.4   8d91d370c2b8  d4c2cd362001
-----

. Verify that `mds` is available and up:
+
[source,sh]
-----
# ceph -s
cluster:
	id:     7d4ee168-d9b9-11eb-bc7e-2cc260754989
	health: HEALTH_WARN
					nodeep-scrub flag(s) set

services:
	mon: 3 daemons, quorum ceph-node01.example.com,ceph-node02,ceph-node03 (age 22m)
	mgr: ceph-node01.example.com.cntwzr(active, since 22m), standbys: ceph-node02.pxyuuu
	mds: 1/1 daemons up, 1 standby
	osd: 3 osds: 3 up (since 22m), 3 in (since 13h)
			 flags nodeep-scrub

data:
	volumes: 1/1 healthy
	pools:   4 pools, 129 pgs
	objects: 34 objects, 4.1 MiB
	usage:   25 MiB used, 30 GiB / 30 GiB avail
	pgs:     129 active+clean

# ceph fs status
cephfs - 0 clients
======
RANK  STATE              MDS                ACTIVITY     DNS    INOS   DIRS   CAPS  
 0    active  cephfs.ceph-node03.ifnlti  Reqs:    0 /s    10     13     12      0   
       POOL           TYPE     USED  AVAIL  
cephfs.cephfs.meta  metadata  96.0k  9609M  
cephfs.cephfs.data    data       0   9609M  
     STANDBY MDS       
cephfs.proxy01.udpgpo  
MDS version: ceph version 16.2.8-85.el8cp (0bdc6db9a80af40dd496b05674a938d406a9f6f5) pacific (stable)

# ceph fs ls
name: cephfs, metadata pool: cephfs.cephfs.meta, data pools: [cephfs.cephfs.data ]

# ceph mds stat
fs_name:1 {0=fs_name.ceph-node01.gojgii=up:active} 1 up:standby
-----


== Cephfs Clients, how to mount Cephfs?

You can mount CephFS file systems with:

- The kernel client
- The FUSE client

The kernel driver is developed separately from the core ceph code, and as such
it sometimes differs from the FUSE driver in feature implementation. The
following details the implementation status of various CephFS features in the
kernel driver. For more information on the current kernel client supporte
features check this upstream https://docs.ceph.com/en/quincy/cephfs/kernel-features/[link]

[WARN]
====
The kernel client requires a Linux which is available starting with RHEL 8.
====

== Pre-Reqs for Mounting a Cephfs Filesytem

In this Lab we are going to mount the Cephfs Filesystems from the
`workstation.example.com` node, so we need to run the following pre-req
commands for this server.

Install the ceph-common package. For the FUSE client, also install the ceph-fuse package.

----
workstation# dnf install ceph-common ceph-fuse -y 
----

A minimal Ceph configuration file is needed stored in `/etc/ceph/ceph.conf` by
default. We are going to copy the conf file from our admin ceph node, in this
case `ceph-node01`

----
workstation# scp -p ceph-node01:/etc/ceph/ceph.conf /etc/ceph/ceph.conf
----

Authorize the client to access the CephFS file system. In this example we give
the user `0` Read/

----
ceph-node01# ceph fs authorize fs_name client.0 / rw
[client.0]
	key = AQBAaLVjpZu3NhAAlu30WoNaBn08obWB6T5IEQ==
----

Get the new authorization key with the ceph auth get command and copy it to the
`/etc/ceph` folder

----
ceph-node01# ceph auth get client.0 -o /etc/ceph/ceph.client.0.keyring
ceph-node01# cat /etc/ceph/ceph.client.0.keyring
[client.0]
	key = AQBAaLVjpZu3NhAAlu30WoNaBn08obWB6T5IEQ==
	caps mds = "allow rw fsname=fs_name"
	caps mon = "allow r fsname=fs_name"
	caps osd = "allow rw tag cephfs data=fs_name"
----

From `workstation` run scp to copy the keyring in `ceph-node01`.

----
# scp -p ceph-node01:/etc/ceph/ceph.client.0.keyring /etc/ceph/ceph.client.0.keyring
Warning: Permanently added 'ceph-node01,172.16.7.61' (ECDSA) to the list of known hosts.
ceph.client.0.keyring                                                                                        100%  181   303.2KB/s   00:00
----

Check that we can now query our ceph cluster from node `workstation`, we
specify the id of the user we created called `0`

----
# ceph --id 0 -s
  cluster:
    id:     a8292be8-8c21-11ed-b76b-2cc26078e4ef
...
----

Now we have the fullfiled the pre-reqs we are ready to mount the CephFS Filesystem


=== FUSE client

With the Fuse Client installed we can simply run the command `ceph-fuse` and
the id of our user

----
# ceph-fuse --id 0 /mnt
ceph-fuse[31811]: starting ceph client
2023-01-04T07:08:45.962-0500 7fb89f204380 -1 init, newargv = 0x55ae882073d0 newargc=15
ceph-fuse[31811]: starting fuse
# df -h | grep mnt
ceph-fuse       9.5G     0  9.5G   0% /mnt
----

ceph-fuse mounts by default the / root of the filesystem, if we want to mount
at a specific tree level we can use the -r paramter, for example:

----
# mkdir /mnt/dir0
# umount /mnt
# ceph-fuse --id 0 -r /dir0 /mnt
fuse[31883]: starting ceph client
2023-01-04T07:11:29.032-0500 7fca021b4380 -1 init, newargv = 0x55f5dd7908d0 newargc=15
ceph-fuse[31883]: starting fuse
# ls -l /mnt
total 0
----

Lets add a file and check the pool df status, to verify that files are being
stored in the fs_name filesystem, still from our workstation:

----
dd if=/dev/zero of=/mnt/100MB bs=4k iflag=fullblock,count_bytes count=100M
# ceph --id 0 fs status
Error EACCES: access denied: does your client key have mgr caps? See http://docs.ceph.com/en/latest/mgr/administrator/#client-authentication
----

Ah ok, so we are getting an error when triying to get information about the
filesystem status with the persmisions we have assigned to user with id 0, we
don't have the needed permisions on our user, let's copy the admin keyring and
use it:

----
# scp -p ceph-node01:/etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring
Warning: Permanently added 'ceph-node01,172.16.7.61' (ECDSA) to the list of known hosts.
ceph.client.admin.keyring                                                                                                 100%  151   232.2KB/s   00:00
ceph fs status
fs_name - 1 clients
=======
RANK  STATE              MDS                 ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active  fs_name.ceph-node01.gojgii  Reqs:    0 /s    13     16     13      4
        POOL           TYPE     USED  AVAIL
cephfs.fs_name.meta  metadata   200k  9606M
cephfs.fs_name.data    data     256M  9606M
       STANDBY MDS
fs_name.ceph-node02.mvtdpg
MDS version: ceph version 16.2.8-85.el8cp (0bdc6db9a80af40dd496b05674a938d406a9f6f5) pacific (stable)
----

If we want to auto mount the FS on boot, we need to add it to /etc/fstab with the following format:
----
# umount /mnt
# echo 'ceph-node01:6789,ceph-node02:6789,ceph-node03:6789 /mnt fuse.ceph ceph.id=0,ceph.client_mountpoint=/dir0,_netdev 0 0' >> /etc/fstab
# mount /mnt
----

Let's leave the FS umounted to try out the kernel client on the next section

----
# umount /mnt
----

[TIP]
====
When using the FUSE client as a non-root user, add user_allow_other in the /etc/
fuse.conf configuration file.
====

=== Kernel Client

Using the same user with id 0, we can go straigth ahead and mount our Cephfs FS
with the kernel client:

----
# cat /etc/ceph/ceph.client.0.keyring  | grep key
	key = AQBAaLVjpZu3NhAAlu30WoNaBn08obWB6T5IEQ==
# mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/dir0 /mnt -o name=0,secret="AQBAaLVjpZu3NhAAlu30WoNaBn08obWB6T5IEQ=="
# ls -l /mnt
total 204800
-rw-r--r--. 1 root root 104857600 Jan  4 07:26 100MB
----

If we want to auto mount the FS on boot, we need to add it to /etc/fstab with the following format:

----
# cat /etc/ceph/ceph.client.0.keyring | grep key | awk '{print $NF}' > /etc/ceph/ceph.client.0.kernel.keyring
# echo 'ceph-node01.example.com,ceph-node02.example.com:/dir0 /mnt ceph name=0,secretfile=/etc/ceph/ceph.client.0.kernel.keyring,_netdev 0 0' > /etc/fstab 
# mount /mnt
----

[TIP]
====
With the kernel client to be able to use the mount `secretfile` option we need to
have the ceph-common packages installed
====

== Some Basic Client Capabilities

Use Ceph authentication capabilities to restrict your file system clients to the lowest possible level of authority needed.

- *Path Restriction*

To restrict clients to only mount and work within a certain directory, use path-based MDS authentication capabilities.

----
ceph fs authorize <fs_name> client.<client_id> <path-in-cephfs> rw
----

- *Free Space Reporting*

By default, when a client is mounting a sub-directory, the used space (df) will be calculated from the quota on that sub-directory, rather than reporting the overall amount of space used on the cluster.

If you would like the client to report the overall usage of the file system, and not just the quota usage on the sub-directory mounted, then set the following config option on the client:

----
client quota df = false
----

- *Layout & Quota Restriction*

To set layouts or quotas, clients require the ‘p’ flag in addition to ‘rw’. This restricts all the attributes that are set by special extended attributes with a “ceph.” prefix, as well as restricting other means of setting these fields (such as openc operations with layouts).

----
client.0
    key: AQAz7EVWygILFRAAdIcuJ12opU/JKyfFmxhuaw==
    caps: [mds] allow rwp
    caps: [mon] allow r
    caps: [osd] allow rw tag cephfs data=cephfs_a
----

- *Snapshot Restrictions*

To create or delete snapshots, clients require the ‘s’ flag in addition to ‘rw’. Note that when capability string also contains the ‘p’ flag, the ‘s’ flag must appear after it (all flags except ‘rw’ must be specified in alphabetical order).

----
client.0
    key: AQAz7EVWygILFRAAdIcuJ12opU/JKyfFmxhuaw==
    caps: [mds] allow rw, allow rws path=/bar
    caps: [mon] allow r
    caps: [osd] allow rw tag cephfs data=cephfs_a
----


=== Read on / root FS, Read/Write only on /dir1

We are going to create a new user with id 1

----
# ceph fs authorize fs_name client.1 / r /dir1 rw
[client.1]
	key = AQBNerVj77J2IBAAnmvdDyaMv20nxT0NyFd2cA==
# ceph auth get client.1 -o /etc/ceph/ceph.client.1.keyring
exported keyring for client.1
# cat /etc/ceph/ceph.client.1.keyring
[client.1]
	key = AQBNerVj77J2IBAAnmvdDyaMv20nxT0NyFd2cA==
	caps mds = "allow r fsname=fs_name, allow rw fsname=fs_name path=/dir1"
	caps mon = "allow r fsname=fs_name"
	caps osd = "allow rw tag cephfs data=fs_name"
----

We can now check by mounting the filesystem with user id `1`, that we can only
read on `/`:

----
# umount /mnt
[root@workstation-lbedc2 ~]# ceph-fuse --id 1 /mnt
ceph-fuse[32460]: starting ceph client
2023-01-04T08:12:26.261-0500 7f311702f380 -1 init, newargv = 0x55a884998430 newargc=15
ceph-fuse[32460]: starting fuse
# ls /mnt
dir0  dir1
# touch /mnt/dir2
touch: cannot touch '/mnt/dir2': Permission denied
# touch /mnt/dir1/file1
# ls /mnt/dir1/file1
/mnt/dir1/file1
----

=== More examples
By yourself checkout what these other example client caps allow the user to do:

1. 
----
# ceph fs authorize cephfs client.2 /client2 rw
----

2.
----
# ceph fs authorize cephfs client.4 /client4 rwp
----



== MDS Autoscaler
CephFS shared file systems require at least one active MDS service for correct operation, and
at least one standby MDS to ensure high availability. The MDS autoscaler module ensures the
availability of enough MDS daemons.
This module monitors the number of ranks and the number of standby daemons, and adjusts the
number of MDS daemons that the orchestrator spawns.

To enable the MDS autoscaler module.

----
# ceph mgr module enable mds_autoscaler
----
