= Deep dive into RGW Architecture.

.Goals
* Review the RGW frontends
* Understanding RGW Rados Pools
* Detailed overview of Bucket Index & Bucket Sharding
* Deep Dive RGW Data Layout 
* S3 MultiPart Upload
* Garbage Collector

:numbered:


== RadosGW Frontends

The Ceph Object Gateway exposes its RESTful interfaces over HTTP. RGW currently
supports two embedded HTTP frontend libraries that can be configured with
`rgw_frontends` :

- CivetWeb web server. CivetWeb is a C/C++ embeddable web server.
- Beast. The beast front end uses the Boost Beast library for HTTP parsing and the Boost.Asio library for asynchronous network i/o.

In the current versions of Ceph, Beast is the default frontend used, It provides
increased performance than it's predecessor, Civetweb. 

Reference the 
https://www.redhat.com/en/blog/comparing-red-hat-ceph-storage-33-bluestorebeast-performance-red-hat-ceph-storage-20-filestorecivetweb[Civetweb
versus Beast Blog] for more information.

----
# ceph config get mon rgw_frontends
beast port=7480
----

The Frontend and Frontend options can be configured with Cephadm, example spec:

----
service_type: rgw
service_id: myrealm.myzone
spec:
    rgw_realm: myrealm
    rgw_zone: myzone
    ssl: true
    rgw_frontend_port: 1234
    rgw_frontend_type: beast
    rgw_frontend_ssl_certificate: ...
----

== RadosGW pools

The Ceph Object Gateway uses several pools for its various storage needs, listed in the Zone object. A single zone named default is created automatically with pool names starting with default.rgw.

When radosgw first tries to operate on a zone pool that does not exist, it will create that pool with the default values from osd pool default pg num and osd pool default pgp num. These defaults are sufficient for some pools, but others (especially those listed in placement_pools for the bucket index and data) will require additional tuning. 

----
# ceph osd lspools | grep rgw
2 .rgw.root
3 default.rgw.log
4 default.rgw.control
5 default.rgw.meta
6 default.rgw.buckets.index
7 default.rgw.buckets.data
----

Pool names particular to a zone follow the naming convention `zone-name.pool-name`. For example, a zone named us-east will have the following pools:

----
.rgw.root
us-east.rgw.control
us-east.rgw.meta
us-east.rgw.log
us-east.rgw.buckets.index
us-east.rgw.buckets.data
----


[TODO] Create  table
 Pool
Description
.rgw.root
Domain Root Pool
.rgw.control
Control Pool
.rgw.gc
The garbage Collection pool, contains hash buckets containing the IDs of the objects to be deleted
.log
Will log bucket and object actions by the RGW
.intent-log
Store a copy of the object update request so that it can be used for undo/redo in case the object request fails
.usage
Contains a log of usage per user
.users
Contains access and secret keys, mapped to user id
.users.email
Contains emails associated with a user
.users.swift
Contains swift sub-user information
.users.uid
Contains a mapping of a user's unique uid
.rgw.buckets.index
Contains the bucket index
.rgw.buckets
Contains object data
.rgw.root
Stores the region and zone definition for the RGW



The zone definitions list several more pools than that, but many of those are consolidated through the use of rados namespaces.

----
radosgw-admin zone get --rgw-zone default
{
    "id": "d9c4f708-5598-4c44-9d36-849552a08c4d",
    "name": "default",
    "domain_root": "default.rgw.meta:root",
    "control_pool": "default.rgw.control",
    "gc_pool": "default.rgw.log:gc",
    "lc_pool": "default.rgw.log:lc",
    "log_pool": "default.rgw.log",
    "intent_log_pool": "default.rgw.log:intent",
    "usage_log_pool": "default.rgw.log:usage",
    "roles_pool": "default.rgw.meta:roles",
    "reshard_pool": "default.rgw.log:reshard",
    "user_keys_pool": "default.rgw.meta:users.keys",
    "user_email_pool": "default.rgw.meta:users.email",
    "user_swift_pool": "default.rgw.meta:users.swift",
    "user_uid_pool": "default.rgw.meta:users.uid",
    "otp_pool": "default.rgw.otp",
    "system_key": {
        "access_key": "",
        "secret_key": ""
    },
    "placement_pools": [
        {
            "key": "default-placement",
            "val": {
                "index_pool": "default.rgw.buckets.index",
                "storage_classes": {
                    "STANDARD": {
                        "data_pool": "default.rgw.buckets.data"
                    }
                },
                "data_extra_pool": "default.rgw.buckets.non-ec",
                "index_type": 0
            }
        }
    ],
    "realm_id": "",
    "notif_pool": "default.rgw.log:notif"
----

We can list rados namespaces with:

----
# rados ls -p default.rgw.meta --all | awk '{ print $1 }' | sort -u
root
users.keys
users.uid
----


== Bucket Index


Rados Gateway maintains an index per bucket that holds the list and the
metadata of all the objects it contains, the Index holds a key-value map in
rados objects, The value holds basic metadata for each object like header, number of objects, total size,

The bucket index is also used for multiple tasks:

* Listing the bucket content
* Maintaining a journal for versioned operations
* Bucket quota metadata
* Log for multi-zone synchronization
* Bucket Versioning


Each Bucket Index is an Omap entry in RocksDB, Omap is a key-value store associated with an object in a way similar to how Extended Attributes associate with a POSIX file. An object’s omap is not physically located in the object’s storage, Omaps are stored in RocksDB.

We can also create Indexless buckets:
Provides a mechanism in which RadosGW does not track objects in specific buckets. This removes resource contention and reduces the number of round trips RadosGW needs to make to the RADOS backend. Not supported on multi-site configurations


Bucket Index pool for the default zone:

----
# ceph osd lspools | grep default.rgw.buckets.index
6 default.rgw.buckets.index
----

We can list the Bucket Index for a specific bucket using the `radosgw-admin bi
list command`, where we can see the metadata it stores.

----
# radosgw-admin bi list --bucket bucket1
[
    {
        "type": "plain",
        "idx": "hosts5",
        "entry": {
            "name": "hosts5",
            "instance": "",
            "ver": {
                "pool": 16,
                "epoch": 3
            },
            "locator": "",
            "exists": "true",
            "meta": {
                "category": 1,
                "size": 4066,
                "mtime": "2022-12-14T16:27:02.562603Z",
                "etag": "71ad37de1d442f5ee2597a28fe07461e",
                "storage_class": "",
                "owner": "test",
                "owner_display_name": "test",
                "content_type": "",
                "accounted_size": 4066,
                "user_data": "",
                "appendable": "false"
            },
            "tag": "_iDrB7rnO7jqyyQ2po8bwqE0vL_Al6ZH",
            "flags": 0,
            "pending_map": [],
            "versioned_epoch": 0
        }
    },
----


If we take a look at the objects in pool `default.rgw.buckets.index` , we have
several .dir objects,  By default, it is a single RADOS .dir object per bucket, but
it is possible since Hammer to shard that map over multiple RADOS objects. We
will cover Bucket sharding in the next section.

----
# rados -p default.rgw.buckets.index  ls
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.9
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.0
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.10
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.1
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.7
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.8
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.2
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.6
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.5
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.4
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.3
----

Each .dir object is a bucket index; we have 11 because it's the default number of
shards per bucket. the .dir is formatted in the following way
.dir.<maker>.<Shard Number> 

We can get the marker for a bucket using the stats command:

----
# radosgw-admin bucket stats --bucket bucket1 | grep marker
    "marker": "7fb0a3df-9553-4a76-938d-d23711e67677.34162.1",
----

Now that we know that the marker for bucket1 is
`7fb0a3df-9553-4a76-938d-d23711e67677.34162.1`. Let's upload an object to
bucket1 called file1:

----
$ aws --endpoint=http://ceph-node02:8080 s3 cp /etc/hosts s3://bucket1/file1 --region default
upload: ../etc/hosts to s3://bucket1/file1
----

let's investigate the bucket index for this bucket at the rados level, by
listing the omapkeys on the bucket index object, we can see we have a key
called file1, the same as the uploaded object name in S3.

----
# rados -p default.rgw.buckets.index listomapkeys .dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.2
file1
----

If we check the values, we can see that the key/value entry in the bucket index
omap for bucket1 is 217 bytes in size. In the hex translation, we see some info
like the object name

----
# rados -p default.rgw.buckets.index listomapvals .dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.2
file1
value (217 bytes) :
00000000  08 03 d3 00 00 00 05 00  00 00 66 69 6c 65 31 01  |..........file1.|
00000010  00 00 00 00 00 00 00 01  07 03 5a 00 00 00 01 32  |..........Z....2|
00000020  05 00 00 00 00 00 00 4b  ab a1 63 95 74 ba 04 20  |.......K..c.t.. |
----

If we add more objects to our bucket we will see new key/value entries for
each object:

----
# rados -p default.rgw.buckets.index listomapkeys .dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.2
file1
file2
file4
file10
----

We can check the usage of the `default.rgw.buckets.index`, and it's 0 bytes,
although we have 11 Objects(11 index shards of our only bucket, bucket1), Why is
that?

----
rados df -p default.rgw.buckets.index
POOL_NAME                  USED  OBJECTS  CLONES  COPIES  MISSING_ON_PRIMARY  UNFOUND  DEGRADED  RD_OPS       RD  WR_OPS      WR  USED COMPR  UNDER COMPR
default.rgw.buckets.index   0 B       11       0      33                   0        0         0     208  207 KiB      41  20 KiB         0 B          0 B

# rados -p default.rgw.buckets.index stat .dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.2
default.rgw.buckets.index/.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.2 mtime 2022-12-20T07:32:11.000000-0500, size 0

----

As we mentioned before, bucket index objects are Omaps that are stored in the rocksdb database of each OSD, not on the actual pool default.rgw.buckets.index
That is why it's essential to use a fast/flash device for our DB partition, as
the DB partition holds the RocksDB database and the Omaps for our bucket index,
having fast media in the DB partition means faster access to our bucket index,
and quicker listing/access to the objects in our buckets.


One final note, if we want to know on which OSDs our bucket index Omaps are
stored, we can use the following command:

----
# ceph osd map default.rgw.buckets.index default.rgw.buckets.index .dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.2
osdmap e90 pool 'default.rgw.buckets.index' (9) object '.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.2/default.rgw.buckets.index' -> pg 9.6fa75bc9 (9.9) -> up ([5,0,10], p5) acting ([5,0,10], p5)
----

Our Bucket Index log for shard 2 of bucket1 is on OSD 5,0 and 10(replica 3, the
primary is OSD.5), If needed for troubleshooting, we could then further
investigate in rocksdb with the ceph-objectstore-tool, more information on how
to use this tool on a containerised setup[link]

== Bucket Sharding

Sharding is the process of breaking down data into multiple locations to increase parallelism, as well as distribute the load. This is a common feature used in databases.

The concept of sharding is used in Ceph object storage for splitting the bucket
index in RADOSGW

RADOS Gateway keeps an index for all the objects in its buckets for faster and easier lookup.

When the number of objects increases, the RADOS object's size also increases. Two problems arise due to the increased index size.

RADOS does not work well with large objects since it’s not designed as such. Operations such as recovery, scrubbing etc.. work on a single object. If the object size increases, OSDs may start hitting timeouts because reading a large object may take a long time. This is one reason that all RADOS client interfaces, such as RBD, RGW, CephFS use a standard 4MB object size.
Since the index is stored in a single RADOS object, only a single operation can be done at any given time. When the number of objects increases, the index stored in the RADOS object grows. Since a single index is handling a large number of objects, and there is a chance the number of operations also increases, parallelism is not possible, which can become a bottleneck. Multiple operations must wait in a queue since a single operation is possible.
The bucket index is divided into multiple parts to work around these problems. Each shard is kept on a separate RADOS object within the index pool.

Sharding is configured with the tunable bucket_index_max_shards. By default,
this tunable is set to 11.

----
# radosgw-admin bucket stats --bucket bucket1 | grep shards
    "num_shards": 11,
----

We can see a shard per object from 0 to 10 at the rados level for bucket1

----
# rados -p default.rgw.buckets.index ls | grep .dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.9
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.0
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.10
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.1
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.7
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.8
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.2
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.6
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.5
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.4
.dir.7fb0a3df-9553-4a76-938d-d23711e67677.34162.1.3
----

At bucket creation time, the number of shards is defined by the parameter bucket_index_max_shards set at zonegroup level, and it is used for all buckets.

If a different number of shards is required for a specific bucket, it is possible to change it.

- Red Hat recommends a maximum of 102,400 objects per bucket index shard
- The current maximum supported number of bucket index shards is 65521



== Dynamic Bucket Re-Sharding

Since Luminous we have a new RGW capability to manage the sharding of RGW bucket index objects automatically. This completely automates the management of RGW's internal index objects

One property of RADOS (Ceph's underlying object store) is that it doesn't keep an index for all of the objects in the system. Instead, it leverages the CRUSH algorithm to calculate the location of any object based on its name, cluster configuration, and cluster state. This is a scalability enabler: the overall IO capacity can scale with the number of OSDs in the system since there aren't any metadata servers or lookups that need to be used for these IO operations. The RADOS gateway (RGW), which provides an S3-compatible object storage service on top of RADOS, leverages this property. Indeed, when accessing RGW objects, there is no need to touch any index.

However, RGW still maintains an index per bucket, which holds a list and metadata of all the objects it contains. This is needed since RGW needs to be able to provide this data when requested (for example, when listing RGW bucket contents), and RADOS itself does not provide an efficient listing capability. This bucket index is also used for other tasks, like maintaining a journal for versioned operations, bucket quota metadata, and a log for multi-zone synchronization. The bucket index does not affect read operations on objects but adds extra operations when writing and modifying RGW objects.

Luminous finally introduces a dynamic bucket resharding capability. Bucket indexes will now reshard automatically as the number of objects in the bucket grows. Furthermore, there is no need to stop IO operations that go to the bucket (although some concurrent operations may experience additional latency when resharding is in progress). The radosgw process automatically identifies buckets that need to be resharded (if the number of the objects per shard is too large), and schedules a resharding for these buckets. A special thread is responsible for processing the scheduled reshard operations.

The feature is enabled by default; no action is needed, and administrators should no longer worry about this implementation detail.

The process for dynamic bucket resharding periodically checks all the Rados Gateway buckets and detects buckets that require resharding. If a bucket has grown over the value specified in the rgw_max_objs_per_shard parameter, Rados Gateway reshards the bucket dynamically in the background.


Dynamic Resharding process can be monitored and controlled with the `radosgw-admin reshard`:

----
#  radosgw-admin reshard
Expected one of the following:
  add
  bucket
  cancel
  list
  process
  stale
  stale-instances
  status
----

== RadosGW data Layout

Although RADOS only knows about pools and objects with their Extended Attributes (xattrs) and object map (OMAP), conceptually, Ceph Object Gateway organizes its data into three different kinds:

- bucket index
- metadata
- data

=== Bucket index we have already covered in detail.

=== Metadata

There are three sections of metadata:

- bucket: Holds a mapping between bucket name and bucket instance ID.
- bucket.instance: Holds bucket instance information.
- user: Holds user information.


They are represented in the default.rgw.meta pool with root namespace. A bucket record is loaded to obtain a marker, which serves as a bucket ID.

----
# radosgw-admin metadata list bucket
[
    "bucket1"
]
----

bucket.instance relation between bucket name and bucket instance id.

----
radosgw-admin metadata list bucket.instance
[
    "bucket1:7fb0a3df-9553-4a76-938d-d23711e67677.34162.1"
]
----

* Account information

The user ID in Ceph Object Gateway is a string, typically the actual user name from the user credentials and not a hashed or mapped identifier.
When accessing a user’s data, the user record is loaded from an object USER_ID in the default.rgw.meta pool with users.uid namespace.

----
# radosgw-admin metadata list user
[
    "sync-user",
    "test"
]
----

=== Data

The object is located in the default.rgw.buckets.data pool. Object name is MARKER_KEY, for example, default.7593.4_image.png, where the marker is default.7593.4, and the key is image.png. These concatenated names are not parsed and are passed down to RADOS only.

Get the name of the data pool for our default zone:

----
# radosgw-admin zone get | grep data_pool
----

I have 10 objects at the S3 object storage level:

----
# aws --endpoint=http://ceph-node02:8080 s3 ls s3://bucket1
2022-12-20 07:32:11       1330 file1
2022-12-20 07:42:45       1330 file10
2022-12-20 07:41:23       1330 file2
2022-12-20 07:41:27       1330 file3
2022-12-20 07:41:30       1330 file4
2022-12-20 07:42:25       1330 file5
2022-12-20 07:42:29       1330 file6
2022-12-20 07:42:32       1330 file7
2022-12-20 07:42:36       1330 file8
2022-12-20 07:42:41       1330 file9
----

And also 10 Objects at the Rados level:
----
# rados -p default.rgw.buckets.data ls
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_file5
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_file7
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_file2
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_file10
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_file6
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_file8
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_file9
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_file1
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_file3
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_file4
----

You can also use the `radosgw-admin rados ls` command to list rados objects for a
bucket:

----
# radosgw-admin bucket radoslist --bucket bucket1
----

An S3/RGW object might consist of several RADOS objects, the first of which is the head that contains the metadata, such as manifest, Access Control List (ACL), content type, ETag, and user-defined metadata. The metadata is stored in xattrs.

In our example, we have a one-to-one relation because the objects I have uploaded are small in size, only 4kb, If I upload a bigger object it will get split into 4MB objects

----
# rados -p default.rgw.buckets.data  stat  7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_file1 
default.rgw.buckets.data/7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_file1 mtime 2022-12-20T07:32:11.000000-0500, size 1330
----

----
# aws --endpoint=http://ceph-node02:8080 s3 cp awscliv2.zip s3://bucket1/bigfile
# aws --endpoint=http://ceph-node02:8080 s3 ls s3://bucket1/bigfile
2022-12-20 15:10:16   20971520 bigfile
----

We can see that for a single file uploaded, we now have several objects in
rados, if the upload data size is greater than `rgw_obj_stripe_size` by default
set to 4MB: 

----
# ceph config get mon rgw_max_chunk_size
4194304
The chunk size is the size of RADOS I/O requests that RGW sends when accessing data objects. RGW read and write operations will never request more than this amount in a single request. This also defines the RGW head object size, as head operations need to be atomic, and anything larger than this would require more than a single operation. When RGW objects are written to the default storage class, up to this amount of payload data will be stored alongside metadata in the head object.
----

multiple objects are saved, respectively, one header object and one or more tails Object
(default 4MB). 
- The name format of the head object is (bucket_id)_objectname
- The name format of the tail object: `(bucket_id)_shadow. (Object_Head: prefix) _ {Natural sequence starting from 1}`


image::object_head_tail.png[Object Head/Tail]

Head object in our example

----
# rados -p default.rgw.buckets.data ls | grep bigfile$
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_bigfile
----

The chunk size is the size of RADOS I/O requests that RGW sends when accessing data objects. RGW read and write operations will never request more than this amount in a single request. This also defines the RGW head object size, as head operations need to be atomic, and anything larger than this would require more than a single operation. When RGW objects are written to the default storage class, up to this amount of payload data will be stored alongside metadata in the head object.

----
# ceph config get mon rgw_max_chunk_size
4194304
----


The header object has the metadata as xattr 

----
rados -p default.rgw.buckets.data listxattr 7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_bigfile
user.rgw.acl
user.rgw.content_type
user.rgw.etag
user.rgw.idtag
user.rgw.manifest
user.rgw.pg_ver
user.rgw.source_zone
user.rgw.tail_tag
user.rgw.x-amz-content-sha256
user.rgw.x-amz-date
----

Tail objects in our example:

----
# rados -p default.rgw.buckets.data ls | grep shadow_bigfile
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__shadow_bigfile.2~E_PYNwiBq0la0EuZcCOY30KgmRrf1pV.1_1
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__shadow_bigfile.2~E_PYNwiBq0la0EuZcCOY30KgmRrf1pV.2_1
----

Tail objects 4MB in size

----
[root@ceph-node01 ~]# rados -p default.rgw.buckets.data stat 7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__shadow_bigfile.2~E_PYNwiBq0la0EuZcCOY30KgmRrf1pV.1_1
default.rgw.buckets.data/7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__shadow_bigfile.2~E_PYNwiBq0la0EuZcCOY30KgmRrf1pV.1_1 mtime 2022-12-20T15:10:16.000000-0500, size 4194304
----

If the S3 uploaded object is 20MB in size, why do we only have two 4MB shadow
files?. The answer to that is the multipart upload feature, covered in the
next section

----
[root@ceph-node01 ~]# rados -p default.rgw.buckets.data ls | grep bigfile
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__shadow_bigfile.2~E_PYNwiBq0la0EuZcCOY30KgmRrf1pV.1_1
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__multipart_bigfile.2~E_PYNwiBq0la0EuZcCOY30KgmRrf1pV.3
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__multipart_bigfile.2~E_PYNwiBq0la0EuZcCOY30KgmRrf1pV.1
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__shadow_bigfile.2~E_PYNwiBq0la0EuZcCOY30KgmRrf1pV.2_1
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__multipart_bigfile.2~E_PYNwiBq0la0EuZcCOY30KgmRrf1pV.2
----

== S3 Multipart Upload

Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.

image::multipart.png[S3 Multipart Upload]

Using multipart upload provides the following advantages:

- Improved throughput – You can upload parts in parallel to improve throughput.
- Quick recovery from any network issues – Smaller part size minimizes the impact of restarting a failed upload due to a network error.
- Pause and resume object uploads – You can upload object parts over time. After you initiate a multipart upload, there is no expiry; you must explicitly complete or stop the multipart upload.
- Begin an upload before you know the final object size – You can upload an object as you are creating it.

Steps:

- Multipart Upload Initiation: When a request comes to upload an object file, the first thing you get is the Upload ID. This is a unique number/identifier for your upload.
- Parts Upload: It’s important to remember that we need the part ID beside the upload ID. It means that there’s an Upload ID and Part ID for every upload. Please note if you upload a new file with an existing Part ID, this part will be overwritten.
- Multipart Upload Completion or Abort: To complete the multipart process, we must finish uploading all our parts. Only when the process is completed do we get the ACK that all the parts are okay, and only then can we mark the upload as completed. Please note that if the upload process is aborted, the multipart process gets stuck and never ends unless there’s a lifecycle rule or you re-upload the multipart objects files.

By default, the chuck size of the AWS cli can be configured with the following options in the .aws/config file, the default chunk size is 8MB

- multipart_threshold is the transfer size threshold for which multipart uploads, downloads, and copies will automatically be triggered. For our script, files larger than 5GB will be uploaded with multipart.
- max_concurrent_requests is the maximum number of threads that will be used.
- multipart_chunksize is the chuck size the parts will be split in.

----
[profile]
aws_access_key_id=foo
aws_secret_access_key=bar
s3 =
  max_concurrent_requests = 20
  max_queue_size = 10000
  multipart_threshold = 64MB
  multipart_chunksize = 8MB
----

We have an Upload ID(Bucket ID/Marker) and a Part ID:

UploadID: .2~E_PYNwiBq0la0EuZcCOY30KgmRrf1pV. | PartID: . 1 (at the end of the line)

So let's check it out with an example, we will set the client chunk size to
5MB, and upload a 20MB file

----
# aws configure set default.s3.multipart_chunksize 5MB
# aws --endpoint=http://ceph-node02:8080 s3 cp text.txt s3://bucket1/5chuncks
----

We are sending 5 MB chunks to RGW, RGW has a stripe width of 4 MB, which means RGW will take the first 4 MB and create a "multipart" file and then a 1 MB "shadow" as a tail file.

----
[root@ceph-node01 ~]# rados -p default.rgw.buckets.data ls | grep 5chuncks
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__shadow_5chuncks.2~r3yyxqL2hYs5DW32L9UXR3uawF4VEKL.2_1
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__multipart_5chuncks.2~r3yyxqL2hYs5DW32L9UXR3uawF4VEKL.2
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__shadow_5chuncks.2~r3yyxqL2hYs5DW32L9UXR3uawF4VEKL.3_1
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__shadow_5chuncks.2~r3yyxqL2hYs5DW32L9UXR3uawF4VEKL.4_1
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__multipart_5chuncks.2~r3yyxqL2hYs5DW32L9UXR3uawF4VEKL.4
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__shadow_5chuncks.2~r3yyxqL2hYs5DW32L9UXR3uawF4VEKL.1_1
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__multipart_5chuncks.2~r3yyxqL2hYs5DW32L9UXR3uawF4VEKL.3
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_5chuncks
7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__multipart_5chuncks.2~r3yyxqL2hYs5DW32L9UXR3uawF4VEKL.1
----

The Multipart header file is 4MB, and the Tail Shadow file is 1MB

----
# rados -p default.rgw.buckets.data stat 7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__multipart_5chuncks.2~r3yyxqL2hYs5DW32L9UXR3uawF4VEKL.2
default.rgw.buckets.data/7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__multipart_5chuncks.2~r3yyxqL2hYs5DW32L9UXR3uawF4VEKL.2 mtime 2022-12-21T03:07:49.000000-0500, size 4194304
# rados -p default.rgw.buckets.data stat 7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__shadow_5chuncks.2~r3yyxqL2hYs5DW32L9UXR3uawF4VEKL.2_1
default.rgw.buckets.data/7fb0a3df-9553-4a76-938d-d23711e67677.34162.1__shadow_5chuncks.2~r3yyxqL2hYs5DW32L9UXR3uawF4VEKL.2_1 mtime 2022-12-21T03:07:49.000000-0500, size 1048576
----

These parts are not assembled or merged on the RGW, this is their final resting
status. The file "7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_5chuncks" is
a header file; it contains the metadata of the full multipart
file/object. It is not a merged file from all parts. From Rados, it's a 0-byte file

----
# rados -p default.rgw.buckets.data stat 7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_5chuncks
default.rgw.buckets.data/7fb0a3df-9553-4a76-938d-d23711e67677.34162.1_5chuncks mtime 2022-12-21T03:07:49.000000-0500, size 0
----

More information on Multipart Upload can be found at
https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html[AWS
Multipart Upload]

== RadosGW Garbage collector

When users delete files or upload files with the same name, the files are overwritten (also in re-multipart), and Ceph will insert them into something called GC.

Ceph does not remove the files immediately, we can use the commands to list all the files scheduled for removal:

----
radosgw-admin gc list
----

----
radosgw-admin gc list --include-all
----

By default, Ceph waits for 2 hours between gc cycles. To manually run the gc deletion process, run:

----
radosgw-admin gc process --include-all
----

GC Tunables that can be configured for heavy delete RGW workloads:

- Increase the amount concurrent io the cluster will spend on gc requests. (rgw_gc_max_concurrent_io)
- Decrease the amount of time rgw will wait before purging an object (rgw_gc_obj_min_wait)
- Decrease the amount of a RGW will hold a lease on the data to gc’d (rgw_gc_processor_max_time)
- Decrease the amount of time between the start of consecutive garbage collector threads (rgw_gc_processor_period)
- rgw_gc_max_trim_chunk

[WARNING]
====
The rgw_gc_max_objs option should NEVER be modified from it's default value in a running cluster. This value should only be modified pre-deployment of the RGW's.
====
