
==  Object Lifecycle Management 

Object expiration is in substance implemented in the following way: ceph lifecycle mechanism (lc) scans the objects' metadata, where the expiration date is being hold, comparing the expiration date with the current date. Once the condition becomes true lc marks the object as deleted, and it is transferred to Ceph’s garbage collector (gc) which in its turn deletes the expired object. lc process runs periodically with a pre-defined and configurable schedule. This mechanism makes it possible to use a declarative policy to expire the objects we have in our cluster, in the same way it can be done in Amazon S3.

Tier transition in its turn provides us with the ability of creating Storage
Classes for hot (fast media, such as SSD/NVMe used for I/O sensitive workloads) and slow (slow, magnetic media such as SAS/SATA used for archiving) areas relying on bucket lifecycle process for moving out data. Thus it's now possible to create a scheduled data movement between storage classes where data is written to the hot class, then it's moved to the slow (after a configured amount of time) and then expires and deleted for good. This feature is the de-facto standard in AWS nowadays, and now it’s available in Ceph as well.


=== Create a new storage class

To create a new storage class named `SLOW` in the `default-placement`
target, start by adding it to the zonegroup in the master zone:

....
$  radosgw-admin zonegroup placement add --rgw-zonegroup multisite_zonegroup --placement-id default-placement --storage-class SLOW
....

Then provide the zone placement info for that storage class in the
master zone:

....
$  radosgw-admin zone placement add --rgw-zone zone1 --placement-id default-placement --storage-class SLOW --data-pool zone1.rgw.slow.storage.class.buckets.data [--compression lz4]
....

Commit the changes we have performed in the master zone to aware all
Ceph RadosGW:

....
$  radosgw-admin period update --commit
....

=== Verify the new storage class

The zonegroup placement configuration can be queried with:

....
$  radosgw-admin zonegroup get | jq [.placement_targets]
[
  [
    {
      "name": "default-placement",
      "tags": [],
      "storage_classes": [
        "SLOW",
        "STANDARD"
      ]
    },
    {
      "name": "ssd",
      "tags": [
        "allowed-ssd"
      ],
      "storage_classes": [
        "STANDARD"
      ]
    }
  ]
]
....




. Create S3 Lifecycle configuration file named for example lc_slow.xml:

----
$ cat << EOF > lc_slow.xml
<LifecycleConfiguration>
  <Rule>
    <ID>Archive all objects older than 1 days</ID>
    <Filter>
      <Prefix></Prefix>
    </Filter>
    <Status>Enabled</Status>
    <Transition>
      <Days>1</Days>
      <StorageClass>SLOW</StorageClass>
    </Transition>
  </Rule>
</LifecycleConfiguration>
EOF
----

. Create a bucket and apply a lifecycle policy to its future contents:

----
$ s3cmd mb s3://transitionbucket
Bucket 's3://transitionbucket/' created
$ s3cmd setlifecycle lc_slow.xml s3://transitionbucket
s3://transitionbucket/: Lifecycle Policy updated
----

Verify the lifecycle policy has been applied to the bucket:

----
$ s3cmd getlifecycle s3://transitionbucket
<?xml version="1.0" ?>
<LifecycleConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
    <Rule>
        <ID>Archive all object older than 1 days</ID>
        <Prefix/>
        <Status>Enabled</Status>
        <Transition>
            <Days>1</Days>
            <StorageClass>SLOW</StorageClass>
        </Transition>
    </Rule>
</LifecycleConfiguration>
----

Upload a test file into our bucket:

----
$ dd if=/dev/zero of=50mb bs=10M count=5
5+0 records in
5+0 records out
52428800 bytes (52 MB, 50 MiB) copied, 0,0594021 s, 883 MB/s

$ s3cmd put 50mb s3://transitionbucket/50mb
upload: '50mb' -> 's3://transitionbucket/50mb'  [part 1 of 4, 15MB] [1 of 1]
 15728640 of 15728640   100% in   35s   436.04 KB/s  done
upload: '50mb' -> 's3://transitionbucket/50mb'  [part 2 of 4, 15MB] [1 of 1]
15728640 of 15728640   100% in   23s   655.30 KB/s  done
upload: '50mb' -> 's3://transitionbucket/50mb'  [part 3 of 4, 15MB] [1 of 1]
15728640 of 15728640   100% in   22s   675.70 KB/s  done
upload: '50mb' -> 's3://transitionbucket/50mb'  [part 4 of 4, 5MB] [1 of 1]
5242880 of 5242880   100% in   13s   384.96 KB/s  done
----

We have 4 chunks 15 MB each of the test 50 MB file stored now in zone1.rgw.buckets.data pool (replicated 3 times, so total 150 MB):

----
$ rados ls -p zone1.rgw.buckets.data
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_50mb.2~x6GnoM_tN5elP9Gp5kLOE9POVoHF6TH.3_1
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_50mb.2~x6GnoM_tN5elP9Gp5kLOE9POVoHF6TH.2_2
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__multipart_50mb.2~x6GnoM_tN5elP9Gp5kLOE9POVoHF6TH.1
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_50mb.2~x6GnoM_tN5elP9Gp5kLOE9POVoHF6TH.2_3
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_50mb.2~x6GnoM_tN5elP9Gp5kLOE9POVoHF6TH.1_3
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1_50mb
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__multipart_50mb.2~x6GnoM_tN5elP9Gp5kLOE9POVoHF6TH.3
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__multipart_50mb.2~x6GnoM_tN5elP9Gp5kLOE9POVoHF6TH.2
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_50mb.2~x6GnoM_tN5elP9Gp5kLOE9POVoHF6TH.3_3
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_50mb.2~x6GnoM_tN5elP9Gp5kLOE9POVoHF6TH.3_2
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_50mb.2~x6GnoM_tN5elP9Gp5kLOE9POVoHF6TH.1_2
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_50mb.2~x6GnoM_tN5elP9Gp5kLOE9POVoHF6TH.4_1
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__multipart_50mb.2~x6GnoM_tN5elP9Gp5kLOE9POVoHF6TH.4
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_50mb.2~x6GnoM_tN5elP9Gp5kLOE9POVoHF6TH.1_1
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_50mb.2~x6GnoM_tN5elP9Gp5kLOE9POVoHF6TH.2_1

$ rados df -p zone1.rgw.buckets.data
POOL_NAME                   USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS     RD WR_OPS     WR USED COMPR UNDER COMPR 
zone1.rgw.buckets.data 150 MiB      15      0     45                  0       0        0     92 34 MiB    175 84 MiB        0 B         0 B 
----

We confirm that zone1.rgw.slow.data is empty so far:

----
$ rados ls -p zone1.rgw.slow.data
$
----

According to the policy, the objects in transitionbucket will transition from pool zone1.rgw.buckets.data to default.rgw.slow.data after 1 days.

TIP: For testing purposes you might want to modify rgw lc debug interval to 60
in the Ceph RGW config database, followed by ceph-radosgw unit restart.
This will enable debug interval for the lifecycle process (each day in the bucket lifecycle configuration equals 60 sec, so 1 days expiration is actually 1 minute).

----
$ ceph config set client.rgw.multi.zone1 rgw_lc_debug_interval 60
----

Now we wait for 1 minutes (rgw lc debug interval = 60), and verify if an object has been moved to SLOW storage class after this period:

----
$ s3cmd info s3://transitionbucket/50mb
s3://transitionbucket/50mb (object):
   File size: 52428800
   Last mod:  Tue, 20 Apr 2021 12:42:48 GMT
   MIME type: application/octet-stream
   Storage:   SLOW 
   MD5 sum:   25e317773f308e446cc84c503a6d1f85
   SSE:       none
   Policy:    none
   CORS:      none
   ACL:       Test User: FULL_CONTROL
----

So it confirms the objects are now on SLOW storage.  Let's verify the contents of zone1.rgw.slow.data pool:

----
$ rados ls -p zone1.rgw.slow.data
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_.OI9wNbjZ5SE5AGEoZjDa4eJpUnuWu5H_1
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_.OI9wNbjZ5SE5AGEoZjDa4eJpUnuWu5H_0
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_.OI9wNbjZ5SE5AGEoZjDa4eJpUnuWu5H_7
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_.OI9wNbjZ5SE5AGEoZjDa4eJpUnuWu5H_4
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_.OI9wNbjZ5SE5AGEoZjDa4eJpUnuWu5H_5
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_.OI9wNbjZ5SE5AGEoZjDa4eJpUnuWu5H_9
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_.OI9wNbjZ5SE5AGEoZjDa4eJpUnuWu5H_8
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_.OI9wNbjZ5SE5AGEoZjDa4eJpUnuWu5H_3
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_.OI9wNbjZ5SE5AGEoZjDa4eJpUnuWu5H_10
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_.OI9wNbjZ5SE5AGEoZjDa4eJpUnuWu5H_12
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_.OI9wNbjZ5SE5AGEoZjDa4eJpUnuWu5H_6
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_.OI9wNbjZ5SE5AGEoZjDa4eJpUnuWu5H_11
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1__shadow_.OI9wNbjZ5SE5AGEoZjDa4eJpUnuWu5H_2
----

----
$ rados df -p zone1.rgw.slow.data
POOL_NAME                USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS  RD WR_OPS     WR USED COMPR UNDER COMPR 
zone1.rgw.slow.data 150 MiB      13      0     39                  0       0        0     11 0 B     35 84 MiB        0 B         0 B 
[...]
----

It is now populated with 150 MB of our test file.

The chunks are no more in zone1.rgw.buckets.data pool and we see 0B USED field in the rados df output:

----
$ rados ls -p zone1.rgw.buckets.data
e6836dbc-e1c5-4cd5-bf3f-a998a6e3886d.34106.1_50mb

$ rados df -p zone1.rgw.buckets.data
POOL_NAME                USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS     RD WR_OPS     WR USED COMPR UNDER COMPR 
zone1.rgw.buckets.data  0 B       1      0      3                  0       0        0    128 84 MiB    205 84 MiB        0 B         0 B 
----

NOTE: If the objects weren’t deleted, it is probably because the garbage collector isn’t synchronized with the lifecycle process. We can execute radosgw-admin gc process --include-all to run the garbage collection manually so we don't have to wait for its scheduled run.
