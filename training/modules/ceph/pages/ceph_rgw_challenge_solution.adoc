= Ceph RadosGW Challenge

:toc:
:toclevels: 3
:icons: font
:source-highlighter: pygments
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:

== Goals

* Deploy RGW in HA mode
* Work with Placement Targets and Storage Classes
* Use Life Cycle policies to transition object to other tiers
* Work with bucket policies
* Configure basic STS authentication
* Secure the RGW endpoint with SSL
* Audit all S3 operations in RGW
* Configure a Multisite Replicated 

== Deploy RGW services

=== Exercide

* Create a realm called `single`, add a zonegroup called `singlezg`, and a master zone called `singlezone`
* Deploy two RGW services, using the above-mentioned realm and zone, on port 8080
* Create a test user called `user1`, and test rgw setup by creating a bucket * called `bucket1` and upload an object with S3 client

=== Solution

----
# radosgw-admin realm create --rgw-realm=single --default
# radosgw-admin zonegroup create --rgw-zonegroup=singlezg --master --default
# radosgw-admin zone create --rgw-zonegroup=singlezg --rgw-zone=singlezone --master --default
# radosgw-admin period update --rgw-realm=single --commit
# ceph orch apply rgw single.zone --realm=single --zone=singlezone --placement="2 proxy01 ceph-node02" --port=8080
# radosgw-admin user create --uid='user1' --display-name='First User' --access-key='user1' --secret-key='user1'
----

----
# aws configure
AWS Access Key ID [None]: user1
AWS Secret Access Key [None]: user1
Default region name [None]: singlezg
Default output format [None]: text
# aws s3 --endpoint http://proxy01:8080 mb s3://bucket1 --region singlezg
----


== Deploy a secured(SSL) Ingress service to provide HA

=== Exercise

* Deploy an ingress service using ip `192.168.56.100` that will load-balance client requests between both of the deployed RGW services
* Configure SSL, the SSL termination must be done at the Ingress/HAproxy level
** You can use certificates available on /root/certificates of the admin nodes,
or create your own custom certificates, the SAN of the provided certs covers `s3.example.com` and `s3zone1.example.com`

=== Solution

----
# cat << EOF >  rgw-ingress.yaml
service_type: ingress
service_id: rgw.objectgw
placement:
  hosts:
    - ceph-node02
    - ceph-node03
spec:
  backend_service: rgw.single.zone
  virtual_ip: 192.168.56.100/24
  frontend_port: 443
  monitor_port:  1967
  ssl_cert: |
     -----BEGIN CERTIFICATE-----
$( cat /root/certificates/certificate.pem | grep -v CERTIFICATE | awk '{$1="     "$1}1' )
     -----END CERTIFICATE-----
     -----BEGIN RSA PRIVATE KEY-----
$( cat /root/certificates/certificate.key | grep -v PRIVATE | awk '{$1="     "$1}1' )
     -----END RSA PRIVATE KEY-----
EOF
----

----
# ceph orch apply -i rgw-ingress.yaml
----

After a couple of minutes:

----
# curl https://s3zone1.example.com <?xml version="1.0" encoding="UTF-8"?><ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><Owner><ID>anonymous</ID><DisplayName></DisplayName></Owner><Buckets></Buckets></ListAllMyBucketsResult>
# aws --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3 ls
----

== Add a new placement target to zonegroup `singlezg`

=== Exercise

* Create dedicated EC pools for the placement target
** Create a new EC pool called `singlezone.rgw.ssd.buckets.data` pg count 32, with 2+1 EC and host failure domain
WARNING: EC 2+1 Setting is not supported in production, just for lab testing, check-out valid configurations https://access.redhat.com/articles/1548993[here]
*** Enable Bluestore compression: aggressive, snappy
** Create a new replicated pool `singlezone.rgw.ssd.buckets.index` pg count: 8, replica 3 and host failure domain
** Create a new replicated pool `singlezone.rgw.ssd.buckets.non-ec` pg count: 8, replica 3 and host failure domain
* Create a Placement target called `rgwec`
** Users should be able to store data on the new placement target using a reference tag when they create their buckets
** Modify `user1` adding the new tag, create a bucket with reference tag `rgwec`

=== Solution

Create the requested pools for the Placement Target and also enable compresion
for the data pool `singlezone.rgw.ssd.buckets.data`

----
# ceph osd pool create zone1.rgw.ssd.buckets.index 8 8 replicated
# ceph osd pool create zone1.rgw.ssd.buckets.non-ec 8 8 replicated
# ceph osd erasure-code-profile set profile21 k=2 m=1
# ceph osd pool create singlezone.rgw.ssd.buckets.data 32 32 erasure profile21
# ceph osd pool application enable zone1.rgw.ssd.buckets.index rgw
# ceph osd pool application enable zone1.rgw.ssd.buckets.non-ec rgw
# ceph osd pool application enable singlezone.rgw.ssd.buckets.data rgw
# ceph osd pool set singlezone.rgw.ssd.buckets.data compression_mode aggressive
# ceph osd pool set singlezone.rgw.ssd.buckets.data compression_algorithm snappy
----

Create the placement group called `rgwec` , and add the dedicated pools for
this new placement group

----
# radosgw-admin zonegroup placement add --rgw-zonegroup singlezg --placement-id rgwec
# radosgw-admin zone placement add --rgw-zone singlezone --placement-id rgwec --data-pool singlezone.rgw.ssd.buckets.data --index-pool singlezone.rgw.ssd.buckets.index --data-extra-pool singlezone.rgw.ssd.buckets.non-ec
#  radosgw-admin period update --commit
----

We are going to add a tag called `allowed-rgwrc` so only users with this tag are able to create buckets in this placement group

----
# radosgw-admin zonegroup get > /etc/ceph/zonegroup.json
# vi /etc/ceph/zonegroup.json
{
    "id": "41eed131-da38-461e-a5ca-b633189f7f38",
    "name": "singlezg",
    "api_name": "singlezg",
    "is_master": "true",
    "endpoints": [],
    "hostnames": [],
    "hostnames_s3website": [],
    "master_zone": "d2aca484-dfa7-433d-8469-8df6a3a242e9",
    "zones": [
        {
            "id": "d2aca484-dfa7-433d-8469-8df6a3a242e9",
            "name": "singlezone",
            "endpoints": [],
            "log_meta": "false",
            "log_data": "false",
            "bucket_index_max_shards": 11,
            "read_only": "false",
            "tier_type": "",
            "sync_from_all": "true",
            "sync_from": [],
            "redirect_zone": "",
            "supported_features": [
                "resharding"
            ]
        }
    ],
    "placement_targets": [
        {
            "name": "default-placement",
            "tags": [],
            "storage_classes": [
                "STANDARD"
            ]
        },
        {
            "name": "rgwec",
            "tags": ["allowed-rgwrc"],       <---- Add Tag to placement targer
            "storage_classes": [
                "STANDARD"
            ]
        }
    ],
    "default_placement": "default-placement",
    "realm_id": "5bc04cc6-1ddb-47e3-ab8b-5d556c63ae9b",
    "sync_policy": {
        "groups": []
    },
    "enabled_features": [
        "resharding"
    ]
}

# radosgw-admin zonegroup set < /etc/ceph/zonegroup.json
# radosgw-admin period update --commit
----

Now we add the tag `allowed-rgwrc` so we allow user1 to use the new placement group we created with the EC data pool

----
# radosgw-admin user modify --uid user1 --placement-id default-placement --storage-class STANDARD --tags allowed-rgwrc
----

As user1 if we create a bucket with specifiying the placement group it will get
create in the default placement called `default-placement`

----
# aws --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3api create-bucket --bucket normalbucket --region singlezg
# radosgw-admin bucket stats --bucket normalbucket | grep placement_rule
    "placement_rule": "default-placement",
----

As user1 if we use the LocationConstraint with `singlezg:rgwec` we will create
the bucket in the rgwec placement group

----
# aws --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3api create-bucket --bucket my-bucket --region singlezg --create-bucket-configuration LocationConstraint=singlezg:rgwec

# radosgw-admin bucket stats --bucket my-bucket | grep placement_rule
    "placement_rule": "rgwec",

# radosgw-admin bucket stats --bucket my-bucket | grep marker
    "marker": "d2aca484-dfa7-433d-8469-8df6a3a242e9.44443.1",
    "max_marker": "0#,1#,2#,3#,4#,5#,6#,7#,8#,9#,10#",
    
# rados -p singlezone.rgw.ssd.buckets.index ls | grep 44443
.dir.d2aca484-dfa7-433d-8469-8df6a3a242e9.44443.1.8
.dir.d2aca484-dfa7-433d-8469-8df6a3a242e9.44443.1.2
.dir.d2aca484-dfa7-433d-8469-8df6a3a242e9.44443.1.9
.dir.d2aca484-dfa7-433d-8469-8df6a3a242e9.44443.1.5
.dir.d2aca484-dfa7-433d-8469-8df6a3a242e9.44443.1.3
.dir.d2aca484-dfa7-433d-8469-8df6a3a242e9.44443.1.7
.dir.d2aca484-dfa7-433d-8469-8df6a3a242e9.44443.1.6
.dir.d2aca484-dfa7-433d-8469-8df6a3a242e9.44443.1.0
.dir.d2aca484-dfa7-433d-8469-8df6a3a242e9.44443.1.10
.dir.d2aca484-dfa7-433d-8469-8df6a3a242e9.44443.1.1
.dir.d2aca484-dfa7-433d-8469-8df6a3a242e9.44443.1.4
----



== Configure a LifeCycle policy to transition objects to a COLD Storage Class

=== Exercise

* Create a new RGW data pool used for storing Cold Data
** Create a new EC pool called `singlezone.rgw.cold.buckets.data` pg count 16, with 2+1 EC and host failure domain
* Create a new Storage Class called `COLD` in the DEFAULT Placement Group
** Create a new bucket called `data1`
** Configure a Life Cycle policy that will transition objects to the `COLD` Storage class after 10 days
** Apply the LCP to the `data1` bucket
** Upload some objects and test the LCP is working(Remember rgw_lc_debug_interval to speed things up)

=== Solution

----
# ceph osd pool create singlezone.rgw.cold.buckets.data 16 16  erasure profile21
# ceph osd pool application enable singlezone.rgw.cold.buckets.data  rgw
----

----
# radosgw-admin zonegroup placement add --rgw-zonegroup singlezg --placement-id default-placement --storage-class COLD
# radosgw-admin zone placement add --rgw-zone singlezone --placement-id default-placement --storage-class COLD --data-pool singlezone.rgw.cold.buckets.data
# radosgw-admin period update --commit
----

----
# aws --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3api create-bucket --bucket data1 --region singlezg 
# aws --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3 cp /etc/hosts s3://data1/hosts --region singlezg
# aws --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3api list-objects-v2 --bucket data1 | jq '.Contents[].StorageClass'
"STANDARD"
----

----
# cat lc_cold.xml
{
    "Rules": [
     {
      "ID": "Archive all objects older than 10 days",
      "Filter": {
        "Prefix": ""
      },
      "Status": "Enabled",
      "Transitions": [
        {
        "Days": 10,
        "StorageClass": "COLD"
        }
     ]
    }
  ]
}


# aws --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3api put-bucket-lifecycle-configuration --bucket data1 --lifecycle-configuration file://lc_cold.xml

# aws --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3api get-bucket-lifecycle-configuration --bucket data1
{
    "Rules": [
        {
            "ID": "Archive all objects older than 10 days",
            "Prefix": "",
            "Status": "Enabled",
            "Transitions": [
                {
                    "Days": 10,
                    "StorageClass": "COLD"
                }
            ]
        }
    ]
}

----


----
# rados ls -p singlezone.rgw.buckets.data
d2aca484-dfa7-433d-8469-8df6a3a242e9.44443.2_hosts2
d2aca484-dfa7-433d-8469-8df6a3a242e9.44443.2_hosts
----

[TIP]
====
just for the transition to happen faster we are going to set the following
value:

# ceph config set client.rgw rgw_lc_debug_interval 10

And restart the RGWs.

# ceph orch restart rgw.single.zone 
====

----
# aws --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3api list-objects-v2 --bucket data1 | jq '.Contents[].StorageClass'
"COLD"
"COLD"
----

== Create a Bucket policy

=== Exercise

* Create a new user called `user2`, and create a bucket called `policy` with the following layout:
** Policy
*** Policy/reads
*** Policy/write
* Create a Bucket policy that will give `user1` the following access to bucket `policy`
** Policy/ <-  list objects(not read/GET)
** Policy/reads <- read(GET) 
** Policy/write <-  write(PUT/DELETE) 

=== Solution

----
# radosgw-admin user create --uid='user2' --display-name='First User' --access-key='user2' --secret-key='user2'
----

----
# cat .aws/credentials
[user1]
aws_access_key_id = user1
aws_secret_access_key = user1
[user2]
aws_access_key_id = user2
aws_secret_access_key = user2

# aws --profile user2 --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3 mb s3://policy --region singlezg
# aws --profile user2 --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3 cp /etc/hosts s3://policy/read/read --region singlezg
# aws --profile user2 --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3 cp /etc/hosts s3://policy/write/write --region singlezg
----

----
# aws --profile user1 --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3 ls s3://policy
An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Unknown
----

----
# cat ok.yaml
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::user/user1"
        ]
      },
      "Action": [
        "s3:ListBucket",
        "s3:ListBucketMultipartUploads",
        "s3:Get*"
      ],
      "Resource": [
        "arn:aws:s3:::policy"
      ]
    },
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::user/user1"
        ]
      },
      "Action": [
      "s3:Get*",
      "s3:ListMultipartUploadParts"
      ],
      "Resource": [
        "arn:aws:s3:::policy/read/*",
        "arn:aws:s3:::policy/read",
        "arn:aws:s3:::policy/read/"
      ]
    },
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::user/user1"
        ]
      },
      "Action": [
      "s3:Get*",
      "s3:PutObject",
      "s3:DeleteObject",
      "s3:AbortMultipartUpload",
      "s3:ListMultipartUploadParts"
      ],
      "Resource": [
        "arn:aws:s3:::policy/write-folder/*",
        "arn:aws:s3:::policy/write-folder",
        "arn:aws:s3:::policy/write-folder/"
      ]
    }
  ]
}

# aws --profile user2 --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3api put-bucket-policy --bucket policy --policy file://ok.yaml
----

----
# aws --profile user1 --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3 ls s3://policy
                           PRE read/
                           PRE write/
# aws --profile user1 --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3 ls s3://policy/read/
2023-03-02 07:30:49       1330 read
# aws --profile user1 --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3 rm s3://policy/read/read
delete failed: s3://policy/read/read An error occurred (AccessDenied) when calling the DeleteObject operation: Unknown
# aws --profile user1 --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3 cp /etc/hosts s3://policy/read/noperms
upload failed: ../etc/hosts to s3://policy/read/noperms An error occurred (AccessDenied) when calling the PutObject operation: Unknown
# aws --profile user1 --ca-bundle /etc/ssl/certs/ca-bundle.crt --endpoint https://s3zone1.example.com s3 cp /etc/hosts s3://policy/write/ok
upload: ../etc/hosts to s3://policy/write/ok
----


== Configure Local Database STS(using AssumeRole)

=== Exercise

* We need to allow `user1` and `user2` to assume an IAM role called `role1` that will give them full access to a bucket called `admin`
** Create a new RGW `admin` user
*** With the `admin` user create an `admin` bucket and add 1 object
** Enable the STS endpoint in the RGW services
** Create a new IAM role called `admin` 
** Create a Role Policy Document that will allow `user1` and `user2` to assume `role1`
** Create a new role policy that allows `user1` and `user2` full access to bucket `admin`
** Using an AWS client assume the IAM `admin` role using STS
** Use the temporal token created to access the `admin` bucket

=== Solution


== Audit all S3 operations
* Configure the opslog, so all S3 operations get stored in * /var/log/ceph/`FSID`/opslog.log

== Configure a new realm that has multisite replication

* New Realm/Zonegroup/Zone
** On site1 the one we already have configured, we need to create:
*** A realm called: `multi`
*** A zonegroup called: `singlezg1`
*** A zone called: `singlezone1`
*** New RGW instances and pools for this Realm

* We need a second Ceph Cluster configured
** The second site ceph deployment needs:
*** A realm called: `multi`
*** A zonegroup called: `singlezg2`
*** A zone called: `singlezone2`
*** RGW instances and pools for this Realm

* Test that the Multisite replication is working between sites.

== Enable per-bucket replication

* Disable Full zonegroup replication
* Create a new bucket called `sync1`
* Configure symmetrical replication for bucket `sync1`
* Create a new bucket called `sync2`
* Configure uni-directional replication from zone `singlezone1` to `singlezone2`
* Test bucket granular replication
