= Setting the Ceph Cluster Baseline before we start Performance Testing

//++++
//<link rel="stylesheet"  href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/3.1.0/css/font-awesome.min.css">
//++++
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:sectnums:
:sectnumlevels: 6
:toc: left
:toclevels: 4




When deploying a brand new ceph cluster, and before starting the benchmarking
or taking the cluster into production, it's always a good idea to do some basic
baseline testing to be sure all the server components are working as expected.

== Define Software Versions

Let's layout the versions of the software that we will be using during the
testing

*Example:*

* *OS.* RHEL 8.7(Kernel )
* *Ceph.* RHCS 5.3
* *Ansible.* 2.9

== Define Hardware 

=== Ceph Nodes

Let's layout the the hardware that we will be used during the
testing

*Example:* 

* Manufacturer and model: Cisco UCS C220-M5
* Processor: 2x 2.5 GHz 8180/205W 28C/38.50MB Cache/DDR4 2666MHz
* Memory: 192 GB
** Disk Layout:
*** 2 x 220 GB SAS/SSD RAID1 for the Operating System.
*** 7 x 4 TB P4500 NVMe disks to use as Ceph OSDs.
*** 1 x 375 GB Intel Optane P4800X disk to store WAL/RocksDB Bluestore components.
** Network Interface Cards: The High Availability is managed at the Cisco Fabric Interconnect layer, so only 2 nics are presented to the OS for Ceph:
***1 x 40 Gbps NIC for the Ceph public network.
** 1 x 40 Gbps NIC for the Ceph cluster network.
** Power Management: IPMI functionality enabled on the server’s motherboard.

=== Client Nodes

Not only the Ceph server specification is importan, the client resources are a
fundamental part of the testing as we will need enough firepower from the
client side to saturate our ceph cluster 

*Example:*

* Manufacturer and model: UCS B200 M4 Blade servers
* Processor: 2x Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz
* Memory: 528 GB
* Network Interface Cards:
** 1 x 20 Gbps NICs for the OSP instances overlay uplink.
** 1 x 20 Gbps NICs for the underlay OSP networks

== Update Hardware Firmware/Drivers to latest.

Update Firmware and Drivers for all the Hardware components involved.

== Configure BIOS options for performance.

This is dependent on the Hardware being used, but in general we recommend to disable P-states and C-states on the Intel® processors, and also to ensure consistency of CPU performance, we disabled Enhanced Intel SpeedStep® technology (EIST) and Intel® Turbo Boost Technology.

*Example:*

----
BIOS Options
Enhanced Speedstep Disabled
Turbo Boost Disabled
Autonomous Core C-Sate Disabled
Processor C State Disabled
Processor C1E Disabled
Processor C3 Report Disabled
Processor C6 Report Disabled
Processor C7 Report Disabled
Global C-State  Control Disabled
----

Very related to the specific Hardware/NiCs that are being used, in many
ocassions no need to modify. Cisco UCS for example does need modifications so
it's always good to check

Number of Transmit and receive queue for each NIC configured on the system, The ring buffer size. 

* Once the ring buffer has been increased, allowing more traffic to be stored to achieve higher traffic rates.
* Increasing the number of queues allows multiple CPU cores to work to service the ring buffer at the same time.

NOTE: The values used here are just an example for a specific workload, don't
copy and paste into other configurations


*Example:*

----
Ethernet Adapter policy Options
Transmit Queues 8
Tx Ring Size 4096
Receive Queues 8
Rx Ring Size 4096
Completion Queues 16
Receive Side Scaling (RSS) Enabled
Accelerated Receive Flow Steering Enabled
----


== Configure Kernel Parameters

A custom Tuned profile for the OS can be used if needed, depending on the Workload

NOTE: The values used here are just an example for a specific workload, don't
copy and paste into other configurations

*Example:*

----
[main]
summary=ceph_perf

[cpu]
governor=performance
energy_perf_bias=performance
min_perf_pct=100
force_latency=1

[disk]
readahead=>4096

[sysctl]
kernel.sched_min_granularity_ns = 10000000
kernel.sched_wakeup_granularity_ns = 15000000
fs.aio-max-nr=1048576
kernel.pid_max=4194303
fs.file-max=26234859
vm.zone_reclaim_mode=0
vm.swappiness=1
vm.dirty_ratio = 10
vm.dirty_background_ratio = 5
net.ipv4.tcp_rmem=4096 87380 134217728
net.ipv4.tcp_wmem=4096 65536 134217728
net.core.rmem_max=268435456
net.core.wmem_max=268435456
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_fin_timeout = 10
net.ipv4.tcp_slow_start_after_idle = 0
net.ipv4.conf.all.send_redirects = 0
net.ipv4.conf.all.accept_redirects = 0
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.tcp_mtu_probing = 1
net.ipv4.tcp_timestamps = 0
net.core.netdev_max_backlog = 50000
net.ipv4.tcp_max_syn_backlog = 30000
net.ipv4.tcp_max_tw_buckets = 2000000
----


== Pre-Test Verifications

=== Burn-in Tools

Burn-in tests are usually performed on hardware to enable the detection of any problems before the performance tests of Ceph begin, the tests will be run individually on each component. If any component is defective, it is most likely to be detected during the Burn-in test, so the defective part can be replaced before the performance tests are run in this way it won’t affect the results in any way.

There are many Burn-in tools, just as an example we cxan use stress-ng tool. stress-ng will stress test a computer system in various selectable ways. It was designed to exercise various physical subsystems of a computer as well as the various operating system kernel interfaces.

*Example:*

The test will be run on the Ceph OSD servers concurrently during 168 hours, the command that will be run, will exercise CPU, memory and the hard disks.

----
# stress-ng --cpu 28 --vm 12 --hdd 8 --fork 12 --switch 4 --metrics-brief
----

=== Breanking-in Flash drives

The dd utility is really not useful as a benchmarking tool, but it is an excellent tool to use to break in NVMes before you run a real benchmark. We will run dd in a loop at least 5 times, each time writing to the NVMe until it's full. Notice dd is not running against a partition or a file in a filesystem.

Example:

----
dd if=/dev/zero of=/dev/nvme0n1 bs=1M oflag=direct
----

=== RAW Disk Baseline Benchmarking tests

This test helps us compare the performance (IOPS) of each hard drive installed on the servers against a known standard of reference provided by the manufacturer, in this case Intel. So we can be certain that disks are performing as expected before we start adding extra layers of complexity on top of them.

This tests will be run locally on each server against each of the 8 disks individually and also against and aggregate of the NVMe disks.

With this test we can check each disk is giving similar IOPS, and that each
host is giving and similar IOPS aggregate.

The tool that we will use to run these I/O tests against the hardrives is fio, fio spawns a number of threads or processes doing a particular type of I/O action as specified by the user. fio takes a number of global parameters, each inherited by the thread unless otherwise parameters given to them overriding
that setting is given.  The typical use of fio is to write a job file matching the I/O load one wants to simulate.


*Example:*

==== Single disk IOPS fio test:

----
[global]
norandommap
refill_buffers
bs=4k
runtime=300
ioengine=libaio
iodepth=32
direct=1
sync=0
buffered=0
randrepeat=0
time_based=1
clocksource=gettimeofday
ramp_time=5
write_bw_log=fio
write_iops_log=fio
write_lat_log=fio
log_avg_msec=1000
write_hist_log=fio
log_hist_msec=10000

[job-/dev/nvme7n1]
filename=/dev/nvmeXn1
rw={randread,randwrite}
size=307200M
numjobs=4
----

==== Full node drive testing:

We can run 2 types of workloads:

* small I/O 4k random read/write to get the global IOPS available from each node
* 4MB random read/write to get the maximum throughput we can expect from each node in the cluster.

Using an IO/depth of 16. Looking to get the maximum out of each Ceph OSD node we have configured FIO with 20 concurrent jobs for the 4KB workloads, for the workload with 4MB blocks we only needed to configure 4 concurrent jobs to saturate the disks on the nodes.
Each test was run four times at 60 minutes per run with a two-minute ramp-up time. The average results of those four runs are represented in the figures below.

----
[global]
norandommap
refill_buffers
bs=4096k
runtime=300
ioengine=libaio
iodepth=4
direct=1
sync=0
buffered=0
randrepeat=0
time_based=1
clocksource=gettimeofday
ramp_time=100
write_bw_log=fio
write_iops_log=fio
write_lat_log=fio
log_avg_msec=6000
write_hist_log=fio
log_hist_msec=10000

[job-/dev/nvme0n1]
filename=/dev/nvme0n1
rw=randread
size=307200M
numjobs=4

[job-/dev/nvme1n1]
filename=/dev/nvme1n1
rw=randread
size=307200M
numjobs=4

[job-/dev/nvme2n1]
filename=/dev/nvme2n1
rw=randread
size=307200M
numjobs=4

[job-/dev/nvme3n1]
filename=/dev/nvme3n1
rw=randread
size=307200M
numjobs=4
----

=== Network Performance: Throughput and Latency baseline tests.

Because Ceph is a distributed storage solution it heavily relays on the network, the performance that we are able to get from Ceph is tightly coupled with the performance of the network, this is why we need to run extensive network performance tests so we can be sure that each component in the network is delivering as expected.

During the tests we will be measuring 3 major indicators:

. *Latency* is the time required to transmit a packet across a network
. *Throughput* is defined as the quantity of data being sent/received by unit of time
. *Packet loss* reflects the number of packets lost per 100 packets sent by a host

To run these tests we can for example use pbench. Pbench is a Benchmarking and
Performance Analysis Framework that provides a set of pre-packaged scripts to
run some common benchmarks using the collection tools and other facilities that
Pbench provides. Pbench has different modules/add-ons that we can use, for
networking we ca use the pbench_uperf module.

uperf represents a new kind of benchmarking tool where instead of running a fixed benchmark or workload, a description (or model) of the workload is provided and the tool generates the load according to the model. By distilling the benchmark or workload into a model, you can now do various things like change the scale of the workload, change different parameters, change protocols, etc and analyse the effect of these changes on your model. Some of the questions you could answer using uperf are bandwidth and latency (unidirectional and bidirectional) with different protocols like TCP, UDP, SCTP or SSL.

Two types of tests can be run from each endpoint:

. *Stream*: Measure bulk data transfer performance, "Throughput in GB/s".
. *RR*: Request/response performance is quoted as "Transactions/s" From a
transaction rate, one can infer one way and round-trip average latency.

This is an example of iperf3 tests mentioned before from all the Ceph nodes in the cluster and at each of the following endpoints :

----
Ceph nodeX public network  <-> Ceph nodeY public network
Ceph nodeX public network  <-> Ceph nodeY private network
Ceph nodeX public network  <-> Client Nodes
Ceph nodeX public network  <-> Client Nodes
----


