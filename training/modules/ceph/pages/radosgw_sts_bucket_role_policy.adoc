== STS assume role Bucket & Role Policy Examples.


In this module we explain the differences between bucket policies and
role policies when using the STS assume role call implementation, we
also share policy examples and how they are applied differently using
bucket or role policies.

=== Bucket Policy vs Role Policy:

We can apply the rules/policy that give access to the s3 resources in 2
places, at the bucket level and at the role level.

One of the advantages I see of applying the rules at the role level, is
that with one command using radosgw-admin role-policy list/get you can
see all the rules/policies for that role. If you use bucket policies,
you have to list the policies on all the buckets to get the
rules/policies that give access to certain roles.

Just as a simplified example. With role policies, you would be able to
get this info with a single command:

rgw policy-role list role1

role1:

    - bucket1/folder1 ro
    - bucket2/folder2 rw
    - bucket3/folder3/db  rw 

To achieve the same with the policy in buckets you would need to run 3
commands and aggregate the info for role1:

get-policy bucket1

role1: /folder1 ro
role2: /folder1 rw
role3: /folder1 ro

 +

get-policy bucket2

role1: /folder2 rw
role2: /folder2 ro

get-policy bucket3

role1: /folder3/db rw
role3: /folder3/db2 rw

In any case we are going  to share a couple of examples of each, role
and bucket policy. 

==== Bucket Policy. 1st Example.

The steps I have followed to setup this example with a rgw local user
called admin, I created the following resources:

* A bucket called bucket-new
* Inside the bucket create a folder called write-folder
* Inside the bucket create a folder called read-folder
* A role called testrole
* A local RGW user called testuser
* A role document policy to give testuser access to the testrole.

 +

....
# aws --profile admin --endpoint http://10.10.0.10:8080 s3 mb s3://bucket-new
# aws --profile admin --endpoint http://10.10.0.10:8080 s3api put-object --bucket bucket-new --key write-folder/
# radosgw-admin role  get --role-name testrole  
warning: line 8: 'log_file' in section 'client.rgw.cepha.rgw0' redefined
{
    "RoleId": "8d427258-ca7c-4620-87dd-d4c935e20ddb",
    "RoleName": "testrole",
    "Path": "/",
    "Arn": "arn:aws:iam:::role/testrole",
    "CreateDate": "2021-05-13T16:37:14.247Z",
    "MaxSessionDuration": 3600,
    "AssumeRolePolicyDocument": "{\"Version\":\"2012-10-17\",\"Statement\":{\"Effect\":\"Allow\",\"Principal\":{\"AWS\":[\"arn:aws:iam:::user/testuser\"]},\"Action\":[\"sts:AssumeRole\"]}}"
}
....

With all that in place I'm going to set the following bucket policy,
with this policy we are setting the testrole as principal of the policy.
and we are giving access to the role to allow the following:

* can list objects(not read/GET) at the bucket-new level
* only write(PUT/DELETE) inside the write-folder/ folder and it's
subfolders, 
* and read(GET) the read-folder/ and it's subfolders.
* With this config it will also be possible to list objects in other
folders at the root level of the bucket, but the user won't be able to
read(GET) them.

....
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::role/testrole"
        ]
      },
      "Action": [
        "s3:ListBucket",
        "s3:ListBucketMultipartUploads",
        "s3:Get*"
      ],
      "Resource": [
        "arn:aws:s3:::bucket-new"
      ]
    },
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::role/testrole"
        ]
      },
      "Action": [
      "s3:Get*",
      "s3:ListMultipartUploadParts"
      ],
      "Resource": [
        "arn:aws:s3:::bucket-new/read-folder/*",
        "arn:aws:s3:::bucket-new/read-folder",
        "arn:aws:s3:::bucket-new/read-folder/"
      ]
    },
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::role/testrole"
        ]
      },
      "Action": [
      "s3:Get*",
      "s3:PutObject",
      "s3:DeleteObject",
      "s3:AbortMultipartUpload",
      "s3:ListMultipartUploadParts"
      ],
      "Resource": [
        "arn:aws:s3:::bucket-new/write-folder/*",
        "arn:aws:s3:::bucket-new/write-folder",
        "arn:aws:s3:::bucket-new/write-folder/"
      ]
    }
  ]
}
....

We now apply the policy to the bucket:

....
# aws --profile admin --endpoint http://10.10.0.10:8080 s3api put-bucket-policy --bucket bucket-new --policy file://bucket-policy.json.all.with_role
# 
....

We are going to test our policies with Hadoop hdfs cli command, we are
using the assume role credential provider(local RGW users) that is
provided by Hadoop. 

*NOTE:* We could check the applied policies with any S3 client or
library that supports S3 STS features, like for example the aws cli.

These are the options I used in the hadoop core-site.xml:

....
[hadoop@hadoop hadoop]$ cat core-site.xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
 
<!-- Put site-specific property overrides in this file. -->
 
<configuration>
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://hadoop:9000</value>
</property>
 
<property>
  <name>hadoop.tmp.dir</name>
  <value>/home/hadoop/hadooptmpdata</value>
</property>
 
<!--
<property>
  <name>hadoop.security.credential.provider.path</name>
  <value>localjceks://file/home/hadoop/token/aws.jceks</value>
  <description>Path to interrogate for protected credentials.</description>
</property>
-->
 
<property>
  <name>fs.s3a.path.style.access</name>
  <value>true</value>
</property>
<property>
    <name>fs.s3a.impl</name>
    <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
</property>
<property>
    <name>fs.AbstractFileSystem.s3a.impl</name>
    <value>org.apache.hadoop.fs.s3a.S3A</value>
</property>
<property>
  <name>fs.s3a.endpoint</name>
  <value>http://10.10.0.10:8080</value>
</property>
 
<property>
  <name>fs.s3a.assumed.role.session.name</name>
  <value>sesiongo</value>
</property>
 
<property>
  <name>fs.s3a.assumed.role.session.duration</name>
  <value>30m</value>
</property>
 
<property>
  <name>fs.s3a.assumed.role.sts.endpoint</name>
  <value>http://10.10.0.10:8080</value>
</property>
 
<property>
  <name>fs.s3a.assumed.role.sts.endpoint.region</name>
  <value></value>
</property>
 
<property>
  <name>fs.s3a.assumed.role.credentials.provider</name>
  <value>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider</value>
</property>
 
<property>
  <name>fs.s3a.aws.credentials.provider</name>
  <value>org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider</value>
</property>
 
<property>
  <name>fs.s3a.assumed.role.arn</name>
  <value>arn:aws:iam:::role/testrole</value>
</property>
 
<property>
  <name>fs.s3a.access.key</name>
  <value>testuser</value>
</property>
 
<property>
  <name>fs.s3a.secret.key</name>
  <value>testuser</value>
</property>
 
<property>
  <name>fs.s3a.encryption.enabled</name>
  <value>false</value>
</property>
 
<property>
  <name>fs.s3a.connection.ssl.enabled</name>
  <value>false</value>
</property>
</configuration>
 
....

*NOTE:* I have removed the debug/info output around the hdfs dfs command
to reduce the output of each command and make it easier to read.

We can check with the hdfs client, that we can access the bucket
bucket-new, and also list files inside the read-folder folder:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-new/
Found 4 items
drwxrwxrwx   - hadoop hadoop          0 2021-06-18 04:53 s3a://bucket-new/read-folder
drwxrwxrwx   - hadoop hadoop          0 2021-06-18 04:53 s3a://bucket-new/write-folder
 
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-new/read-folder/
Found 1 items
-rw-rw-rw-   1 hadoop hadoop        585 2021-06-18 05:09 s3a://bucket-new/read-folder/host-file
....

We can also test and check that we can GET/read the objects inside the
read folder

....
[hadoop@hadoop hadoop]$ hdfs dfs  -cat  s3a://bucket-new/read-folder/host-file
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.10.0.5 bastion.cephocs.com bastion
 
[hadoop@hadoop hadoop]$ hdfs dfs  -get s3a://bucket-new/read-folder/host-file
 
[hadoop@hadoop hadoop]$ cat host-file
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.10.0.5 bastion.cephocs.com bastion
....

We can also read and get objects inside a subfolder tree inside
bucket-new/read-folder/

....
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-new/read-folder/
Found 2 items
drwxrwxrwx   - hadoop hadoop          0 2021-06-19 02:41 s3a://bucket-new/read-folder/folder1
-rw-rw-rw-   1 hadoop hadoop        585 2021-06-18 05:09 s3a://bucket-new/read-folder/host-file
 
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-new/read-folder/folder1/
Found 1 items
drwxrwxrwx   - hadoop hadoop          0 2021-06-19 02:41 s3a://bucket-new/read-folder/folder1/folder2
 
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-new/read-folder/folder1/folder2
Found 1 items
-rw-rw-rw-   1 hadoop hadoop        585 2021-06-19 02:48 s3a://bucket-new/read-folder/folder1/folder2/host-file
....

But we can't delete or PUT/write new objects into the folder
read-folder:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -rm s3a://bucket-new/read-folder/host-file
2021-06-18 05:10:56,280 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
rm: s3a://bucket-new/read-folder/host-file: delete on s3a://bucket-new/read-folder/host-file: com.amazonaws.services.s3.model.AmazonS3Exception: null (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: tx0000000000000002deb7c-0060cc6320-fa06c-lab1; S3 Extended Request ID: fa06c-lab1-lab), S3 Extended Request ID: fa06c-lab1-lab:AccessDenied
 
[hadoop@hadoop hadoop]$ hdfs dfs  -put /etc/hosts s3a://bucket-new/read-folder/host-file2
put: read-folder/host-file2._COPYING_: put on read-folder/host-file2._COPYING_: com.amazonaws.services.s3.model.AmazonS3Exception: null (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: tx0000000000000002debaf-0060cc632d-fa06c-lab1; S3 Extended Request ID: fa06c-lab1-lab), S3 Extended Request ID: fa06c-lab1-lab:AccessDenied
....

If we now move into the write folder, we can check that we can list and
get/read objects like with the read-folder, but we can also put/write
new objects and delete them:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-new/write-folder/
Found 1 items
-rw-rw-rw-   1 hadoop hadoop        585 2021-06-18 05:09 s3a://bucket-new/write-folder/host-file-write
 
[hadoop@hadoop hadoop]$ hdfs dfs  -rm  s3a://bucket-new/write-folder/host-file-write
Deleted s3a://bucket-new/write-folder/host-file-write
 
[hadoop@hadoop hadoop]$ hdfs dfs  -put /etc/hosts  s3a://bucket-new/write-folder/host-new-file
 
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-new/write-folder/
Found 1 items
-rw-rw-rw-   1 hadoop hadoop        177 2021-06-19 02:34 s3a://bucket-new/write-folder/host-new-file
....

Finally, with other folders at the root level, we are able to list(not
read) the objects inside this folder2, only one level down:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-new/
Found 4 items
drwxrwxrwx   - hadoop hadoop          0 2021-06-19 02:52 s3a://bucket-new/folder-new2
drwxrwxrwx   - hadoop hadoop          0 2021-06-19 02:52 s3a://bucket-new/folder2
drwxrwxrwx   - hadoop hadoop          0 2021-06-19 02:52 s3a://bucket-new/read-folder
drwxrwxrwx   - hadoop hadoop          0 2021-06-19 02:52 s3a://bucket-new/write-folder
 
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-new/folder2/
 
Found 3 items
drwxrwxrwx   - hadoop hadoop          0 2021-06-19 02:52 s3a://bucket-new/folder2/folder3
-rw-rw-rw-   1 hadoop hadoop        585 2021-06-18 10:35 s3a://bucket-new/folder2/host1
-rw-rw-rw-   1 hadoop hadoop        585 2021-06-18 10:35 s3a://bucket-new/folder2/host2
 
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-new/folder2/folder3/
ls: folder2/folder3/: getFileStatus on folder2/folder3/: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: tx0000000000000003427c9-0060cd9435-fa06c-lab1; S3 Extended Request ID: null), S3 Extended Request ID: null:403 Forbidden
 
[hadoop@hadoop hadoop]$ hdfs dfs  -cat  s3a://bucket-new/folder2/host1
cat: s3a://bucket-new/folder2/host1: getFileStatus on s3a://bucket-new/folder2/host1: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: tx00000000000000034283d-0060cd944d-fa06c-lab1; S3 Extended Request ID: null), S3 Extended Request ID: null:403 Forbidden
 
....

=== Bucket Policy. 2nd Example. Use of NotResource

 

The steps I have followed with a rgw local user called admin, I create:

* A bucket called bucket-data
* Inside the bucket create a folder called write-folder
* Inside the bucket create a folder called read-folder
* A role called readrole
* A role called writerole
* A local RGW user called readuser
* A local RGW user called writeuser
* A role document policy to give readuser access to the readrole.
* A role document policy to give writeuser access to the writerole.

The policy we are using for the bucket is the following:

 

....
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::role/readrole",
          "arn:aws:iam:::role/writerole"
        ]
      },
      "Action": [
        "s3:ListBucket",
        "s3:ListBucketMultipartUploads",
        "s3:Get*"
      ],
      "Resource": [
        "arn:aws:s3:::bucket-data"
      ]
    },
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::role/readrole"
        ]
      },
      "Action": [
        "s3:Get*",
        "s3:ListMultipartUploadParts"
      ],
      "Resource": [
        "arn:aws:s3:::bucket-data",
        "arn:aws:s3:::bucket-data/read-folder",
        "arn:aws:s3:::bucket-data/read-folder/*"
      ]
    },
    {
      "Effect": "Deny",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::role/readrole"
        ]
      },
      "Action": "s3:*",
      "NotResource": [
        "arn:aws:s3:::bucket-data",
        "arn:aws:s3:::bucket-data/read-folder",
        "arn:aws:s3:::bucket-data/read-folder/*"
      ]
    },
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::role/writerole"
        ]
      },
      "Action": [
      "s3:Get*",
      "s3:PutObject",
      "s3:DeleteObject",
      "s3:AbortMultipartUpload",
      "s3:ListMultipartUploadParts"
      ],
      "Resource": [
        "arn:aws:s3:::bucket-data",
        "arn:aws:s3:::bucket-data/write-folder",
        "arn:aws:s3:::bucket-data/write-folder/*"
      ]
    },
    {
      "Effect": "Deny",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::role/writerole"
        ]
      },
      "Action": "s3:*",
      "NotResource": [
        "arn:aws:s3:::bucket-data",
        "arn:aws:s3:::bucket-data/write-folder",
        "arn:aws:s3:::bucket-data/write-folder/*"
      ]
    }
  ]
}
....

 +

**NOTE: **We are using the NotResource and Effect Deny workaround
because we found a bug when using the more straight forward approach to
limit the access of the user to certain folders/prefixes that would be
to use a condition with the StringLike to match the folder we want to
use, for example:

....
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::user/testuser"
        ]
      },
      "Action": [
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::svcbucket"
      ],
      "Condition": {
        "StringLike": {
          "s3:prefix": [
            "readfolder4",
            "readfolder4/"
          ]
        }
      }
    },
....

 *NOTE:* There is an open BZ to fix this issue:
https://bugzilla.redhat.com/show_bug.cgi?id=1974678 , important to take
in account that this BZ only affects role policies, bucket policies work
fine.

 +

Let's briefly explain the different sections of the policy. In the first
statement we are allowing access to the bucket-data bucket:

....
"Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::role/readrole",
          "arn:aws:iam:::role/writerole"
        ]
      },
      "Action": [
        "s3:ListBucket",
        "s3:ListBucketMultipartUploads",
        "s3:Get*"
      ],
      "Resource": [
        "arn:aws:s3:::bucket-data"
      ]
    },
....

 In the next statement we are allowing access to read/GET objects and
subfolders under the read-folder: 

....
{
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::role/readrole"
        ]
      },
      "Action": [
        "s3:Get*",
        "s3:ListMultipartUploadParts"
      ],
      "Resource": [
        "arn:aws:s3:::bucket-data",
        "arn:aws:s3:::bucket-data/read-folder",
        "arn:aws:s3:::bucket-data/read-folder/*"
      ]
    },
....

Next, we use the Effect Deny with the NotResource, this denies all
actions on objects inside the bucket except for bucket-data/read-folder
and bucket-data/read-folder/* , that is why we have the previous
statement which explicitly allows the actions on the resources
bucket-data/read-folder bucket-new/data-folder/*

....
    {
      "Effect": "Deny",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::role/readrole"
        ]
      },
      "Action": "s3:*",
      "NotResource": [
        "arn:aws:s3:::bucket-data",
        "arn:aws:s3:::bucket-data/read-folder",
        "arn:aws:s3:::bucket-data/read-folder/*"
      ]
    }, 
....

We first use the readrole in hadoop to check that we can only access and
read/GET inside the read-folder in bucket-data, the core-site.xml is the
same as the previous example except for the following:

....
[hadoop@hadoop hadoop]$ cat core-site.xml | grep -C 3 -E '(user|role.arn)'
</property>
 
<property>
  <name>fs.s3a.assumed.role.arn</name>
  <value>arn:aws:iam:::role/readrole</value>
</property>
 
<property>
  <name>fs.s3a.access.key</name>
  <value>readuser</value>
</property>
 
<property>
  <name>fs.s3a.secret.key</name>
  <value>readuser</value>
</property>
 
<property>
....

 

Some examples with the hdfs client, we are able to list all the folders
at the root level of the bucket:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-data/
Found 2 items
drwxrwxrwx   - hadoop hadoop          0 2021-06-20 03:19 s3a://bucket-data/read-folder
drwxrwxrwx   - hadoop hadoop          0 2021-06-20 03:19 s3a://bucket-data/write-folder
....

But we can only list files inside the read-folder, access to the
write-folder is denied:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-data/write-folder/
ls: write-folder/: getFileStatus on write-folder/: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: tx0000000000000003b5fae-0060ceec15-fa06c-lab1; S3 Extended Request ID: null), S3 Extended Request ID: null:403 Forbidden
....

As expected we can access/list files and subfolders in the read-folder:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-data/read-folder/
Found 1 items
-rw-rw-rw-   1 hadoop hadoop        585 2021-06-19 11:58 s3a://bucket-data/read-folder/read-object
....

 We can also read/GET from the objects inside the read-folder:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -cat  s3a://bucket-data/read-folder/read-object
 
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.10.0.5 bastion.cephocs.com bastion
....

 But we can't delete or write inside the read-folder:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -rm  s3a://bucket-data/read-folder/read-object
rm: s3a://bucket-data/read-folder/read-object: delete on s3a://bucket-data/read-folder/read-object: com.amazonaws.services.s3.model.AmazonS3Exception: null (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: tx0000000000000003b69d6-0060ceede8-fa06c-lab1; S3 Extended Request ID: fa06c-lab1-lab), S3 Extended Request ID: fa06c-lab1-lab:AccessDenied
[hadoop@hadoop hadoop]$ hdfs dfs  -put /etc/hosts  s3a://bucket-data/read-folder/hosts-file
put: read-folder/hosts-file._COPYING_: put on read-folder/hosts-file._COPYING_: com.amazonaws.services.s3.model.AmazonS3Exception: null (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: tx0000000000000003b6a2c-0060ceedf9-fa06c-lab1; S3 Extended Request ID: fa06c-lab1-lab), S3 Extended Request ID: fa06c-lab1-lab:AccessDenied
....

 

If we now try to assume the writerole with the readuser, it will fail: 

....
[hadoop@hadoop hadoop]$ cat core-site.xml | grep -C 3 -E '(user|role.arn)'
</property>
 
<property>
  <name>fs.s3a.assumed.role.arn</name>
  <value>arn:aws:iam:::role/writerole</value>
</property>
 
<property>
  <name>fs.s3a.access.key</name>
  <value>readuser</value>
</property>
 
<property>
  <name>fs.s3a.secret.key</name>
  <value>readuser</value>
</property>
....

....
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-data/read-folder/
2021-06-20 03:34:20,069 ERROR auth.AssumedRoleCredentialProvider: Failed to get credentials for role arn:aws:iam:::role/writerole
....

So we are now going to assume the writerole with the writeuser:

....
[hadoop@hadoop hadoop]$ cat core-site.xml | grep -C 3 -E '(user|role.arn)'
</property>
 
<property>
  <name>fs.s3a.assumed.role.arn</name>
  <value>arn:aws:iam:::role/writerole</value>
</property>
 
<property>
  <name>fs.s3a.access.key</name>
  <value>writeuser</value>
</property>
 
<property>
  <name>fs.s3a.secret.key</name>
  <value>writeuser</value>
</property>
--
....

We can list all the folders at the root level of the bucket:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-data/
 
Found 2 items
drwxrwxrwx   - hadoop hadoop          0 2021-06-20 03:38 s3a://bucket-data/read-folder
drwxrwxrwx   - hadoop hadoop          0 2021-06-20 03:38 s3a://bucket-data/write-folder
....

But we can only access the write-folder if we try and list what is
inside the read-folder it will fail:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-data/read-folder/
ls: read-folder/: getFileStatus on read-folder/: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: tx0000000000000003b77a9-0060cef05f-fa06c-lab1; S3 Extended Request ID: null), S3 Extended Request ID: null:403 Forbidden
....

As expected we can list subfolders and objects inside the write-folder,
and also read/GET objects:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-data/write-folder/
Found 1 items
-rw-rw-rw-   1 hadoop hadoop        585 2021-06-19 11:59 s3a://bucket-data/write-folder/write-object
[hadoop@hadoop hadoop]$ hdfs dfs  -cat  s3a://bucket-data/write-folder/write-object
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.10.0.5 bastion.cephocs.com bastion
....

Because the policy gives access to the writerole to write inside the
write-folder we can check this is the case:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -rm  s3a://bucket-data/write-folder/write-object
Deleted s3a://bucket-data/write-folder/write-object
[hadoop@hadoop hadoop]$ hdfs dfs  -put /etc/hosts  s3a://bucket-data/write-folder/write-object-new
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-data/write-folder/
Found 1 items
-rw-rw-rw-   1 hadoop hadoop        177 2021-06-20 03:39 s3a://bucket-data/write-folder/write-object-new
....

=== Role Policy examples.

Role policies are written in the same way as the bucket policies, the
only difference is that the principal of the policy in a role policy is
always the role, that is why  in role policies the principal statement
is absent. A small example to understand this better:

Bucket policy, has a principal with the role:

....
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::role/readrole",
        ]
      },
      "Action": [
        "s3:ListBucket",
      ],
      "Resource": [
        "arn:aws:s3:::bucket-data"
      ]
    }
  ]
} 
....

The same rule in a Role Policy, the principal is absent because
implicitly the principal is always going to be the role to which we
apply the role policy:

....
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket",
      ],
      "Resource": [
        "arn:aws:s3:::svcbucket"
      ]
    }
  ]
}
....

==== Role Policy. Example. Local RGW user.

 

In this example we are going to try and achieve the same thing we did
with example number 2 of bucket policies, where we have a read and a
write role, the readuser assuming the readrole will only be able to
access the readfolder, and the writeuser assuming the writerole will
only be able to access the writefolder.

 

The steps I have followed with a rgw local user called admin, I created:

* A bucket called bucket-pol
* Inside the bucket create a folder called write-folder
* Inside the bucket create a folder called read-folder
* A role called readrole
* A role called writerole
* A local RGW user called readuser
* A local RGW user called writeuser
* A role document policy to give readuser access to the readrole.
* A role document policy to give writeuser access to the writerole.

 

Once we have all this configuration in place, we can just double check
the roles are in place:

....
[root@cepha /]# radosgw-admin role list | grep -C 3 -E '(readrole|writerole)'
warning: line 8: 'log_file' in section 'client.rgw.cepha.rgw0' redefined 
    },
    {
        "RoleId": "2b0926d2-5fed-47f2-8001-0385cc0f22e9",
        "RoleName": "readrole",
        "Path": "/",
        "Arn": "arn:aws:iam:::role/readrole",
        "CreateDate": "2021-06-19T16:02:35.296Z",
        "MaxSessionDuration": 3600,
        "AssumeRolePolicyDocument": "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"AWS\":[\"arn:aws:iam:::user/readuser\",\"arn:aws:iam:::user/readuser2\"]},\"Action\":[\"sts:AssumeRole\"]}]}"
--
    },
    {
        "RoleId": "0b1abf82-abd8-4741-95e7-7070f08c3850",
        "RoleName": "writerole",
        "Path": "/",
        "Arn": "arn:aws:iam:::role/writerole",
        "CreateDate": "2021-06-19T16:03:15.20Z",
        "MaxSessionDuration": 3600,
        "AssumeRolePolicyDocument": "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"AWS\":[\"arn:aws:iam:::user/writeuser\",\"arn:aws:iam:::user/writeuser2\"]},\"Action\":[\"sts:AssumeRole\"]}]}"
....

We are now going to double check there is no bucket policy configured in
bucket bucket-pol that might interfere with our role policies.

....
[root@bastion ~]# aws --profile admin --endpoint http://10.10.0.10:8080 s3api delete-bucket-policy --bucket bucket-pol
[root@bastion ~]# aws --profile admin --endpoint http://10.10.0.10:8080 s3api get-bucket-policy --bucket bucket-pol
An error occurred (NoSuchBucketPolicy) when calling the GetBucketPolicy operation: The bucket policy does not exist
....

The role policy we are going to apply is the same as the bucket policy
but removing the principal statement:

....
[root@bastion policy]# cat read-role-rolepolicy.json | jq .
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket",
        "s3:ListBucketMultipartUploads",
        "s3:Get*"
      ],
      "Resource": [
        "arn:aws:s3:::bucket-pol"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:Get*",
        "s3:ListMultipartUploadParts"
      ],
      "Resource": [
        "arn:aws:s3:::bucket-pol",
        "arn:aws:s3:::bucket-pol/read-folder",
        "arn:aws:s3:::bucket-pol/read-folder/*"
      ]
    },
    {
      "Effect": "Deny",
      "Action": "s3:*",
      "NotResource": [
        "arn:aws:s3:::bucket-pol",
        "arn:aws:s3:::bucket-pol/read-folder",
        "arn:aws:s3:::bucket-pol/read-folder/*"
      ]
    }
  ]
}
....

 +

One thing to take in account is that the radosgw-admin command we use to
apply the role policy doesn’t like spaces in the .json, so we need to
remove them from the previous policy before applying:

....
[root@bastion policy]# cat read-role-rolepolicy.json | tr -d " \t\n\r" 
{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Action":["s3:ListBucket","s3:ListBucketMultipartUploads","s3:Get*"],"Resource":["arn:aws:s3:::bucket-pol"]},{"Effect":"Allow","Action":["s3:Get*","s3:ListMultipartUploadParts"],"Resource":["arn:aws:s3:::bucket-pol","arn:aws:s3:::bucket-pol/read-folder","arn:aws:s3:::bucket-pol/read-folder/*"]},{"Effect":"Deny","Action":"s3:*","NotResource":["arn:aws:s3:::bucket-pol","arn:aws:s3:::bucket-pol/read-folder","arn:aws:s3:::bucket-pol/read-folder/*"]}]}
[root@bastion policy]# cat read-role-rolepolicy.json | tr -d " \t\n\r" > read-role-rolepolicy-rgw.json
....

We now apply the policy to the readrole using the radosgw-admin command:

....
[root@cepha tmp]# radosgw-admin role-policy put --role-name=readrole --policy-name=access-list-bucket --policy-doc=$(<read-role-rolepolicy-rgw.json)
warning: line 8: 'log_file' in section 'client.rgw.cepha.rgw0' redefined 
Permission policy attached successfully
....

Once applied we configure our core-site.xml to use the readuser local
RGW user, and assume the readrole:

....
<property>
  <name>fs.s3a.assumed.role.arn</name>
  <value>arn:aws:iam:::role/readrole</value>
</property>


<property>
  <name>fs.s3a.access.key</name>
  <value>readuser</value>
</property>


<property>
  <name>fs.s3a.secret.key</name>
  <value>readuser</value>
</property>
....

 +

With this in place let’s test the access with the hdfs command. We can
access the bucket and we will be able to list all folders at the root
level like expected:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-pol/
Found 2 items
drwxrwxrwx   - hadoop hadoop          0 2021-06-26 03:50 s3a://bucket-pol/read-folder
drwxrwxrwx   - hadoop hadoop          0 2021-06-26 03:50 s3a://bucket-pol/write-folder
....

If we try to access/list the write-folder we should get a 403:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -ls  s3a://bucket-pol/write-folder/
ls: write-folder/: getFileStatus on write-folder/: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: tx000000000000000096aa6-0060d6dc99-197571-lab1; S3 Extended Request ID: null), S3 Extended Request ID: null:403 Forbidden
....

If we try to access/list the read-folder we should get access:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -cat  s3a://bucket-pol/read-folder/hosts1
deprecated. Instead, use fs.s3a.server-side-encryption.key
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.10.0.5 bastion.cephocs.com bastion
....

Because this is a read-role if we try to put/write it should fail:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -mkdir  s3a://bucket-pol/read-folder/folder1
mkdir: read-folder/folder1/: PUT 0-byte object  on read-folder/folder1/: com.amazonaws.services.s3.model.AmazonS3Exception: null (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: tx000000000000000097ccf-0060d6dfd7-197571-lab1; S3 Extended Request ID: 197571-lab1-lab), S3 Extended Request ID: 197571-lab1-lab:AccessDenied

[hadoop@hadoop hadoop]$ hdfs dfs  -put /etc/hosts  s3a://bucket-pol/read-folder/hosts3
2021-06-26 04:06:45,701 WARN s3a.S3AInstrumentation: Closing output stream statistics while data is still marked as pending upload in OutputStreamStatistics{blocksSubmitted=1, blocksInQueue=1, blocksActive=0, blockUploadsCompleted=0, blockUploadsFailed=0, bytesPendingUpload=177, bytesUploaded=0, blocksAllocated=1, blocksReleased=1, blocksActivelyAllocated=0, exceptionsInMultipartFinalize=0, transferDuration=0 ms, queueDuration=0 ms, averageQueueTime=0 ms, totalUploadDuration=0 ms, effectiveBandwidth=0.0 bytes/s}
put: read-folder/hosts3._COPYING_: put on read-folder/hosts3._COPYING_: com.amazonaws.services.s3.model.AmazonS3Exception: null (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: tx000000000000000097e28-0060d6e015-197571-lab1; S3 Extended Request ID: 197571-lab1-lab), S3 Extended Request ID: 197571-lab1-lab:AccessDenied
....

Let’s move to the write role example, we follow the same steps, we first
write the policy:

....
[root@bastion policy]# cat write-role-rolepolicy.json 
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket",
        "s3:ListBucketMultipartUploads",
        "s3:Get*"
      ],
      "Resource": [
        "arn:aws:s3:::bucket-pol"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
       "s3:Get*",
       "s3:PutObject",
       "s3:DeleteObject",
       "s3:AbortMultipartUpload",
       "s3:ListMultipartUploadParts"
      ],
      "Resource": [
        "arn:aws:s3:::bucket-pol",
        "arn:aws:s3:::bucket-pol/write-folder",
        "arn:aws:s3:::bucket-pol/write-folder/*"
      ]
    },
    {
      "Effect": "Deny",
      "Action": "s3:*",
      "NotResource": [
        "arn:aws:s3:::bucket-pol",
        "arn:aws:s3:::bucket-pol/write-folder",
        "arn:aws:s3:::bucket-pol/write-folder/*"
      ]
    }
  ]
}
....

....
[root@bastion policy]# cat write-role-rolepolicy.json | tr -d " \t\n\r"
{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Action":["s3:ListBucket","s3:ListBucketMultipartUploads","s3:Get*"],"Resource":["arn:aws:s3:::bucket-pol"]},{"Effect":"Allow","Action":["s3:Get*","s3:PutObject","s3:DeleteObject","s3:AbortMultipartUpload","s3:ListMultipartUploadParts"],"Resource":["arn:aws:s3:::bucket-pol","arn:aws:s3:::bucket-pol/write-folder","arn:aws:s3:::bucket-pol/write-folder/*"]},{"Effect":"Deny","Action":"s3:*","NotResource":["arn:aws:s3:::bucket-pol","arn:aws:s3:::bucket-pol/write-folder","arn:aws:s3:::bucket-pol/write-folder/*"]}]}[root@bastion policy]#

[root@bastion policy]# cat write-role-rolepolicy.json | tr -d " \t\n\r" > write-role-rolepolicy-rgw.json
[root@cepha tmp]# radosgw-admin role-policy put --role-name=writerole --policy-name=access-list-bucket --policy-doc=$(<write-role-rolepolicy-rgw.json)
warning: line 8: 'log_file' in section 'client.rgw.cepha.rgw0' redefined 
Permission policy attached successfully
....

If we try to create a dir or delete a file inside the write-folder it
works as expected:

....
[hadoop@hadoop hadoop]$ hdfs dfs  -mkdir   s3a://bucket-pol/write-folder/folder4
[hadoop@hadoop hadoop]$ hdfs dfs  -ls   s3a://bucket-pol/write-folder/
Found 3 items
drwxrwxrwx   - hadoop hadoop          0 2021-06-26 04:24 s3a://bucket-pol/write-folder/folder4
-rw-rw-rw-   1 hadoop hadoop        585 2021-06-26 03:16 s3a://bucket-pol/write-folder/hosts2
[hadoop@hadoop ~]$ hdfs dfs  -rm  s3a://bucket-pol/write-folder/hosts2
Deleted s3a://bucket-pol/write-folder/hosts2
....

 +

 +

 +
