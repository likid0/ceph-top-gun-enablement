= Ceph Deployment Challenge

:toc:
:toclevels: 3
:icons: font
:source-highlighter: pygments
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:

== Goals

* Review the the Ceph deployment with Cephadm
* Setting specific cpu/memory targets for the ceph containers
* Review the Pool and replica concepts.
* Review scaling-out the cluster with mode nodes
* Review PG autoscaler configuration
* Review the configuration of segregated physicall pools


== Deploy a Ceph Cluster

* Deploy a Ceph cluster with 4 Nodes: ceph-mon01, ceph-mon02, ceph-mon03, proxy02
** Bootstrap node will be ceph-mon01
** The cluster needs to have segregated networks, public and cluster network
*** Public Network will be: `192.168.56.0/24`
*** Cluster Network will be: `172.16.7.0/24`
** The cluster will be deployed using a specification file, to include the four
nodes during initial bootstrap
** Only one OSD per Node is required
** Limit the amount of memory an osd can consume to 2GB
** Limit the amount of resources the MGR can use: 1cpu/1gb of Ram
*** The nodes will belong to a crushmap location of `RACK` called `RACK1`

== Create RBD Pools

* Once the Ceph cluster is deployed we have to create two pools
** pool1 called `rbdreplica2` with replica 2 
** pool2 called `rbdreplica3` with replica 3

== Scale out the Cluster

* We have to add nodes `ceph-node01,ceph-node02,ceph-node03,proxy01` to the cluster
** The new nodes will belong to a datacenter crushmap location of `RACK` called `RACK2`
** Nodes `ceph-node01,ceph-node02,ceph-node03` will configure one OSD per node
** Move the manager services so we have one MGR on each DC.

== Create EC cephfs Pool
* Create a new pool for cephfs data called `cephfsec` with EC replication 4+2, set the failure domain to host

== Create a cephfs Pool
* Create a new pool for cephfs called `cephfsreplica2` with PG count 16, replica 2, and the failure domain set to Datacenter

== Autoscale Pools
* Enable autoscale mode on all pools, and configure the target size ratio with the following ratios:
** rbdreplica2. 10%
** rbdreplica3. 20%
** cephfsreplica2 20%
** cephfsec 50%

== Add OSDs to the cluster

* We need to add a dedicated data pool for RGW, that has to be physically segregated from the rest of cluster data
** We need to add 2 new drives from nodes `ceph-node01,ceph-node02 and ceph-node03`
** We will configure 2 OSD's per drive, With Encryption enabled at the OSD level.
** We need to use a specific device class for the new osds that we wan to segregate (take a look at `ceph osd crush class`, cephadm in 5.3 doesn't support specifiying a class during bootstrap)
** Create a pool called `rgw-security` with `replica 3` , it will use a rule that uses the new device classes we created
