= Ceph Deployment Challenge

:toc:
:toclevels: 3
:icons: font
:source-highlighter: pygments
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:

== Goals

* Review the the Ceph deployment with Cephadm
* Setting specific CPU/memory targets for the ceph containers
* Review the Pool and replica concepts.
* Review scaling out the cluster with more nodes
* Review PG autoscaler configuration
* Review the configuration of segregated physical pools

[NOTE]
====
To increase the difficulty of the challenge you can do a disconnected
deployment of ceph and the observavility stack, you can use the workstation
node as the container registry.
====

== Deploy a Ceph Cluster

* Deploy a Ceph cluster with 4 Nodes: `ceph-node01, ceph-node02, ceph-node03, proxy01`
** Bootstrap node will be ceph-node01
** The cluster needs to have segregated networks, public and cluster network
*** Public Network will be: `192.168.56.0/24`
*** Cluster Network will be: `172.16.7.0/24`
** The cluster will be deployed using a specification file, to include the four nodes during the initial bootstrap
** Only one OSD per Node is required
** Limit the amount of memory an osd can consume to 2GB
*** The nodes will belong to a crushmap location of `RACK` called `RACK1`

== Create RBD Pools https://docs.ceph.com/en/latest/rados/operations/pools/#pools[Docs]

* Once the Ceph cluster is deployed we have to create two pools
** pool1 called `rbdreplica2` with replica 2 
** pool2 called `rbdreplica3` with replica 3

== Scale out the Cluster

* We have to add nodes `ceph-mon01,ceph-mon02,ceph-mon03,proxy02` to the cluster https://docs.ceph.com/en/quincy/cephadm/host-management/#adding-hosts[Docs]
** The new nodes will belong to a crushmap location of `RACK` called `RACK2` https://docs.ceph.com/en/pacific/rados/operations/crush-map/#crush-maps[Docs]
** Nodes `ceph-node01,ceph-node02,ceph-node03` will configure one OSD per node
** Move the MGR services so we have one MGR on each RACK: RACK1 and RACK2.

[IMPORTANT]
====
To add Nodes to our ceph cluster, We need passwordless ssh access to nodes `ceph-mon01,ceph-mon02,ceph-mon03,proxy02` from the
bootstrap node `ceph-node01`

The easiest way to achieve this is to copy the public key being used in ceph-node01 to the
.ssh/authorized_keys file in hosts `ceph-mon01,ceph-mon02,ceph-mon03,proxy02`

Take into account that the private ssh key being used for passwordless ssh in
the hosts(deployed by automation)
is ~/.ssh/ceph , as you can see from the ssh config file:

----
[root@ceph-node01 ~]# cat .ssh/config
Host *
User root
IdentityFile ~/.ssh/ceph
StrictHostKeyChecking no
----
====


== Create EC cephfs Pool
* Create a new pool for cephfs data called `cephfsec` with EC replication 4+2, set the failure domain to host

== Create a cephfs Pool
* Create a new pool for cephfs called `cephfsreplica2` 
** with PG count 16
** replica 2, and the failure domain set to Rack

== Autoscale Pools https://docs.ceph.com/en/latest/rados/operations/placement-groups/#placement-groups[Docs]
* Enable autoscale mode on all pools, and configure the target size ratio with the following ratios:
** rbdreplica2. 10%
** rbdreplica3. 20%
** cephfsreplica2 20%
** cephfsec 50%

== Add OSDs to the cluster

* We need to add a dedicated data pool for RGW, that has to be physically segregated from the rest of the cluster data
** We need to add 2 new drives from nodes `ceph-node01,ceph-node02 and ceph-node03`
** We will configure 2 OSD's per drive, With Encryption enabled at the OSD level. https://docs.ceph.com/en/quincy/cephadm/services/osd/#additional-options[Docs]
** We need to use a specific device class for the new osds that we want to segregate (take a look at `ceph osd crush class`, cephadm in 5.3 doesn't support specifying a class during bootstrap)
** Create a pool called `rgw-security` with `replica 3`, it will use a rule that uses the new device classes we created
