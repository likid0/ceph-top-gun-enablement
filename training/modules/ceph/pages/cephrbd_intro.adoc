= RADOS Block Device (RBD)

//++++
//<link rel="stylesheet"  href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/3.1.0/css/font-awesome.min.css">
//++++
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:sectnums:
:sectnumlevels: 6
:toc: left
:toclevels: 4


== Introduction 

Ceph supports block storage through the RADOS Block Device (aka RBD) access
method for Linux distributions and Kubernetes. RBDs can be accessed either through a kernel module (Linux, Kubernetes)
or through the `librbd` API (OpenStack, Proxmox). In the Kubernetes world,
RBDs are designed to address the need for RWO PVCs. `Pacific` provides a
major update to `librbd` aimed at reducing the performance gap with `kRBD`.

== Details

Each block device is known as a RBD Image and has a set of characteristics:

* Image Name - a character string that supports Unicode
* Image Order - Also known as Image Object Size
* Image Size - The size of the block device
* Stripe Unit - To configure `librbd` striping (defaults to 4MiB)
* Stripe Count - To configure `librbd` striping (defaults to 1)
* Format - To control the image format (defaults to 2)
* Image features 
** `layering`: layering support
** `striping`: striping v2 support
** `exclusive-lock`: exclusive locking support
** `object-map`: object map support (requires exclusive-lock)
** `fast-diff`: fast diff calculations (requires object-map)
** `deep-flatten`: snapshot flatten support
** `journaling`: journaled IO support (requires exclusive-lock)
** `data-pool`: erasure coded pool support

Explanation of the Image features you can enable:

* Layering: Layering enables you to use cloning
** Config numeric value: 1
** CLI value: layering
* Striping v2: Striping spreads data across multiple objects. Striping helps with parallelism for sequential read/write workloads.
** Config numeric value: 2
* CLI value: striping
** Exclusive locking: When enabled, it requires a client to get a lock on an object before making a write. Exclusive lock should only be enabled when a single client is accessing an image at the same time.
* Config numeric value: 4
** CLI value: exclusive-lock
* Object map: Object map support depends on exclusive lock support. Block devices are thin provisioned—​meaning, they only store data that actually exists. Object map support helps track which objects actually exist (have data stored on a drive). Enabling object map support speeds up I/O operations for cloning; importing and exporting a sparsely populated image; and deleting.
** Config numeric value: 8
** CLI value: object-map
* Fast-diff: Fast-diff support depends on object map support and exclusive lock support. It adds another property to the object map, which makes it much faster to generate diffs between snapshots of an image, and the actual data usage of a snapshot much faster.
* Config numeric value: 16
* CLI value: fast-diff
** Deep-flatten: Deep-flatten makes rbd flatten work on all the snapshots of an image, in addition to the image itself. Without it, snapshots of an image will still rely on the parent, so the parent will not be delete-able until the snapshots are deleted. Deep-flatten makes a parent independent of its clones, even if they have snapshots.
Config numeric value: 32
** CLI value: deep-flatten
* Journaling: Journaling support depends on exclusive lock support. Journaling records all modifications to an image in the order they occur. RBD mirroring utilizes the journal to replicate a crash consistent image to a remote cluster.
** Config numeric value: 64
** CLI value: journaling

In order to provide the best performance and avoid bottlenecks Ceph Block Devices
are broken down into smaller pieces. Therefor, a single 32MiB RBD Image will be broken
down in as many object as possible, each object conforming to the size configured
for a specific RBD. 

As a result of this breakdown each object of a given RBD Image will have a different name.
The name of each object is formatted as `{rbd_image_id}.{offset}`. You can visualize RBD objects
in a Ceph pool using the `rados -p {pool_name} ls`. As each object is assigned a different
RADOS name, each object will be assigned to a specific Placement Group as described earlier
in the Data Placement chapter.

NOTE: The cluster sets a default parameter `rbd_default_order` to a value of `22`
that configures the RBD Images to use 4MiB objects. This parameter can be set
on a per client basis.

WARNING: To avoid unnecessary cluster warning, make sure that a pool that contains RBD
Images is assigend a `rbd` label. This is easily achieved through the `rbd init pool {pool_name}`.

The RBD Image order is expressed as an integer between 12 and 25. It is a bit-shift operator and as such
the default `22` value is for 4MiB objects while `21` maps to 2MiB and `23` maps to 8MiB. Since
Red Hat Ceph Storage 4 (Nautilus based) you can specify either the size of the object as an order
(`--image-order`) or as a pure object size (`--object-size`).

IMPORTANT: All RBD Images are thin provisioned. To reclaim the unused sectors of a RBD Image
you must configure the `discard` option when mounting the filesystem hosted on the RBD Image
or run the `fstrim` command at regular intervals.

== Accessing RBD Images

The kernel RBD driver offers superior performance while not providing the
same level of functionality. e.g., no RBD Mirroring until `Pacific`.

.kRBD Diagram
image::cephrbd-krbd-nobg.png[Kernel based RADOS Block Device, align="center"]

To map a RBD Image using the kernel RBD module, aka `krbd`, use the `rbd map {pool_name}/{image_name}`
or the `rbd -p {pool_name} {image_name}` commands. As a result the RBD Image is mapped as
`/dev/rbd{n}` on the Linux server where the command is issued. `{n}` will be `0` for the
first RBD Image mapped onto the node.

To view the RBD Images mapped into a node you can use `rbd showmapped` or `rbd device ls` starting
with Red Hat Storage 5 for the latter.

----
# rbd showmapped
id pool namespace image snap device
0 rbd test - /dev/rbd0
----

Once the RBD Image has been mapped onto the server it is available for regular block device
commands such as `mkfs`.

To unmap a RBD Image from a node use the `rbd unmap {rbd_device_name}`. e.g. `rbd unmap /dev/rbd0`.

WARNING: A the `rbd map` and `rbd unmap` commands manipulate the Linux `dev` directory content
`root` privilege is required to issue those commands.

NOTE: By default the cluster is configured to use the pool name `rbd` to access RBD Images.
The default pool can be modified on a per client machine basis using the `rbd_default_pool`
parameter. Therefor, if the name of the pool is omitted the `rbd` command will try to
access the default pool.

*_Userspace RBD (librbd)_*

This access method leverages all existing RBD features such as RBD Mirroring.

.librbd Diagram
image::cephrbd-librbd-nobg.png[Userspace RADOS Block Device, align="center"]

NOTE: The RBD Cache is local to the client that issues the IO request. By default RBD Cache is
enabled on Ceph client machine.

IMPORTANT: By default the Cache is enabled in write-back mode but it can be set to write-through mode.

The following parameters can be used to control each `librbd` client caching:

* `rbd_cache` - `true` or `false` (defaults to `true`)
* `rbd_cache_size` - Cache size in bytes (defaults to 32MiB per RBD Image)
* `rbd_cache_max_dirty` - Max dirty bytes (defaults to 24MiB. Set to 0 for write-through mode)
* `rbd_cache_target_dirty` - Dirty bytes to start preemptive flush (defaults to 16MiB)

NOTE: Use the `ceph config set {client} {parameter} {value}` to change those parameters.


== Manipulating

The `rbd` command is used to create, modify and in general manipulate RBD Images.
The following table provides a summary of the different RBD commands.

* `rbd create` - To create a RBD Image
* `rbd rm` - To delete a RBD Image
* `rbd ls` - List the RBD Images in a pool
* `rbd info` - To view RBD Image parameters
* `rbd du` - To view the space used in a RBD Image
* `rbd snap` - To create a snapshot of a RBD Image
* `rbd clone` - To create a clone based on a RBD Image snapshot

== Snapshots

A snapshot is a read-only logical copy of an image at a particular point in
time: a checkpoint. One of the advanced features of Ceph block devices is that
you can create snapshots of images to retain point-in-time state history. Ceph
also supports snapshot layering, which allows you to clone images (for example,
VM images) quickly and easily. Ceph block device snapshots are managed using
the rbd command and several higher-level interfaces.


IMPORTANT: Because RBD is unaware of any file system within an image (volume), snapshots are merely crash-consistent unless they are coordinated within the mounting (attaching) operating system. We therefore recommend that you pause or stop I/O before taking a snapshot.  Remember to suspend access to the block device via the `fsfreeze` command before creating a snapshot and to thaw the block device using `fsfreeze --unfreeze` once done.

Creating a snapshot is simple: `rbd snap create {pool_name/}{rbd_image}@{snap_name}`

To manipulate the snapshots, the following command are available:

* `rbd snap ls` - To list the snapshot of a RBD Image
* `rbd snap rollback` - To rollback a snapshot (restore)
* `rbd snap rm` - To delete a snapshot
* `rbd snap protect` - To protect s snapshot (used for cloning)

== Clones

Ceph supports the ability to create many copy-on-write (COW) clones of a block device snapshot. Snapshot layering enables Ceph block device clients to create images very quickly. For example, you might create a block device image with a Linux VM written to it, snapshot the image, protect the snapshot, and create as many copy-on-write clones as you like. A snapshot is read-only, so cloning a snapshot simplifies semantics, making it possible to create clones rapidly.

The cloning process has three steps:

* Create a snapshot
* Protect the snapshot
* Create a clone using the protected snapshot

WARNING: By default Copy-On-Read (COR) is not enabled on RBD clones. This results in the data
potentially always being read from the parent RBD Image as long as the original RBD Image parent
has not been modified. The `rbd_clone_copy_on_read` is used to control COR.

IMPORTANT: By default only Copy_On_Write (COW) is enabled and can not be disabled.

To manipulate RBD Clones the following commands are available:

* `rbd children` - To list the clones of a RBD Image
* `rbd clone` - to create a clone
* `rbd flatten` - To flatten a clone

NOTE: A RBD clone, as it behaves like a regular RBD Image is deleted via the `rbd rm` command.
