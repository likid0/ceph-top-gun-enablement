= Block/File Testing


== Introduction


== Tools

* If you have a K8s/OCP cluster at your disposal for any kind of Block or File IO testing, we would recommend using the https://github.com/cloud-bulldozer/benchmark-operator[BenchMark-Operator]
** It has all the tools you will need to baseline and benchmark your Ceph Cluster: `Iperf3, FIO, Smallfile`
* If you don't have OCP at your disposal https://github.com/ceph/cbt[CBT] is a
** great framework for benchmarking Ceph, Nice Repo from Karan helping out setting
up https://github.com/ksingh7/ceph-cbt[CBT]
** Another tool that works fine and that can aggregate FIO data results is
https://github.com/distributed-system-analysis/pbench/blob/main/agent/bench-scripts/pbench-fio.md[pbench-fio]

== Avoiding OCP range limits, limits and quotas

IMPORTANT: Before running any performance test, If using OCP as a client, make sure the namespace where your fio workloads are running does not have limits or quotas.

Delete or disable the following OpenShift features to ensure no resource limitation from the client's perspective:


* Limitranges.
* Namespace limits.
* Namespace quota.


== Client/Load Generators resources


Having Enough client resources to saturate the Ceph cluster is pivotal to
achieving a positive output of the benchmarking. If the clients don't have
enough firepower to saturate some part of the Ceph cluster, you will not be able
to know the limits of your ceph cluster.


== Always Drop Caches between tests


Almost all of the benchmarking suites have a hook that you can execute before
each test so all the caches can be dropped, achieving a deterministic result on
every test.


== Run the tests with a considerable dataset size


When running tests, consider creating a dataset that is bigger than the RAM
size, ideally, the total RAM of all nodes would be less than 20% of the total
size of the test dataset.

With this, we are trying to avoid the Linux filesystem caching
skewing our results and ending up with a false report of higher performance.


== Consider running the test with an almost empty cluster but also with 50%/60% used capacity


It's great to have the numbers IOPS for an empty cluster, but it's
also important to have a more realistic view of how your cluster will perform
with storage used capacity of around 50/60% that will be closer to a real
life scenario.

== Run the same test several times


Each test run needs to be run a minimum of three times, using
the mathematical average across all test runs. This will help us get rid of
outliers in testing.

An example would be:


* Three test iterations executed for 20 minutes
* with a two-minute ramp-up time
** for a total of 22-minute per test
* an iteration or 66 minutes per pass. 
* Before each iteration, the test script clears all Linux filesystem caches. 


== Configure a ramp-up time


Before each run, it's recommended to have a ramp-up time so every test finds
the caches at the same point, providing fair results in all the test runs.


== Use the same ceph configuration that you will have in production


When running benchmarks, we are always on a quest to achieve more IOPS, and more Throughput in that ques we can be tempted to modify the ceph configuration in certain ways that will allow increasing the performance, IOPS count, total throughput, but if the configuration we are using during the benchmark is not production friendly we won’t have valuable data in our hands, and example:


* Removing cephx authentication
* Using replica x 2, when we will use replica x 3 in prod
* Disabling bluestore CRC checking


== Looking for the best fio workload


First, we must find the best fio workload and ensure all performance tests are executed with the same workload to compare apples with apples.

Ideally, we need a fio workload that can saturate the ceph cluster while ensuring low latency and high cluster IOPS. For that reason, we will need to ensure the following:


* Different fio servers are deployed to spread the workload properly.
* Fio numjobs are set to 8 to ensure concurrency per fio server and increase the load in the ceph cluster.
* Total number of fio jobs will be: fio servers * fio numjobs
* Maintain a reasonable number of I/O units to keep in flight against the file (iodepth=8). If we increase this value too much, we will see a huge increase in operation latency while not increasing the cluster IOPS.
* To increase the number of tests, we have decided to run only one sample per test.
* To avoid the impact of Ceph thin-provisioning, all tests will prefill the RBD volumes before the test execution.
* Fio randwrite jobs will be used as the most demanding workload for the ceph cluster.

After several tests, you should end up with a configuration that gives you the best results for your cluster


* 2 fio servers (one per OCP worker).
* 8 numjobs.
* IOdepth=8.


This is an example FIO file using the FIO RBD engine. No actual operating system mount happens when using this engine. 

NOTE: With FIO RBD engine, we have to use a job count of 1 per RBD volume. 


----
[global]
ioengine=rbd
clientname=admin
pool=rbdpool
#IO-Depth changes depending on the test
iodepth=$IODEPTH
runtime=600
direct=1
sync=0
buffered=0
#Blocksize changes depending on the test
bs=$BLOCKSIZE
#RR,RW, or a mixed workload, this changes depending on the test
rw=$TYPEOFTEST
norandommap
randrepeat=0
startdelay=15
rwmixread=70
invalidate=0	# mandatory
time_based=1
refill_buffers
###compression/dedupe related
#dedupe compress tests
#dedupe_percentage=80
#buffer_compress_percentage=10
#buffer_pattern=0xdeadface
ramp_time=180
write_bw_log=fio
write_iops_log=fio
write_lat_log=fio
log_avg_msec=6000
write_hist_log=fio
log_hist_msec=60000

[rbd_vol00]
rbdname=template-vol00
numjobs = 1
clientname=admin
pool=rbdpool


# One section per volume
[rbd_vol0X]
rbdname=template-vol0X
numjobs = 1
clientname=admin
pool=rbdpool
----


Example RipSAW/Bench-mark operator FIO file:


----

apiVersion: ripsaw.cloudbulldozer.io/v1alpha1
kind: Benchmark
metadata:
  name: fio-benchmark
  namespace: my-ripsaw
spec:
  elasticsearch:
    server: elastic-server.com
    port: 80
  clustername: test_2servers
  test_user: fio_user
  workload:
    name: "fio_distributed"
    args:
      image: registry/fio:latest
      prefill: true
      samples: 1
      servers: 2
      pin_server: ''
      jobs:
        - randwrite
      bs:
        - 4KiB
      numjobs:
        - 8
      iodepth: 8
      read_runtime: 600
      read_ramp_time: 5
      filesize: 2GiB
      log_sample_rate: 5000
      storageclass: ocs-block
      storagesize: 200Gi
      rook_ceph_drop_caches: True
      rook_ceph_drop_cache_pod_ip: IPPODCACHE
#######################################

#  EXPERT AREA - MODIFY WITH CAUTION  #

#######################################
#  global_overrides:
#     - ioengine=sync
#    - key=value
  job_params:
    - jobname_match: w
      params:
        - fsync_on_close=1
        - create_on_open=1
    - jobname_match: read
      params:
        - time_based=1
        - runtime={{ fiod.read_runtime }}
        - ramp_time={{ fiod.read_ramp_time }}
    - jobname_match: rw
      params:
        - rwmixread=70
        - time_based=1
        - runtime={{ fiod.read_runtime }}
        - ramp_time={{ fiod.read_ramp_time }}
    - jobname_match: readwrite
      params:
        - rwmixread=70
        - time_based=1
        - runtime={{ fiod.read_runtime }}
        - ramp_time={{ fiod.read_ramp_time }}
----


