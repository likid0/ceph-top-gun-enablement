= Block/File Testing

//++++
//<link rel="stylesheet"  href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/3.1.0/css/font-awesome.min.css">
//++++
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:sectnums:
:sectnumlevels: 6
:toc: left
:toclevels: 4

== Introduction

In this section we will be providing very high level guidelines on how to conduct a block/file benchmark of Ceph

== Tools

* *OCP/ODF*
** If you have a K8s/OCP cluster at your disposal for any kind of Block or File IO testing, we would recommend using the https://github.com/cloud-bulldozer/benchmark-operator[BenchMark-Operator]
*** It has all the tools you will need to baseline and benchmark your Ceph Cluster: `Iperf3, FIO, Smallfile`
** There is also a nice tool for K8s/OCP cluster benchmarking that is very easy to get started with: https://github.com/manojtpillai/kubuculum
* *Ceph StandAlone*
** If you don't have OCP at your disposal:
*** https://github.com/ceph/cbt[CBT] is a great framework for benchmarking Ceph.
**** Nice Repo from Karan helping out setting up https://github.com/ksingh7/ceph-cbt[CBT]
*** Another tool that works fine and that can aggregate FIO data results is
https://github.com/distributed-system-analysis/pbench/blob/main/agent/bench-scripts/pbench-fio.md[pbench-fio]

== Use RadosBench only for Smoketests.

* Rados bench operates on entire objects, which adds additional latency and skews results
* Rados bench results are unreliable on large writes/reads
* Rados bench operates on AIOs, meaning it gets bottlenecked by CPU fast and you can’t workaround it
* You can run only single rados bench per pool on single host
* But it’s great to verify whether your cluster is operating at more-or-less expected performance, or what
performance expectations you should have

*Radosbench* Example:

Ceph includes the rados bench command to do performance benchmarking on a RADOS storage cluster. The command will execute a write test and two types of read tests. The --no-cleanup option is important to use when testing both read and write performance. By default the rados bench command will delete the objects it has written to the storage pool. Leaving behind these objects allows the two read tests to measure sequential and random read performance.

----
# ceph osd pool create testbench 64 64
# rados bench -p testbench 180 write --no-cleanup
Maintaining 16 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 180 seconds or 0 objects
Object prefix: benchmark_data_ceph-node01_28934
  sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
    0      16        16         0         0         0           -           0
    1      16        62        46   183.773       184    0.155345     0.29248
    2      16       118       102   203.836       224    0.300808    0.290263
    3      16       176       160   213.209       232    0.153399    0.281029
    4      16       238       222   221.897       248    0.458016    0.278409
    5      16       301       285    227.91       252    0.365664     0.27384
    6      16       362       346   230.586       244    0.168794    0.271074
    7      16       422       406   231.918       240    0.278761    0.268571
    8      16       491       475   237.421       276    0.503654    0.263729
    9      16       555       539   239.481       256    0.160842    0.260208
   10      16       622       606   242.328       268     0.22003     0.25984
   11      16       688       672   244.242       264    0.194828    0.258116
   12      16       747       731   243.552       236    0.288289    0.259725
   13      16       811       795   244.507       256    0.201756    0.258559
   14      16       874       858    245.04       252    0.227478    0.258762
   15      16       935       919   244.968       244    0.130362       0.259
   16      16       994       978   244.406       236    0.341293    0.259149
   17      16      1053      1037   243.895       236    0.101433    0.259835
   18      16      1107      1091   242.344       216    0.246856    0.261484
   19      16      1157      1141   240.114       200    0.272116    0.264294
2023-02-20T11:48:57.112118-0500 min lat: 0.0697342 max lat: 0.90435 avg lat: 0.265594

# rados bench -p testbench 180 seq
# rados bench -p testbench 180 rand
# rados -p testbench cleanup
----

== rbd bench

You can also do simple `block/RBD` benchmarking with the help of the `rbd
bench` command

----
rbd bench rbd {{ pool_name }}/{{ rbd_image_name }} --io-type {{ io_type }} --io-size {{ io_size }} --io-threads {{ io_threads }} --io-total {{ io_total }} --io-pattern {{ io_pattern }} --rw-mix-read {{ rw_mix_read }}'
----


Example:

----

# rbd create image1 --size 5G -p testbench
# rbd bench testbench/image1 --io-type write  --io-size 4096 --io-threads 4  --io-pattern rand
bench  type write io_size 4096 io_threads 4 bytes 1073741824 pattern random
  SEC       OPS   OPS/SEC   BYTES/SEC
    1      3040   3001.94    12 MiB/s
    2      3088   1490.82   5.8 MiB/s
    3      3132   1045.32   4.1 MiB/s
    4      3448   861.482   3.4 MiB/s
    5      7048   1309.79   5.1 MiB/s
...
   54    252212   4801.87    19 MiB/s
   55    257208   4873.89    19 MiB/s
elapsed: 56   ops: 262144   ops/sec: 4667.5   bytes/sec: 18 MiB/s
----

[TIP]
====
Don't forget about the iotop rbd feature:
----
# rbd perf image iotop -p testbench
----
====

== When Bechmarking in OCP, Avoiding OCP range limits, limits and quotas

IMPORTANT: Before running any performance test, If using OCP as a client, make sure the namespace where your fio workloads are running does not have limits or quotas.

Delete or disable the following OpenShift features to ensure no resource limitation from the client's perspective:


* Limitranges.
* Namespace limits.
* Namespace quota.


== Client/Load Generators resources


Having Enough client resources to saturate the Ceph cluster is pivotal to
achieving a positive output of the benchmarking. If the clients don't have
enough firepower to saturate some part of the Ceph cluster, you will not be able
to know the limits of your ceph cluster.


== Always Drop Caches between tests


Almost all of the benchmarking suites have a hook that you can execute before
each test so all the caches can be dropped, achieving a deterministic result on
every test.


== Run the tests with a considerable dataset size


When running tests, consider creating a dataset that is bigger than the RAM
size, ideally, the total RAM of all nodes would be less than 20% of the total
size of the test dataset.

With this, we are trying to avoid the Linux filesystem caching
skewing our results and ending up with a false report of higher performance.


== Consider running the test with an almost empty cluster but also with 50%/60% used capacity


It's great to have the numbers IOPS for an empty cluster, but it's
also important to have a more realistic view of how your cluster will perform
with storage used capacity of around 50/60% that will be closer to a real
life scenario.

== Run the same test several times


Each test run needs to be run a minimum of three times, using
the mathematical average across all test runs. This will help us get rid of
outliers in testing.

An example would be:


* Three test iterations executed for 20 minutes
* with a two-minute ramp-up time
** for a total of 22-minute per test
* an iteration or 66 minutes per pass. 
* Before each iteration, the test script clears all Linux filesystem caches. 


== Configure a ramp-up time


Before each run, it's recommended to have a ramp-up time so every test finds
the caches at the same point, providing fair results in all the test runs.


== Use the same ceph configuration that you will have in production


When running benchmarks, we are always on a quest to achieve more IOPS, and more Throughput in that ques we can be tempted to modify the ceph configuration in certain ways that will allow increasing the performance, IOPS count, total throughput, but if the configuration we are using during the benchmark is not production friendly we won’t have valuable data in our hands, and example:


* Removing cephx authentication
* Using replica x 2, when we will use replica x 3 in prod
* Disabling bluestore CRC checking


== Looking for the best fio workload


First, we must find the best fio workload and ensure all performance tests are executed with the same workload to compare apples with apples.

Ideally, we need a fio workload that can saturate the ceph cluster while ensuring low latency and high cluster IOPS. For that reason, we will need to ensure the following:


* Different fio servers are deployed to spread the workload properly.
* Fio numjobs are set to 8 to ensure concurrency per fio server and increase the load in the ceph cluster.
* Total number of fio jobs will be: fio servers * fio numjobs
* Maintain a reasonable number of I/O units to keep in flight against the file (iodepth=8). If we increase this value too much, we will see a huge increase in operation latency while not increasing the cluster IOPS.
* To increase the number of tests, we have decided to run only one sample per test.
* To avoid the impact of Ceph thin-provisioning, all tests will prefill the RBD volumes before the test execution.
* Fio randwrite jobs will be used as the most demanding workload for the ceph cluster.
* Use single image for single client (exclusive lock kills performance)

After several tests, you should end up with a configuration that gives you the best results for your cluster


* 2 fio servers (one per OCP worker).
* 8 numjobs.
* IOdepth=8.


This is an example FIO file using the FIO RBD engine. No actual operating system mount happens when using this engine. 

NOTE: With FIO RBD engine, we have to use a job count of 1 per RBD volume. 

NOTE: With FIO RBD engine, use single image for single client (exclusive lock kills performance)

----
[global]
ioengine=rbd
clientname=admin
pool=rbdpool
#IO-Depth changes depending on the test
iodepth=$IODEPTH
runtime=600
direct=1
sync=0
buffered=0
#Blocksize changes depending on the test
bs=$BLOCKSIZE
#RR,RW, or a mixed workload, this changes depending on the test
rw=$TYPEOFTEST
norandommap
randrepeat=0
startdelay=15
rwmixread=70
invalidate=0	# mandatory
time_based=1
refill_buffers
###compression/dedupe related
#dedupe compress tests
#dedupe_percentage=80
#buffer_compress_percentage=10
#buffer_pattern=0xdeadface
ramp_time=180
write_bw_log=fio
write_iops_log=fio
write_lat_log=fio
log_avg_msec=6000
write_hist_log=fio
log_hist_msec=60000

[rbd_vol00]
rbdname=template-vol00
numjobs = 1
clientname=admin
pool=rbdpool


# One section per volume
[rbd_vol0X]
rbdname=template-vol0X
numjobs = 1
clientname=admin
pool=rbdpool
----


Example RipSAW/Bench-mark operator FIO file:


----

apiVersion: ripsaw.cloudbulldozer.io/v1alpha1
kind: Benchmark
metadata:
  name: fio-benchmark
  namespace: my-ripsaw
spec:
  elasticsearch:
    server: elastic-server.com
    port: 80
  clustername: test_2servers
  test_user: fio_user
  workload:
    name: "fio_distributed"
    args:
      image: registry/fio:latest
      prefill: true
      samples: 1
      servers: 2
      pin_server: ''
      jobs:
        - randwrite
      bs:
        - 4KiB
      numjobs:
        - 8
      iodepth: 8
      read_runtime: 600
      read_ramp_time: 5
      filesize: 2GiB
      log_sample_rate: 5000
      storageclass: ocs-block
      storagesize: 200Gi
      rook_ceph_drop_caches: True
      rook_ceph_drop_cache_pod_ip: IPPODCACHE
#######################################

#  EXPERT AREA - MODIFY WITH CAUTION  #

#######################################
#  global_overrides:
#     - ioengine=sync
#    - key=value
  job_params:
    - jobname_match: w
      params:
        - fsync_on_close=1
        - create_on_open=1
    - jobname_match: read
      params:
        - time_based=1
        - runtime={{ fiod.read_runtime }}
        - ramp_time={{ fiod.read_ramp_time }}
    - jobname_match: rw
      params:
        - rwmixread=70
        - time_based=1
        - runtime={{ fiod.read_runtime }}
        - ramp_time={{ fiod.read_ramp_time }}
    - jobname_match: readwrite
      params:
        - rwmixread=70
        - time_based=1
        - runtime={{ fiod.read_runtime }}
        - ramp_time={{ fiod.read_ramp_time }}
----

