= Ceph CephFS Deep Dive and Advanced Topics

//++++
//<link rel="stylesheet"  href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/3.1.0/css/font-awesome.min.css">
//++++
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:sectnums:
:sectnumlevels: 6
:toc: left
:toclevels: 4


== Ceph File System IO-PATH

All file data in CephFS is stored as RADOS objects. CephFS clients can directly access RADOS to operate on file data. MDS only handles metadata operations.

To read/write a CephFS file, client needs to have ‘file read/write’ capabilities for corresponding inode. If client does not have required capabilities, it sends a ‘cap message’ to MDS, telling MDS what it wants. MDS will issue capabilities to client when it is possible. Once client has ‘file read/write’ capabilities, it can directly access RADOS to read/write file data. File data are stored as RADOS objects in the form of <inode number>.<object index>. See ‘Data Striping’ section of Architecture for more information. If the file is only opened by one client, MDS also issues ‘file cache/buffer’ capabilities to the only client. The ‘file cache’ capability means that file read can be satisfied by client cache. The ‘file buffer’ capability means that file write can be buffered in client cache.

image:::cephfs_io_path.png[cephfs layout,840,680]

== Cephfs File Layouts

The layout of a file controls how its contents are mapped to Ceph RADOS objects. You can read and write a file’s layout using virtual extended attributes or xattrs.

The name of the layout xattrs depends on whether a file is a regular file or a directory. Regular files’ layout xattrs are called ceph.file.layout, whereas directories’ layout xattrs are called ceph.dir.layout. Where subsequent examples refer to ceph.file.layout, substitute dir as appropriate when dealing with directories.

Layout fields
-------------

*pool*

String, giving ID or name. String can only have characters in the set [a-zA-Z0-9\_-.]. Which RADOS pool a file's data objects will be stored in.

*pool_id*

String of digits. This is the system assigned pool id for the RADOS pool whenever it is created.

*pool_name*

String, given name. This is the user defined name for the RADOS pool whenever user creates it.

*pool_namespace*

String with only characters in the set [a-zA-Z0-9\_-.].  Within the data pool, which RADOS namespace the objects will
    be written to.  Empty by default (i.e. default namespace).

*stripe_unit*

Integer in bytes.  The size (in bytes) of a block of data used in the RAID 0 distribution of a file. All stripe units for a file have equal size. The last stripe unit is typically incomplete–i.e. it represents the data at the end of the file as well as unused “space” beyond it up to the end of the fixed stripe unit size.


*stripe_count*

Integer.  The number of consecutive stripe units that constitute a RAID 0 “stripe” of file data.

*object_size*

Integer in bytes.  File data is chunked into RADOS objects of this size.


[NOTE]
====
RADOS enforces a configurable limit on object sizes: if you increase CephFS object sizes beyond that limit then writes may not succeed. The OSD setting is osd_max_object_size, which is 128MB by default. Very large RADOS objects may prevent smooth operation of the cluster, so increasing the object size limit past the default is not recommended.
====


== Cephfs Layouts Hands-On

From the `workstation` server we first we need to install the `attr` RPM so we have access to the `setfattr` and
`getfattr` commands

----
# dnf install -y attr
----

We mount the cepfs filesystem at the root level with user id 0, this user has
Read/Write client caps at the / level

----
mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/ /mnt -o name=0,secret="AQA+KrxjWUovChAACWGj0YUbUEZHSKmNtYxriw=="
----

We can check the layout attributes of the root filesystem 

----
# getfattr -n ceph.dir.layout /mnt
# file: mnt
ceph.dir.layout="stripe_unit=4194304 stripe_count=1 object_size=4194304 pool=cephfs.fs_name.data"
----

Because user `0` was created with the following client capabilities.

----
# ceph auth get client.0
[client.0]
	key = AQA+KrxjWUovChAACWGj0YUbUEZHSKmNtYxriw==
	caps mds = "allow rw fsname=fs_name"
	caps mon = "allow r fsname=fs_name"
	caps osd = "allow rw tag cephfs data=fs_name"
----

If we try to modify the attribute for the / cephfs filesystem we will get
permision denied

----
# setfattr -n ceph.dir.layout.stripe_count -v 2 /mnt
setfattr: /mnt: Permission denied
----

TIP: Users need the 'P' client capability to be able to modify FS attributes
and quotas


Lets create a new cephx user that has the `P` capability flag to modify
attributes in the `/dir4` folder

----
# mkdir /mnt/dir4
#  ceph fs authorize fs_name client.4 / rw /dir4 rwp
[client.4]
	key = AQBmK71j0FcKERAAJqwhXOHoucR+iY0nzGV9BQ==
----

Now lets re-mount the FS using user with id `4` that we just created

----
# umount /mnt
[root@workstation-lb1719 ~]# mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/ /mnt -o name=4,secret="AQBmK71j0FcKERAAJqwhXOHoucR+iY0nzGV9BQ=="
----

Let's modify the attributes of a file in dir4

----
# touch /mnt/dir4/file1
# setfattr -n ceph.file.layout.stripe_count -v 2 /mnt/dir4/file1
# getfattr -n ceph.file.layout /mnt/dir4/file1 
# file: mnt/dir4/file1
ceph.file.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 pool=cephfs.fs_name.data"
----

NOTE: Files inherit the layout of their parent directory at creation time. However, subsequent changes to the parent directory’s layout do not affect children.

NOTE: Files created as descendents of the directory also inherit the layout, if the intermediate directories do not have layouts set

=== Creating files in different Rados Namespaces

Using the dir layout we can select a rados namespace for a directory

----
# mkdir /mnt/dir4/dir-namespace
# setfattr -n ceph.dir.layout.pool_namespace -v client4 /mnt/dir4/dir-namespace
# getfattr -n ceph.dir.layout /mnt/dir4/dir-namespace
# file: mnt/dir4/dir-namespace
ceph.dir.layout="stripe_unit=4194304 stripe_count=1 object_size=4194304 pool=cephfs.fs_name.data pool_namespace=client4"
----

Let's mount at the new directory level and create a file 

----
# umount /mnt
# mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/dir4/dir-namespace /mnt -o name=4,secret="AQBmK71j0FcKERAAJqwhXOHoucR+iY0nzGV9BQ=="
# echo "Here we go" > /mnt/file-in-namespace
# getfattr -n ceph.file.layout /mnt/file-in-namespace
# file: mnt/file-in-namespace
ceph.file.layout="stripe_unit=4194304 stripe_count=1 object_size=4194304 pool=cephfs.fs_name.data pool_namespace=client4"
----

If we check at the rados level we can see that the new file is created in the
client4 rados namespace, there is no data on the default namespace for the data
cephfs pool

----
# rados ls -p cephfs.fs_name.data
#
# rados ls -p cephfs.fs_name.data -N client4
10000000003.00000000
# echo "Here we go 2" > /mnt/file-in-namespace2
# rados ls -p cephfs.fs_name.data --all
client4	10000000003.00000000
client4	10000000004.00000000
----

[TIP]
====
How to map a cephfs file to a rados object?

----
# pwd
/mnt/dir4/dir-namespace
# ls -l
total 1
-rw-r--r--. 1 root root 11 Jan 10 05:50 file-in-namespace
-rw-r--r--. 1 root root 13 Jan 10 05:53 file-in-namespace2
# printf '%x\n' $(stat -c %i file-in-namespace)
10000000003
# rados ls -p cephfs.fs_name.data -N client4
10000000003.00000000
10000000004.00000000
----

====


=== Adding a different pool to a path in a filesystem

We are going to create a new EC cephfs data pool and use it in our cephfs
`fs_name` filesystem

----
# ceph osd pool create cephfs-data-ec 16 erasure
pool 'cephfs-data-ec' created
# ceph osd pool set cephfs-data-ec allow_ec_overwrites true
set pool 5 allow_ec_overwrites to true
# ceph osd pool application enable cephfs-data-ec cephfs
enabled application 'cephfs' on pool 'cephfs-data-ec'
----

Lets add the new pool to our filesystem

----
# ceph fs add_data_pool fs_name cephfs-data-ec
added data pool 5 to fsmap
----

We create a new user that is going to have permissions to access the new
filesystem directory that will map to the new EC pool we created

----
# ceph auth get-or-create client.ec mon 'allow r' mds 'allow r, allow rw path=/clientec' osd 'allow rw pool=cephfs-data-ec'
[client.ec]
	key = AQDpSr1jQm/yGBAAst+6elZFUY3BIZqeySZZ+w==
----

Let's mount the Cephfs root filesystem with the admin key, so we can modify the
layout properties for the fs directory `/clientec`


----
# umount /mnt
# cat /etc/ceph/ceph.client.admin.keyring | grep key
	key = AQDQBLxjayHzNRAAfyt3lqvdAK6kQLN4QgQTSg==
# mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/ /mnt -o name=admin,secret="AQDQBLxjayHzNRAAfyt3lqvdAK6kQLN4QgQTSg=="
----

Let's create the directory `/clientec` and change the attributes, so we use a
rados namespace called `clientec` inside the new pool `cephfs-data-ec`

----
# mkdir /mnt/clientec
# setfattr -n ceph.dir.layout.pool_namespace -v clientec /mnt/clientec
# setfattr -n ceph.dir.layout.pool -v cephfs-data-ec /mnt/clientec
----

Now let's mount the direcory with the client.ec we created before, and by
creating a file we can check that the file went to the rados namespace called `clientec` inside the new pool `cephfs-data-ec`

----
# mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/clientec /mnt -o name=ec,secret="AQDpSr1jQm/yGBAAst+6elZFUY3BIZqeySZZ+w=="
# echo "Here we go 2" > /mnt/file-in-ecpool
# rados ls -p cephfs-data-ec --all
clientec	10000000006.00000000
----


== CephFS Quotas

=== *Quota Restrictions*

Quotas are cooperative and non-adversarial. CephFS quotas rely on the cooperation of the client who is mounting the file system to stop writers when a limit is reached. A modified or adversarial client cannot be prevented from writing as much data as it needs. Quotas should not be relied on to prevent filling the system in environments where the clients are fully untrusted.

Quotas are imprecise. Processes that are writing to the file system will be stopped a short time after the quota limit is reached. They will inevitably be allowed to write some amount of data over the configured limit. How far over the quota they are able to go depends primarily on the amount of time, not the amount of data. Generally speaking writers will be stopped within 10s of seconds of crossing the configured limit.

Quotas are implemented in the kernel client 4.17 and higher. Quotas are supported by the userspace client (libcephfs, ceph-fuse). Linux kernel clients >= 4.17 support CephFS quotas but only on mimic+ clusters. Kernel clients (even recent versions) will fail to handle quotas on older clusters, even if they may be able to set the quotas extended attributes.

Quotas must be configured carefully when used with path-based mount restrictions. The client needs to have access to the directory inode on which quotas are configured in order to enforce them. If the client has restricted access to a specific path (e.g., /home/user) based on the MDS capability, and a quota is configured on an ancestor directory they do not have access to (e.g., /home), the client will not enforce it. When using path-based access restrictions be sure to configure the quota on the directory the client is restricted too (e.g., /home/user) or something nested beneath it.

=== *Example*

As the admin user, let's mount the Filesystem

----
# umount /mnt
# mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/ /mnt -o name=admin,secret="AQDQBLxjayHzNRAAfyt3lqvdAK6kQLN4QgQTSg=="
----

And we set a quota of 10MB max in size and a max of 10 files on the root of
the filesystem

----
# setfattr -n ceph.quota.max_bytes -v 10000000 /mnt
# setfattr -n ceph.quota.max_files -v 10 /mnt
# getfattr -n ceph.quota.max_bytes /mnt
ceph.quota.max_bytes="10000000"
----

If we check with the `df` command we can see the available space is the one we have specified with the quota

----
# df -h /mnt/clientec
Filesystem                     Size  Used Avail Use% Mounted on
192.168.56.61,192.168.56.62:/  8.0M     0  8.0M   0% /mnt
----

If we now try to exeed the file count quota or max space, we can see it takes some time to
sync and block the writes as it specifies in the second point of the quota
restrictions

----
# for i in {1..1000};do touch /mnt/file-${i}.txt;done
touch: cannot touch '/mnt/file-503.txt': Disk quota exceeded
touch: cannot touch '/mnt/file-504.txt': Disk quota exceeded

# dd if=/dev/zero of=/mnt/test-quota bs=1M count=100
dd: error writing '/mnt/test-quota': Disk quota exceeded
14+0 records in
13+0 records out
13631488 bytes (14 MB, 13 MiB) copied, 0.0367228 s, 371 MB/s

# df -h /mnt/
Filesystem                     Size  Used Avail Use% Mounted on
192.168.56.61,192.168.56.62:/  8.0M  8.0M     0 100% /mnt
----

To remove the Quotas

----
# setfattr -n ceph.quota.max_files -v 0 /mnt/
# setfattr -n ceph.quota.max_bytes -v 0 /mnt/
----

== MDS High Availability

If an MDS daemon stops communicating with the cluster’s monitors, the monitors will wait mds_beacon_grace seconds (default 15) before marking the daemon as laggy. If a standby MDS is available, the monitor will immediately replace the laggy daemon.

Each file system may specify a minimum number of standby daemons in order to be considered healthy. This number includes daemons in the standby-replay state waiting for a rank to fail. Note that a standby-replay daemon will not be assigned to take over a failure for another rank or a failure in a different CephFS file system). The pool of standby daemons not in replay counts towards any file system count.

Each file system may set the desired number of standby daemons by setting the
`ceph fs set <fs name> standby_count_wanted <count>` command.

By default it's set to 1, if we for example increase it to two, wi will get a
new MDS daemon deployed as standby

----
# ceph fs get fs_name | grep standby_count_wanted
standby_count_wanted	1
# ceph fs status
fs_name - 1 clients
=======
RANK  STATE            MDS               ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active  fs_name.proxy01.bbnkcu  Reqs:    0 /s   520     21     15      1
        POOL           TYPE     USED  AVAIL
cephfs.fs_name.meta  metadata  10.5M  9695M
cephfs.fs_name.data    data    39.0M  9695M
   cephfs-data-ec      data    12.0k  18.9G
       STANDBY MDS
fs_name.ceph-node01.uygvno  <------ We have one MDS daemon as standby
MDS version: ceph version 16.2.8-85.el8cp (0bdc6db9a80af40dd496b05674a938d406a9f6f5) pacific (stable)
----

Because we configured the MDS service placement to a count of two and with
dedicated hosts `ceph-node01,proxy01`, ceph can't add a new MDS daemon to
fulfill the requirement of two `standby_count_wanted` per Filesystem.

----
# ceph -s
  cluster:
    id:     e6c62efc-9016-11ed-b206-2cc26078e4ef
    health: HEALTH_WARN
            insufficient standby MDS daemons available
----

We are going to modify our MDS service configuration, to add one more host into
the placement, let's use a spec file for a change.


----
# ceph orch ls mds mds.fs_name --export | tee mds.yaml
service_type: mds
service_id: fs_name
service_name: mds.fs_name
placement:
  count: 2
  hosts:
  - ceph-node01
  - proxy01
----

Once we have exported the config to a file open with an editor and increment
the `count: 3` and the `hosts:` with ceph-node02

----
# vi mds.yaml
# cat mds.yaml
service_type: mds
service_id: fs_name
service_name: mds.fs_name
placement:
  count: 3
  hosts:
  - ceph-node01
  - ceph-node02
  - proxy01
----

Apply the config(we can use dry-run option if we want)

----
ceph orch apply -i mds.yaml
Scheduled mds.fs_name update...
----

Lets check if we now have 2 standby MDS demons

----
# ceph -s | grep mds
    mds: 1/1 daemons up, 2 standby
# ceph orch ps | grep mds
mds.fs_name.ceph-node01.uygvno  ceph-node01               running (10h)     8m ago   7h    37.3M        -  16.2.8-85.el8cp  b2c997ff1898  d6169aee0209  
mds.fs_name.ceph-node02.kxxoad  ceph-node02               running (68s)    63s ago  68s    32.7M        -  16.2.8-85.el8cp  b2c997ff1898  6ba364178785  
mds.fs_name.proxy01.bbnkcu      proxy01                   running (14h)     9m ago   6h    70.5M        -  16.2.8-85.el8cp  b2c997ff1898  df91160e7fa6
# ceph fs status
fs_name - 1 clients
=======
RANK  STATE            MDS               ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active  fs_name.proxy01.bbnkcu  Reqs:    0 /s   520     21     15      1
        POOL           TYPE     USED  AVAIL
cephfs.fs_name.meta  metadata  10.5M  9694M
cephfs.fs_name.data    data    39.0M  9694M
   cephfs-data-ec      data    12.0k  18.9G
       STANDBY MDS
fs_name.ceph-node01.uygvno    <----------- 2 standby 
fs_name.ceph-node02.kxxoad    <-----------
MDS version: ceph version 16.2.8-85.el8cp (0bdc6db9a80af40dd496b05674a938d406a9f6f5) pacific (stable)
----


=== MDS configure Standby-replay

Each CephFS file system may be configured to add standby-replay daemons. These standby daemons follow the active MDS’s metadata journal in order to reduce failover time in the event that the active MDS becomes unavailable. Each active MDS may have only one standby-replay daemon following it.

[TIP]
====
Once an MDS has entered the standby-replay state, it will only be used as a standby for the rank that it is following. If another rank fails, this standby-replay daemon will not be used as a replacement, even if no other standbys are available. For this reason, it is advised that if standby-replay is used then every active MDS should have a standby-replay daemon.
====

Configuration of standby-replay on a file system is done using the below:

----
# ceph fs set fs_name allow_standby_replay true
# ceph -s | grep mds
    mds: 1/1 daemons up, 1 standby, 1 hot standby
# ceph fs status
fs_name - 0 clients
=======
RANK      STATE                  MDS                 ACTIVITY     DNS    INOS   DIRS   CAPS
 0        active      fs_name.ceph-node01.uygvno  Reqs:    0 /s  1022     21     15      0
0-s   standby-replay    fs_name.proxy01.bbnkcu    Evts:    0 /s  1013     12      6      0  <-------- New Standby reply for Rank 0
        POOL           TYPE     USED  AVAIL
cephfs.fs_name.meta  metadata  10.5M  9693M
cephfs.fs_name.data    data    39.0M  9693M
   cephfs-data-ec      data    12.0k  18.9G
       STANDBY MDS
fs_name.ceph-node02.kxxoad
----

=== MDS Stand-by Affinity
 
When failing over MDS daemons, a cluster’s monitors will prefer standby daemons with mds_join_fs equal to the file system name with the failed rank. If no standby exists with mds_join_fs equal to the file system name, it will choose an unqualified standby (no setting for mds_join_fs) for the replacement, or any other available standby, as a last resort. Note, this does not change the behavior that standby-replay daemons are always selected before other standbys.


We can check our current status with the `ceph fs dump` command, our standby
daemon `ceph-node02.kxxoad` currently doesn't have any affinity defined

----
# ceph fs dump | tail -7
[mds.fs_name.ceph-node01.uygvno{0:144102} state up:active seq 22 join_fscid=1 addr [v2:192.168.56.61:6800/1486188007,v1:192.168.56.61:6801/1486188007] compat {c=[1],r=[1],i=[7ff]}]
[mds.fs_name.proxy01.bbnkcu{0:144104} state up:standby-replay seq 1 join_fscid=1 addr [v2:192.168.56.24:6800/2208661102,v1:192.168.56.24:6801/2208661102] compat {c=[1],r=[1],i=[7ff]}]

Standby daemons:
[mds.fs_name.ceph-node02.kxxoad{-1:144140} state up:standby seq 1 join_fscid=1 addr [v2:192.168.56.62:6800/331662506,v1:192.168.56.62:6801/331662506] compat {c=[1],r=[1],i=[7ff]}]
----

Let's set the affinity for `ceph-node02.kxxoad` to our FS `fs_name`

----
# ceph config set mds.fs_name.ceph-node02.kxxoad  mds_join_fs fs_name
----

== Running more than one Active MDS per Filesystem

You should configure multiple active MDS daemons when your metadata performance is bottlenecked on the single MDS that runs by default.

Adding more daemons may not increase performance on all workloads. Typically, a single application running on a single client will not benefit from an increased number of MDS daemons unless the application is doing a lot of metadata operations in parallel.

Workloads that typically benefit from a larger number of active MDS daemons are those with many clients, perhaps working on many separate directories.

*Example*

First let's reduce the count of wanted stand by daemos, currently we have it
set to `2`

----
# ceph fs set fs_name  standby_count_wanted 1
----

Now let's modify the count and placement in the mds service spec file, I'm
taking out the host `ceph-node02` from the list:

----
# vi mds.yaml
# cat mds.yaml
service_type: mds
service_id: fs_name
service_name: mds.fs_name
placement:
  count: 2
  hosts:
  - ceph-node01
  - proxy01
# ceph orch apply -i mds.yaml
Scheduled mds.fs_name update...
----

Now we can check that we only have 2 daemons, one of them as a hot-standby

----
# ceph orch ps | grep mds
mds.fs_name.ceph-node01.uygvno  ceph-node01               running (56m)     5m ago  17h    39.4M        -  16.2.8-85.el8cp  b2c997ff1898  4711409b661d
mds.fs_name.proxy01.bbnkcu      proxy01                   running (56m)     5m ago  17h    53.3M        -  16.2.8-85.el8cp  b2c997ff1898  5030c8246c68
# ceph -s | grep mds
    mds: 1/1 daemons up, 1 hot standby
----

Now we can start with the configuration of a new Active MDS for our FS.

----
# ceph fs get fs_name | grep max_mds
max_mds	1
# ceph fs set fs_name max_mds 2
# ceph -s
  cluster:
    id:     e6c62efc-9016-11ed-b206-2cc26078e4ef
    health: HEALTH_WARN
            1 filesystem is online with fewer MDS than max_mds
----

We are missing one MDS daemon, to have 2 active MDS daemons, as we requested. Let's modify the spec again

----
# vi mds.yaml
# cat mds.yaml
service_type: mds
service_id: fs_name
service_name: mds.fs_name
placement:
  count: 3
  hosts:
  - ceph-node01
  - ceph-node02
  - proxy01
# ceph orch apply -i mds.yaml
Scheduled mds.fs_name update...
# ceph -s | grep mds
    mds: 2/2 daemons up, 1 hot standby
# ceph fs status
fs_name - 0 clients
=======
RANK      STATE                  MDS                 ACTIVITY     DNS    INOS   DIRS   CAPS  
 0        active      fs_name.ceph-node01.uygvno  Reqs:    0 /s  1022     21     15      0   
 1        active      fs_name.ceph-node02.jsvbzq  Reqs:    0 /s    10     13     11      0   
0-s   standby-replay    fs_name.proxy01.bbnkcu    Evts:    0 /s  1013     12      6      0   
----

[TIP] 
====
- *dns*: dentries. Dentries is a data structure which represents a directory or a folder.
- *inos*: inodes. Inode is a data structure which provides a representation of a file
====


[TIP]
====
Once an MDS rank is assigned to a daemon, it proceeds through a series of states to start:
replay – replaying journal
resolve – disambiguating perhaps-incomplete MDCache operations such as import/export
reconnect – waiting for clients to send reconnect messages
rejoin – bring cache into consistent state with peers (load)
clientreplay – execute part-done client requests
active – normal operation
====


Currently Rank0 with MDS `fs_name.ceph-node01.uygvno` has a standby daemon, but
out new MDS `fs_name.ceph-node02.jsvbzq` with rank1 has not standy daemon,
let's add one more MDS for standby.

NOTE: Even with multiple active MDS daemons, a highly available system still requires standby daemons to take over if any of the servers running an active daemon fail.

----
# vi mds.yaml
# cat mds.yaml
service_type: mds
service_id: fs_name
service_name: mds.fs_name
placement:
  count: 4
  hosts:
  - ceph-node01
  - ceph-node02
  - ceph-node03
  - proxy01
# ceph orch apply -i mds.yaml
# ceph orch ps | grep mds
mds.fs_name.ceph-node01.uygvno  ceph-node01               running (67m)     5m ago  17h    41.3M        -  16.2.8-85.el8cp  b2c997ff1898  4711409b661d
mds.fs_name.ceph-node02.jsvbzq  ceph-node02               running (6m)      6m ago   6m    15.8M        -  16.2.8-85.el8cp  b2c997ff1898  83f83ab42547
mds.fs_name.ceph-node03.zamikx  ceph-node03               running (27s)    24s ago  27s    22.1M        -  16.2.8-85.el8cp  b2c997ff1898  a62184868686
mds.fs_name.proxy01.bbnkcu      proxy01                   running (66m)     5m ago  17h    53.5M        -  16.2.8-85.el8cp  b2c997ff1898  5030c8246c68
# ceph fs status
fs_name - 0 clients
=======
RANK      STATE                  MDS                 ACTIVITY     DNS    INOS   DIRS   CAPS
 0        active      fs_name.ceph-node01.uygvno  Reqs:    0 /s  1022     21     15      0
 1        active      fs_name.ceph-node02.jsvbzq  Reqs:    0 /s    10     13     11      0
0-s   standby-replay    fs_name.proxy01.bbnkcu    Evts:    0 /s  1013     12      6      0
1-s   standby-replay  fs_name.ceph-node03.zamikx  Evts:    0 /s     0      3      1      0
----

*In multiple active metadata server configurations* a balancer runs which works to spread metadata load evenly across the cluster. 

This usually works well enough for most users but sometimes it is desirable to override the dynamic balancer with explicit mappings of metadata to particular ranks. 

This can allow the administrator or users to evenly spread application load or limit impact of users’ metadata requests on the entire cluster.

image:::subtree-partitioning.svg[Subtree Partitioning,840,680]

Let's mount our FS with the admin user at the root of the FS

----
# mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/ /mnt -o name=admin,secret="AQDQBLxjayHzNRAAfyt3lqvdAK6kQLN4QgQTSg=="
# ls -l /mnt
total 13312
drwxr-xr-x. 2 root root        1 Jan 10 06:41 clientec
drwxr-xr-x. 3 root root        2 Jan 10 05:45 dir4
-rw-r--r--. 1 root root 13631488 Jan 10 16:17 test-quota
----

Let's configure static pinning, `dir4` will go to `rank0` and `clientec` to `rank1`

----
# setfattr -n ceph.dir.pin -v 1 /mnt/dir4/
# setfattr -n ceph.dir.pin -v 0 /mnt/clientec/
# getfattr -n ceph.dir.pin /mnt/dir4
# file: mnt/dir4
ceph.dir.pin="1"
----

The problem with traditional subtree partitioning is that the workload growth by depth (across a single MDS) leads to a hotspot of activity. This results in lack of vertical scaling and wastage of non-busy resources/MDSs.

This led to the adoption of a more dynamic way of handling metadata: Dynamic
Subtree Partitioning, where load intensive portions of the directory hierarchy
from busy MDSs are migrated to non busy MDSs. , check out more information on
this https://docs.ceph.com/en/quincy/cephfs/multimds/#setting-subtree-partitioning-policies[upstream doc]


== MDS Cache

While the data for inodes in a Ceph file system is stored in RADOS and accessed by the clients directly, inode metadata and directory information is managed by the Ceph metadata server (MDS). The MDS’s act as mediator for all metadata related activity, storing the resulting information in a separate RADOS pool from the file data.

The Metadata Server coordinates a distributed cache among all MDS and CephFS clients. The cache serves to improve metadata access latency and allow clients to safely (coherently) mutate metadata state (e.g. via chmod). The MDS issues capabilities and directory entry leases to indicate what state clients may cache and what manipulations clients may perform (e.g. writing to a file).

A capability grants the client the ability to cache and possibly manipulate some portion of the data or metadata associated with the inode. When another client needs access to the same information, the MDS will revoke the capability and the client will eventually return it, along with an updated version of the inode’s metadata (in the event that it made changes to it while it held the capability).


. *mds_cache_memory_limit*
This sets a target maximum memory usage of the MDS cache and is the primary tunable to limit the MDS memory usage. The MDS will try to stay under a reservation of this limit (by default 95%; 1 - mds_cache_reservation) by trimming unused metadata in its cache and recalling cached items in the client caches. 

----
# ceph config get mds.fs_name.ceph-node01.uygvno mds_cache_memory_limit
4294967296    ----> 4GB
----

. *mds_cache_reservation*
The cache reservation (memory or inodes) for the MDS cache to maintain. Once the MDS begins dipping into its reservation, it will recall client state until its cache size shrinks to restore the reservation.

----
# ceph config get mds.fs_name.ceph-node01.uygvno mds_cache_reservation
0.050000   ----> 5%
----

 
== FS Volumes and Subvolumes

A single source of truth for CephFS exports is implemented in the volumes module of the Ceph Manager daemon (ceph-mgr). The OpenStack shared file system service (manila), Ceph Container Storage Interface (CSI), storage administrators among others can use the common CLI provided by the ceph-mgr volumes module to manage the CephFS exports.

The ceph-mgr volumes module implements the following file system export abstractions:

.FS volumes, an abstraction for CephFS file systems
.FS subvolumes, an abstraction for independent CephFS directory trees
.FS subvolume groups, an abstraction for a directory level higher than FS subvolumes to effect policies

NOTE: Volumes and Subvolumes are used by rook in turn by ODF & Fusion/ODF.

----
# ceph fs volume ls
[
    {
        "name": "fs_name"
    }
]
# ceph fs subvolume create fs_name subvol1 --namespace-isolated
# ceph fs subvolume ls fs_name
[
    {
        "name": "subvol1"
    }
]
# ceph fs subvolume authorize  fs_name subvol1 cli_sub
# ceph fs subvolume authorized_list fs_name subvol1
[
    {
        "subvol1": "rw"
    }
]
# ceph auth get client.cli_sub
[client.cli_sub]
	key = AQCvfL5jF5NLGBAATtMf58oSE8KInuRqgG4Lug==
	caps mds = "allow rw path=/volumes/_nogroup/subvol1/8ee41860-a331-4aae-b95c-1ba0540ae883"
	caps mon = "allow r"
	caps osd = "allow rw pool=cephfs.fs_name.data namespace=fsvolumens_subvol1"
# ceph fs subvolume getpath fs_name subvol1
/volumes/_nogroup/subvol1/8ee41860-a331-4aae-b95c-1ba0540ae883
# mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/volumes/_nogroup/subvol1/8ee41860-a331-4aae-b95c-1ba0540ae883 /mnt -o name=cli_sub,secret="AQCvfL5jF5NLGBAATtMf58oSE8KInuRqgG4Lug=="
# touch /mnt/ok
#
----

== Snapshots
CephFS enables asynchronous snapshots by default These snapshots are stored in a hidden directory called .snap.

To create a snapshot, first mount the CephFS file system on your client node. 
Create a subdirectory inside the .snap directory. The snapshot name is the new subdirectory name. This
snapshot contains a copy of all the current files in the CephFS file system.

----
# umount /mnt
# mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/ /mnt -o name=admin,secret="AQDQBLxjayHzNRAAfyt3lqvdAK6kQLN4QgQTSg=="
# mkdir /mnt/.snap/snapshot1
# mkdir /mnt/.snap/snapshot1
# ls /mnt
clientec  dir4  test-quota  volumes
# ls /mnt/.snap/snapshot1
clientec  dir4  test-quota  volumes
# rm -Rf /mnt/.snap/snapshot1
----

TIP: We are using admin, but you need to authorize clients to make snapshots
for the CephFS file system with the `s` client capability option.

=== Schedule snapshots

The snap_schedule module is enabled with:

----
ceph mgr module enable snap_schedule
----

You can schedule a snapshot and check it's status with the help of the `ceph fs snap-schedule` command

----
# ceph fs snap-schedule add / 1h
Schedule set for path /
# ceph fs snap-schedule list /
/ 1h
# ceph fs snap-schedule status / | jq .
{
  "fs": "fs_name",
  "subvol": null,
  "path": "/",
  "rel_path": "/",
  "schedule": "1h",
  "retention": {},
  "start": "2023-01-11T00:00:00",
  "created": "2023-01-11T09:38:07",
  "first": null,
  "last": null,
  "last_pruned": null,
  "created_count": 0,
  "pruned_count": 0,
  "active": true
}
----

keep 24 snapshots at least an hour apart

----
# ceph fs snap-schedule retention add / h 24
Retention added to path /
----

NOTE: The snapshot schedule data is stored in a rados object in the cephfs metadata pool. At runtime all data lives in a sqlite database that is serialized and stored as a rados object.

== Client Sessions

If a client is misbehaving, or has died, we may want to evict it from the system
Eviction terminates the client's session with the MDS (the one where you ran the command), and blacklists the client: i.e. records its address in the OSDMap as an instruction to the OSDs to refuse I/O to the client.
Other MDS daemons also watch the OSDMap and kill their sessions with the client, and refuse to open new sessions with it.
Eviction is a “big hammer”: it's for when something has gone wrong, not a convenient way of unmounting clients.


Each client has a session with each MDS daemon to which it submits MDS requests.  MDS also stores a small metadata map for each session.  We can list these:

====
# ceph tell mds.0 client ls
2023-01-11T05:44:46.262-0500 7f0803fff700  0 client.144601 ms_handle_reset on v2:192.168.56.24:6800/2208661102
2023-01-11T05:44:46.284-0500 7f0803fff700  0 client.144659 ms_handle_reset on v2:192.168.56.24:6800/2208661102
[
    {
        "id": 144451,
        "entity": {
            "name": {
                "type": "client",
                "num": 144451
            },
            "addr": {
                "type": "v1",
                "addr": "192.168.56.252:0",
                "nonce": 797768304
            }
        },
        "state": "open",
        "num_leases": 0,
        "num_caps": 1,
        "request_load_avg": 0,
        "uptime": 4779.8243540920002,
        "requests_in_flight": 0,
        "num_completed_requests": 1,
        "num_completed_flushes": 0,
        "reconnecting": false,
        "recall_caps": {
            "value": 0,
            "halflife": 60
        },
        "release_caps": {
            "value": 0,
            "halflife": 60
        },
        "recall_caps_throttle": {
            "value": 0,
            "halflife": 1.5
        },
        "recall_caps_throttle2o": {
            "value": 0,
            "halflife": 0.5
        },
        "session_cache_liveness": {
            "value": 0,
            "halflife": 300
        },
        "cap_acquisition": {
            "value": 0,
            "halflife": 10
        },
        "delegated_inos": [],
        "inst": "client.144451 v1:192.168.56.252:0/797768304",
        "completed_requests": [
            {
                "tid": 9,
                "created_ino": "0x0"
            }
        ],
        "prealloc_inos": [],
        "client_metadata": {
            "client_features": {
                "feature_bits": "0x0000000000007bff"
            },
            "metric_spec": {
                "metric_flags": {
                    "feature_bits": "0x000000000000001f"
                }
            },
            "entity_id": "admin",
            "hostname": "workstation-lb1719.rhpds.opentlc.com",
            "kernel_version": "4.18.0-305.el8.x86_64",
            "root": "/"
        }
    }
]
====

MDS daemons will sometimes evict a client automatically, if the client has not communicated for a period of time.

Can be troublesome if the system has badly behaved clients:

. Clients with flaky networks
. Overloaded clients that go into swap-hell and stop responding

You can configure timeout `mds_session_timeout` (default 60s) and set `mds_session_blacklist_on_timeout` to false to attempt to let flaky clients rejoin after eviction.

== Deep Dive Metadata layout

Let's take a look at the CephFS metadata layout in RADOS.

TIP: `ceph daemon mds.<id> flush journal` first if your filesystem is new to flush the metadata

----
# rados -p cephfs.fs_name.meta ls
601.00000000
602.00000000
600.00000000
603.00000000
60a.00000000
613.00000000
1.00000000.inode
200.00000000
610.00000000
200.00000001
606.00000000
607.00000000
60e.00000000
mds1_openfiles.0
mds0_openfiles.0
60f.00000000
608.00000000
500.00000001
501.00000000
612.00000000
401.00000000
604.00000000
101.00000000.inode
500.00000000
mds_snaptable
605.00000000
101.00000000
mds1_sessionmap
mds0_inotable
611.00000000
100.00000000
mds0_sessionmap
60c.00000000
201.00000001
mds1_inotable
609.00000000
60b.00000000
60d.00000000
201.00000000
400.00000000
100.00000000.inode
1.00000000
----

*mds_snaptable* Tracks snapshots (obviously), One per filesystem: owned by rank 0, Map of snapid_t to SnapInfo Stored as normal object (not omap)
*Inode table* mds0_inotable/mds1_inotable. Free list of inode numbers (no actual inodes in here) One per MDS rank: each rank is granted 1/MAX_MDS share of overall space.
----
# cephfs-table-tool 0 show inode
{
    "0": {
        "data": {
            "version": 0,
            "inotable": {
                "projected_free": [
                    {
                        "start": 1099511627776,
                        "len": 1099511627776
                    }
                ],
                "free": [
                    {
                        "start": 1099511627776,
                        "len": 1099511627776
                    }
                ]
            }
        },
        "result": 0
    }
}
----

*Session Map* mds0_sessionmap/mds1_sessionmap. One per MDS rank Internal class
`SessionMapStore` Stored as OMAP, keys are `entity_name_t`, values are
`Session`

----
# cephfs-table-tool all show session 
{
    "0": {
        "data": {
            "sessions": []
        },
        "result": 0
    },
    "1": {
        "data": {
            "sessions": []
        },
        "result": 0
    }
}
----



*Journal Pointer objects* 400.00000000. MDS daemons have ability to rewrite their journals safely by writing out a fresh one and then switching the “pointer” object over before purging the old one.
*Journal objects* 200.00000000,200.00000001,200.0000000X .Object names increment as journal grows, multiple objects will exist depending on length of journal.  200.0000000 remains as journal header (contains expire/trim/write positions)


=== Objects from Data pool

Cephfs uses striping: like RBD, RGW.

It follows the patter: <inode number>.<stripe index>

----
# rados -p cephfs.fs_name.data ls
100000003f0.00000003
100000003f0.00000002
100000003f0.00000001
100000003f0.00000000
----

Controlled by `file_layout_t` , Layout can only be changed on empty files: once data is written it is not moved around.


== MDS Autoscaler
CephFS shared file systems require at least one active MDS service for correct operation, and
at least one standby MDS to ensure high availability. The MDS autoscaler module ensures the
availability of enough MDS daemons.
This module monitors the number of ranks and the number of standby daemons, and adjusts the
number of MDS daemons that the orchestrator spawns.

To enable the MDS autoscaler module.

----
# ceph mgr module enable mds_autoscaler
----

