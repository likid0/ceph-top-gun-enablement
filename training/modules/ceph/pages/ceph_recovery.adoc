= Ceph Recovery from OSD Failure


== Introduction

One of the most important features of Ceph is Self Healing and Automatic
rebalancing, by design Ceph has a Peer-to-peer architecture that seamlessly handles failures and ensures data distribution throughout the cluster.

== Detecting OSD Failures

Each Ceph OSD Daemon checks the heartbeat of other Ceph OSD Daemons at random
intervals(`osd_heartbeat_interval`) less than every 6 seconds, If a neighboring
Ceph OSD Daemon doesn’t show a heartbeat within a 20 second grace period(`osd_heartbeat_grace`), the Ceph OSD Daemon may consider the neighboring Ceph OSD Daemon down and report it back to a Ceph Monitor

[WARN]
====
Setting low values for the `osd_heartbeat_grace` Ceph parameter  is not recommended as with really low values the chances to generate false alarms for our Ceph cluster increase dramatically.
====

image:::ceph_osd_heartbeat.png[OSD/MON Heartbeat]

By default, two(`mon_osd_min_down_reporters`) Ceph OSD Daemons from different chrush subtrees(`mon_osd_reporter_subtree_level`) must report to the Ceph Monitors that another Ceph OSD Daemon is down before the Ceph Monitors acknowledge that the reported Ceph OSD Daemon is down

image:::ceph_2osds_out.png[OSD reporter]

If an Ceph OSD Daemon doesn’t report to a Ceph Monitor, the Ceph Monitor will
consider the Ceph OSD Daemon down after the mon osd report timeout elapses(`mon_osd_report_timeout`).

== When OSD failure happens

When a disk or storage node fails, Ceph self-healing process triggers (default
600 seconds,`mon_osd_down_out_interval`) and Ceph begins to automatically recover the unavailable PGs from other copies on other OSDs in the cluster. As a result, a node failure has several effects:

• Total cluster capacity is reduced by some fraction.
• Total cluster throughput is reduced by some fraction.
• The cluster enters an I/O-heavy recovery process, temporarily diverting an additional fraction of the available throughput.

The time required for the recovery process is directly proportional to how much data was on the failed node and how much throughput the rest of the cluster can sustain.

Because of the TCO and the performance impact of using a replica 3 policy, many RHCS all-flash clusters are being configured with pools using replica 2,  with only 2 copies of the data, it’s vital to test and measure how much time our cluster takes to recover from failure.

== Ceph recovery configuration options

Ceph has several configuration options to control the recovery process. We can make the recovery process resource-intensive and get faster recovery times, or make it less resource intensive and increase the recovery time. Since we are running with replica 2 we want to recover as fast as possible but finding just the right balance by which the intense use of resources during recovery won’t severely affect the running production workloads.

There are several parameters we can use to control the recovery process, we are going to briefly mention some of the most important parameters available, there are more knobs to control the recovery process.

=== Priorities

We can specify priorities for the client and recovery operations, by default the client has the maximum priority configured, and the recovery operations the least.

* *osd_client_op_priority:* This is the priority set for client operation
* *osd_recovery_op_priority:* This is the priority set for the recovery operation

We have to find a balance between the recovery time and how much we can affect clients IO.

=== Controlling the  Recovery Operations.

*Recovery*, this is log-based recovery ,where we  compare the PG logs and move over the objects which have changed. There are several parameters that help us  control the impact the recovery operations will have in our client IOPs.
 
* *osd_recovery_max_active:* The number of active recovery requests per OSD at a given moment.
* *osd_recovery_threads:* The number of threads needed for recovering data.

=== Controlling the Backfill Recovery Operations.

*Backfill*, requires us to incrementally move through the entire PG's hash space and compare the source PG’s with the destination PG’s

* *osd_max_backfills:* The maximum number of backfills allowed to or from a single OSD
* *osd_backfill_scan_min:* The minimum number of objects per backfill scan
* *osd_backfill_scan_max:* The maximum number of objects per backfill scan

=== Delaying the recovery/backfill Operations.

There is a parameter that introduces a sleep time between recovery operations, by default the recovery_sleep is 0 for ssds, but if we are running very sensitive workloads and we wan’t to severely reduce the impact on client IO we can use this parameter, it’s highly granular as we can configure sleep in decimals for example: 0.00001.

* *osd_recovery_sleep_ssd":* "0.000000"

== Ceph Configuration Parameters related to Error detection

=== How to prevent flapping OSD to come back to the cluster

Sometimes some of our OSDs may have problems and will flap in and out in the Ceph cluster. This is controlled by two parameters in Ceph:

* *osd_max_markdown_count:* Default value: 5.
* *osd_max_markdown_period:* Default value: 600 seconds.

The OSD that is reported down from its peers to the Ceph monitor 5 times in 10 minutes will be immediately marked as out to prevent flapping OSD. This will immediately trigger data rebalance in our Ceph cluster.

We can control this behaviour setting the parameters osd_max_markdown_count and osd_max_markdown_period appropriately.

* Global parameters:

[cols="^,^,^,^",options="header",]
|===
|Parameter |Description |Default value |Tuned value
|osd_heartbeat_grace |The elapsed time when a Ceph OSD Daemon has not
shown a heartbeat that the Ceph Storage Cluster considers it down |20
seconds |1/2/5/10/15 seconds

|osd_heartbeat_interval |How often an Ceph OSD Daemon pings its peers
(in seconds) |6 seconds |1 second

|mon_osd_adjust_heartbeat_grace |If set to true, Ceph will scale
parameter `osd_heartbeat_grace` based on laggy estimations |true |false

|osd_mon_report_interval |The number of seconds a Ceph OSD Daemon may
wait from startup or another reportable event before reporting to a Ceph
Monitor |5 seconds |1 second

|mon_client_ping_interval |The client will ping the monitor every N
seconds |10 seconds |1 second

|mon_client_ping_timeout |Timeout for monitor-client ping interaction
|30 seconds |3 seconds
|===

* Ceph Monitor parameters:
** When Monitor and OSDs are colocated in the same hosts, we have
observed that these parameters help to reduce I/O freeze upon unexpected
failure in one of the OSD nodes.

[cols="^,^,^,^",options="header",]
|===
|Parameter |Description |Default value |Tuned value
|mon_election_timeout |On election proposer, maximum waiting time for
all ACKs in seconds |5 seconds |1 second

|mon_lease_ack_timeout_factor |The monitor leader will wait `mon_lease`
* `mon_lease_ack_timeout_factor` for the providers to acknowledge the
lease extension |2.0 |1.001

|mon_accept_timeout_factor |The Leader will wait `mon_lease` *
`mon_accept_timeout_factor` for the requester(s) to accept a Paxos
update. It is also used during the Paxos recovery phase for similar
purposes |2.0 |1.001
|===

* Ceph OSD parameters:

[cols="^,^,^,^",options="header",]
|===
|Parameter |Description |Default value |Tuned value
|osd_client_watch_timeout |If the client loses its connection to the
primary OSD for a watched object, the watch will be removed after a
timeout configured with `osd_client_watch_timeout`. Watches are
automatically reestablished when a new connection is made, or a
placement group switches OSDs |30 seconds |10 seconds
|===

*WARNING:* Modifying the parameters described above can help to minimize
the I/O pause upon unexpected failure in one of the OSD nodes but not
completely resolve it. Also, setting these parameters with less than
default values will generate false alarms for Ceph clusters if there is
a situation like high load on nodes, network congestion is high or the
network quality is bad. Therefore these parameter changes should be
tested in a lab environment before putting into production.

*NOTE:* Tuning the parameters described above might increase both Ceph
resource consumption and Ceph network traffic.

*WARNING:* Changing default OSD heartbeat parameters is not supported. A
Support Exception (through a support case) is needed in order to change
these parameters in Ceph.

