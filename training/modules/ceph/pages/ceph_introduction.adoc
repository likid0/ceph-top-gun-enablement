= Ceph Introduction
//++++
//<link rel="stylesheet"  href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/3.1.0/css/font-awesome.min.css">
//++++
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:sectnums:
:sectnumlevels: 6
:toc: left
:toclevels: 4


== Understanding the storage micro-universe

=== Challenges

Storage in general faces a few challenges that have been around for quite some time:

* *Data growth* - Since information technology has been around data simply keeps growing. The move
from slow storage devices such as punch cards or tape simply accelerated the growth. The first reason
being that tapes occupy more physical space to store the same amount of data, even though
both tape and disk saw their density augment on a regular basis.
* *New technologies* - First generation computers only had to deal with character oriented data while
modern IT infrastructures deal with many different types of information combined with more
information sharing technologies.
* *Legacy* - IT evolutions force companies to preserve their existing investments (proprietary
hardware and solutions) while being able to respond to the growing demand and expansion via scale out
rather than scale up.

When dealing with evolutions, IT departments always have pay attention to the following items:

* *Storage costs* - Traditional storage infrastructure involve large budgets to install, upgrade and
maintain traditional hardware-driven solutions. This often leads to silos with the organization
with specific solutions for specific use cases (e.g. SAN vs NAS or Open vs Mainframe). Combined
with the sales strategy of traditional storage vendors it can very often lead to crazy expenses
(e.g. keeping a storage array under maintenance can be as expensive as getting a new storage
array).
* *Resource and time costs* - Traditional storage infrastructure will often require to have dedicated
teams to look after each system. Recruitment of versatile storage administrators can become tricky
when the job market becomes tensed and as we all know, exceptional professional lead to exceptional
packages.

Supporting different storage access types has always been a requirement for the past 25 years
in the open system based infrastructure. The following access types are recognized as the basic
types that any modern storage infrastructure must support:

* *Block access* - Storage is presented as a storage device and access directly through the operating system.
In most cases the device is formatted with a hierarchical filesystem to facilitate the consumption
an d the management of the device. This is traditional within the realm of SANs (Storage Area Networks).
* *File access* - Storage is presented over the network as readily consumable filesystem by a client application.
The filesystem is managed directly by the storage subsystem hosting the data. Various protocols are
used for this solution depending on the operating system deployed on the client side (NFS or Network FileSystem
for UNIX and Linux environments, CIFS or Common Internet FileSystem for Microsoft Windows environments).
* *Object access* - Storage is presented as a non structured collection of objects, each object being the
combination of a data portion, often referred to as a `blob`, and a metadata portion that will be used
to store the attributes of the object. Object storage is accessed via an API with each object being
assigned a unique ID. The first commercial object storage solution was marketed by EMC^TM^ as the Centera
solution after their acquisition of FilePool a Belgium based company. The ancestor of object storage is an
ICL solution developed in the 1960's called CAFS.

=== Software defined storage

SDS (Software Defined Storage) was designed as a way to avoid being locked into a proprietary hardware and to
offer the ability to scale a storage infrastructure using cheap commodity hardware such as x86 servers and
disk drives embarked into the servers.

To become a valid player in the storage field SDS had to evolve enough to offer some of the basic
features provided by proprietary storage infrastructures. It is also good to know that since the middle of
the 2000s, many proprietary storage platforms are in fact closed source SDS solutions that run on either
proprietary or common hardware.

* Support for all storage access types (block, file and object)
* Be fault tolerant and be SPOF proof
* Run on commodity hardware

This will provide the following benefits:

* Using commodity hardware will help maintain costs as low as possible
* Using commodity hardware is better suited for a scale out growth model
* Being software based allows the addition of features over time without the need of proprietary hardware
* Being software based enables the solution to easily be Open Source

== Ceph introduction

=== Timeline

The Ceph project has a long history as you can see in the timeline below.

.Ceph Project History
image::./images/ceph101-timeline.png[Ceph Project Timeline, align="center"]

It is a battle-tested software defined storage (SDS) solution that has been
available as a storage backend for OpenStack and Kubernetes for quite some
time.

Ceph version that have been around. This table includes the information any good `Cepher`
is used to discussed during conversations throughout the community.

[%autowidth,cols=3,cols="^,^,^",options=header]
|===
|Name|Version Number|Initial release
|`Argonaut`|0.48|2012-07-03
|`Bobtail`|0.56|2013-01-01
|`Cuttlefish`|0.61|2013-05-07
|`Dumpling`|0.67|2013-08-01
|`Emperor`|0.67|2013-11-01*
|`Firefly`|0.67|2014-05-01
|`Giant`|0.87|2014-10-01*
|`Hammer`|0.94|2015-04-01
|`Infernalis`|9.2|2015-11-01*
|`Jewel`|10.2|2016-04-01
|`Kraken`|11.2|2017-01-01*
|`Luminous`|12.2|2017-08-01
|`Mimic`|13.2|2018-06-01
|`Nautilus`|14.2|2019-03-19
|`Octopus`|15.2|2020-03-23
|`Pacific`|16.2|2021-03-31
|`Quincy`|17.2|2022-04-19
|===

NOTE: Between  `Dumpling` and `Luminous`, all the second half versions released were recognized
as development version (marked with a _*_) and never recommended for production use. Red Hat, after
its Inktank acquisition has only released Red Hat Ceph Storage using one in 2 versions (RHCS 5
being `Pacific` based, RHCS 4 being `Nautilus` based, RHCS 3 being `Luminous` based, RHCS 2 being
`Jewel` based, RHCS 1.3 being `Hammer` based. The first RHCS named release was 1.2 based on `Firefly`.

NOTE: Also note the change of release numbering with `Infernalis`. Remember
that the release cadence changed from 6 month to a 9 months one starting with `Mimic`. Although
`Kraken` and `Luminous` were released the same year it is only due to `Kraken` being delayed
for bug fixing reasons.

=== Goals

The goals of Ceph were set as follows:

* Every component is scalable
* No single point of failure (SPOF)
* Be a software based solution
* Be Open Source
* Run on commodity hardware
* Reduce interventions through self management wherever possible
