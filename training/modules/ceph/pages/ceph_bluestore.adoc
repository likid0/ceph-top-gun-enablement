= Bluestore

//++++
//<link rel="stylesheet"  href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/3.1.0/css/font-awesome.min.css">
//++++
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:sectnums:
:sectnumlevels: 6
:toc: left
:toclevels: 4


== Introduction

BlueStore is the default backend object store for the Ceph OSD daemons. The original object store, FileStore, requires a file system on top of raw block devices. Objects are then written to the file system. Unlike the original FileStore back end, BlueStore stores object directly on the block devices without any file system interface, which improves the performance of the cluster.

NOTE: Filestore has been Deprecated.

== Bluestore Architecture

Data is directly written to the raw block device and all metadata operations are managed by RocksDB. The device containing the OSD is divided between RocksDB metadata and the actual user data stored in the cluster.  User data objects are stored as blobs directly on the raw block device, once the data has been written to the block device, RocksDB metadata gets updated with the required details about the new data blobs.

image:::filestore-vs-bluestore-2.png[Filestore vs Bluestore]

RocksDB is an embedded high-performance key-value store that excels with flash storage, RocksDB can’t directly write to the raw disk device, it needs an underlying filesystem to store its persistent data, this is where BlueFS comes in, BlueFS is a Filesystem developed with the minimal feature set needed by RocksDB to store its .sst files.

RocksDB uses WAL as a transaction log on persistent storage, unlike Filestore where all the writes went first to the journal, in bluestore we have two different datapaths for writes, one where data is written directly to the block device and the other where we use deferred writes, with deferred writes data gets written to the WAL device and later asynchronously flushed to disk.

New Bluestore features:
* it enables compression of data at the lowest level.
** if compression is enabled data blobs allocated on the raw device will be compressed. This means that any data written into RH Ceph Storage, no matter the client used(rbd,rados, etc), can benefit from this feature.
* An additional benefit of BlueStore is that it stores data and meta-data in the cluster with checksums for increased integrity.
** Whenever data is read from persistent storage its checksum is verified

== Bluestore improvements

* Direct management of storage devices
BlueStore consumes raw block devices or partitions. This avoids any intervening layers of abstraction, such as local file systems like XFS, that might limit performance or add complexity.

* Metadata management with RocksDB
BlueStore uses the RocksDB’ key-value database to manage internal metadata, such as the mapping from object names to block locations on a disk.

* Full data and metadata checksumming
By default all data and metadata written to BlueStore are protected by one or more checksums. No data or metadata are read from the disk or returned to the user without verification.
Efficient copy-on-write

* The Ceph Block Device and Ceph File System snapshots rely on a copy-on-write clone mechanism that is implemented efficiently in BlueStore. This results in efficient I/O both for regular snapshots and for erasure-coded pools which rely on cloning to implement efficient two-phase commits.
No large double-writes

* BlueStore first writes any new data to unallocated space on a block device and then commits a RocksDB transaction that updates the object metadata to reference the new region of the disk. Only when the write operation is below a configurable size threshold, it falls back to a write-ahead journaling scheme, similar to how FileStore operates.

== Bluestore Layout

Bluestore has a number of possible storage layout configurations:

* The main device that stores the object data.
* An optional RocksDB device that stores the metadata
** A DB device (identified as block.db in the data directory) can be used for storing BlueStore’s internal metadata. BlueStore (or rather, the embedded RocksDB) will put as much metadata as it can on the DB device to improve performance. If the DB device fills up, metadata will spill back onto the primary device (where it would have been otherwise). Again, it is only helpful to provision a DB device if it is faster than the primary device.
* An optional WAL device that stores the transaction logs.
** A write-ahead log (WAL) device (identified as block.wal in the data directory) can be used for BlueStore’s internal journal or write-ahead log. It is only useful to use a WAL device if the device is faster than the primary device (e.g., when it is on an SSD and the primary device is an HDD).

For example, you could configure the main device on the HDD drive, this is
where the user data will be stored. We then configured the WAL and RocksDB
devices on a faster Flash NVMe drive, having WAL and RocksDB on a high-performing drive will give us more IOPS with lower latency.

We can check inside an OSD container
`/var/lib/ceph/910c8bb8-95bc-11ed-b7d6-2cc26078e4ef/osd.3/` dir to get the osd
layout. This OSD has been configured specifying the 3 devices block,block.db
and block.wal that is why we have 3 different devices for each

----
# ls -ltra /var/lib/ceph/910c8bb8-95bc-11ed-b7d6-2cc26078e4ef/osd.3/ | grep block
lrwxrwxrwx 1 ceph ceph  111 Jan 16 11:57 block -> /dev/mapper/ceph--f5a5ccbd--5dc2--44d3--9ea1--031153783317-osd--block--2f3561bc--81c5--483e--9cc2--cb1e3bb69dc8
lrwxrwxrwx 1 ceph ceph  108 Jan 16 11:57 block.db -> /dev/mapper/ceph--b6c2103e--6cc0--4040--b117--a5acfa756bc9-osd--db--987930d8--0aaa--4c8f--812b--edc12edb2160
lrwxrwxrwx 1 ceph ceph  109 Jan 16 11:57 block.wal -> /dev/mapper/ceph--d71f301f--cdd5--40f9--b7aa--8ee84c092d81-osd--wal--5de39adc--5a3e--419e--ab3f--3ae239dd2f78
----

The devices point to LVM device mappers on the Node, so each device is a
dedicated LVM logical volume, each lvol is using a separe VG with a dedicated
drive

----
# lvs
  LV                                             VG                                        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  osd-db-987930d8-0aaa-4c8f-812b-edc12edb2160    ceph-b6c2103e-6cc0-4040-b117-a5acfa756bc9 -wi-ao---- <10.00g
  osd-wal-5de39adc-5a3e-419e-ab3f-3ae239dd2f78   ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81 -wi-ao---- <10.00g
  osd-block-2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8 ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317 -wi-ao---- <10.00g

# pvs
  PV         VG                                        Fmt  Attr PSize   PFree
  /dev/vdc   ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317 lvm2 a--  <10.00g    0
  /dev/vdd   ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81 lvm2 a--  <10.00g    0
  /dev/vde   ceph-b6c2103e-6cc0-4040-b117-a5acfa756bc9 lvm2 a--  <10.00g    0
----

From inside the OSD container we can also use the `ceph-volume` command to
check-out the OSD layout details

----
# ceph-volume lvm list

====== osd.3 =======

  [db]          /dev/ceph-b6c2103e-6cc0-4040-b117-a5acfa756bc9/osd-db-987930d8-0aaa-4c8f-812b-edc12edb2160

      block device              /dev/ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317/osd-block-2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      block uuid                bP7bb6-RmSZ-2Un8-bj63-yJbR-RK21-KdYcDZ
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      db device                 /dev/ceph-b6c2103e-6cc0-4040-b117-a5acfa756bc9/osd-db-987930d8-0aaa-4c8f-812b-edc12edb2160
      db uuid                   2Lk2gH-Ud0M-SH2J-GNGE-4DfM-0xIN-CvJ8E0
      encrypted                 0
      osd fsid                  2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      osd id                    3
      osdspec affinity          osd_using_paths
      type                      db
      vdo                       0
      wal device                /dev/ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81/osd-wal-5de39adc-5a3e-419e-ab3f-3ae239dd2f78
      wal uuid                  VufD8W-g9uX-FIDd-2p6G-90hJ-nBYt-HZG5X7
      devices                   /dev/vde

  [wal]         /dev/ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81/osd-wal-5de39adc-5a3e-419e-ab3f-3ae239dd2f78

      block device              /dev/ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317/osd-block-2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      block uuid                bP7bb6-RmSZ-2Un8-bj63-yJbR-RK21-KdYcDZ
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      encrypted                 0
      osd fsid                  2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      osd id                    3
      osdspec affinity          osd_using_paths
      type                      wal
      vdo                       0
      wal device                /dev/ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81/osd-wal-5de39adc-5a3e-419e-ab3f-3ae239dd2f78
      wal uuid                  VufD8W-g9uX-FIDd-2p6G-90hJ-nBYt-HZG5X7
      devices                   /dev/vdd

  [block]       /dev/ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317/osd-block-2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8

      block device              /dev/ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317/osd-block-2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      block uuid                bP7bb6-RmSZ-2Un8-bj63-yJbR-RK21-KdYcDZ
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      db device                 /dev/ceph-b6c2103e-6cc0-4040-b117-a5acfa756bc9/osd-db-987930d8-0aaa-4c8f-812b-edc12edb2160
      db uuid                   2Lk2gH-Ud0M-SH2J-GNGE-4DfM-0xIN-CvJ8E0
      encrypted                 0
      osd fsid                  2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      osd id                    3
      osdspec affinity          osd_using_paths
      type                      block
      vdo                       0
      wal device                /dev/ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81/osd-wal-5de39adc-5a3e-419e-ab3f-3ae239dd2f78
      wal uuid                  VufD8W-g9uX-FIDd-2p6G-90hJ-nBYt-HZG5X7
      devices                   /dev/vdc
----

You can also use the `ceph-bluestore-tool` to check the labels

WARNING: The OSD process can't be running when using the  `ceph-bluestore-tool`
or the ceph-objectstore-tool tool, check steps on how to start an OSD container
without the OSD process for debugging xref:troubleshooting_bluestore.adoc[here].

----
# ceph-bluestore-tool show-label --dev /dev/mapper/ceph--f5a5ccbd--5dc2--44d3--9ea1--031153783317-osd--block--2f3561bc--81c5--483e--9cc2--cb1e3bb69dc8
{
    "/dev/mapper/ceph--f5a5ccbd--5dc2--44d3--9ea1--031153783317-osd--block--2f3561bc--81c5--483e--9cc2--cb1e3bb69dc8": {
        "osd_uuid": "2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8",
        "size": 10733223936,
        "btime": "2023-01-16T16:57:43.638530+0000",
        "description": "main",
        "bfm_blocks": "2620416",
        "bfm_blocks_per_key": "128",
        "bfm_bytes_per_block": "4096",
        "bfm_size": "10733223936",
        "bluefs": "1",
        "ceph_fsid": "910c8bb8-95bc-11ed-b7d6-2cc26078e4ef",
        "kv_backend": "rocksdb",
        "magic": "ceph osd volume v026",
        "mkfs_done": "yes",
        "osd_key": "AQAFgsVjR+/KKRAA8QmuVH4WJ9mlDaVcO91xVg==",
        "osdspec_affinity": "osd_using_paths",
        "ready": "ready",
        "require_osd_release": "16",
        "whoami": "3"
    }
}
----


== Bluestore Spill Over

Spill Over happens when RocksDB starts using the `slow` block device affecting
performance.

BlueFS wraps RocksDB understanding of the filesystem and transforms it into a structure well suited for block devices.

. *db is* allocated from device options.bluestore_block_db_path, (informal block.db)
. *db.slow* is allocated from device options.bluestore_block_path, (informal block)
. *db.wal* is allocated from device options.bluestore_wal_path, (informal block.wal)

During processing, RocksDB has a temporary higher demand for space. It asks to create a file on "/db/xxxx" but exhausts space on block.db and starts consuming block space. This redirection of allocation is done internally in BlueFS and RocksDB is unaware that the relocation occurred. This means that the file with name "/db/xxxxx" is located on a slow block device, so RocksDB still thinks it is fast. After the peak is gone, data allocated on block remains there, with a lot of space free on block.db.

*How to check?*

If you have spillover you can expect the "slow_bytes" values to be > 0.

----
# ceph daemon osd.0 perf dump bluefs | grep -E "db_|slow_"
         "db_total_bytes": 21470642176,
         "db_used_bytes": 179699712,
         "slow_total_bytes": 0,
         "slow_used_bytes": 0,
----

== Bluestore deployment Strategy

It's recommended to follow these guidelines:

* If all devices are the same type, for example, all rotational drives, and there are no fast devices to use for metadata, it makes sense to specify the block device only and do not separate block.db or block.wal.
* If you have a mix of fast and slow devices (SSD / NVMe and rotational), it is recommended to place block.db on the faster device while block (data) lives on the slower (spinning drive).

When using a mixed spinning and solid drive setup it is important to make a large enough block.db logical volume for BlueStore. Generally, block.db should have as large as possible logical volumes.

The general recommendation is to have block.db size in between 1% to 4% of block size. For RGW workloads, it is recommended that the block.db size isn’t smaller than 4% of the block device, because RGW heavily uses it to store metadata (omap keys). For example, if the block size is 1TB, then block.db shouldn’t be less than 40GB. For RBD workloads, 1% to 2% of block size is usually enough.

You can check detailed information on the OSD layout with the `ceph osd metadata command` 

----
# ceph osd metadata osd.0 | grep blue
    "bluefs": "1",
    "bluefs_dedicated_db": "0",
    "bluefs_dedicated_wal": "0",
    "bluefs_single_shared_device": "1",
    "bluestore_bdev_access_mode": "blk",
    "bluestore_bdev_block_size": "4096",
    "bluestore_bdev_dev_node": "/dev/dm-0",
    "bluestore_bdev_devices": "vdb",
    "bluestore_bdev_driver": "KernelDevice",
    "bluestore_bdev_partition_path": "/dev/dm-0",
    "bluestore_bdev_rotational": "1",
    "bluestore_bdev_size": "10733223936",
    "bluestore_bdev_support_discard": "0",
    "bluestore_bdev_type": "hdd",
    "osd_objectstore": "bluestore",
----

[TIP]
====
In older releases, internal level sizes mean that the DB can fully utilize only
specific partition / LV sizes that correspond to sums of L0, L0+L1, L1+L2, etc.
sizes, which with default settings means roughly 3 GB, 30 GB, 300 GB, and so
forth. Most deployments will not substantially benefit from sizing to
accommodate L3 and higher.

Improvements in the latest releases beginning with Nautilus 14.2.12 and Octopus 15.2.6 enable better utilization of arbitrary DB device sizes, and the Pacific release brings experimental dynamic-level support.
====

== Bluestore Cache

The BlueStore cache is a collection of buffers that, depending on configuration, can be populated with data as the OSD daemon does reading from or writing to the disk.

=== Bluestore Automatic Cache Sizing

BlueStore can be configured to automatically resize its caches when TCMalloc is configured as the memory allocator and the bluestore_cache_autotune setting is enabled. This option is currently enabled by default. BlueStore will attempt to keep OSD heap memory usage under a designated target size via the osd_memory_target configuration option. This is a best effort algorithm and caches will not shrink smaller than the amount specified by osd_memory_cache_min.

Automatic Cache sizing works great with most workloads, as such, we recommend
using it.

=== Bluestore Manual Cache Sizing

When bluestore_cache_autotune is disabled and bluestore_cache_size_ssd parameter is set, BlueStore cache gets subdivided into 3 different caches:

* *cache_meta:* used for BlueStore Onode and associated data.
* *cache_kv:* used for RocksDB block cache including indexes/bloom-filters
* *data cache:* used for BlueStore cache for data buffers.

The amount of space that goes to each cache is configurable using ratios, just
an example

----
bluestore_cache_autotune = 0
bluestore_cache_kv_ratio = 0.2
bluestore_cache_meta_ratio = 0.8
----

TIP: you can check per OSD memory usage details with the following command `ceph daemon osd.$OSD_ID dump_mempools"`

=== The importance of the Bluestore Onode Cache.

With all NVMe deployments, and especially with RBD workloads the size of the
bluestore cache can have a huge impact on performance. Onode caching in bluestore is hierarchical.  If an onode is not cached, it will be read from the DB disk, populated into the KV cache, and finally populated into the bluestore onode cache. As you can imagine having a direct hit in the Onode cache is much faster than reading from the disk or the KV cache.

When all onodes in a data set fit into bluestore's block cache, the onodes are never read from disk, and thus onodes never have to be populated into the KV cache at all.  This is the optimal scenario for RBD. On the other hand, a worst-case scenario is where you end up needing to read onodes from disk, you'll end up populating both the rocksdb KV cache and the bluestore onode cache with fresh data and force out older onodes, which may be read back in again from disk later.

== Bluestore Database Sharding

BlueStore can divide this data into multiple RocksDB column families. When keys have similar access frequency, modification frequency and lifetime, BlueStore benefits from better caching and more precise compaction. This improves performance, and also requires less disk space during compaction since each column family is smaller and can compact independently of others.

OSDs deployed in Pacific or later use RocksDB sharding by default. If Ceph is upgraded to Pacific from a previous version, sharding is off.

To check if sharding is enabled on your cluster

----
# ceph config get osd.1 bluestore_rocksdb_cf
true
# ceph config get osd.0 bluestore_rocksdb_cfs
m(3) p(3,0-12) O(3,0-13)=block_cache={type=binned_lru} L P
----

== Minimum Allocation Size.

There is a configured minimum amount of storage that BlueStore will allocate on an OSD. In practice, this is the least amount of capacity that a RADOS object can consume. The value of bluestore_min_alloc_size is derived from the value of bluestore_min_alloc_size_hdd or bluestore_min_alloc_size_ssd depending on the OSD’s rotational attribute.

Through the Mimic release, the default values were 64KB and 16KB for rotational (HDD) and non-rotational (SSD) media, respectively. Octopus changed the default for SSD (non-rotational) media to 4KB, and Pacific changed the default for HDD (rotational) media to 4 KB.

For example, when an RGW client stores a 1KB S3 object, it is written to a single RADOS object. With the default min_alloc_size value, 4KB of underlying drive space is allocated. This means that roughly (4KB - 1KB) == 3KB is allocated but never used, corresponding to 300% overhead or 25% efficiency

This happens for each replica. So when using the default three copies of data (3R), a 1KB S3 object consumes roughly 9KB of storage device capacity. If erasure coding (EC) is used instead of replication, the amplification may be even higher: for a k=4,m=2 pool, our 1KB S3 object will allocate (6 * 4KB) = 24KB of device capacity.

[IMPORTANT]
====
Note that this BlueStore attribute takes effect only at OSD creation; if changed later, a given OSD’s behaviour will not change unless/until it is destroyed and redeployed with the appropriate option value(s). Upgrading to a later Ceph release will not change the value used by OSDs deployed under older releases or with other settings.
====

== Configuring OSDs with a separate device for Block and Block.DB

Assuming you have 3 spare/free drives on each of your ceph cluster nodes, like
in this example:

----
# ceph orch device ls | grep Yes
ceph-node01  /dev/vdc  hdd   e17487cd-a216-43db-a  10.7G  Yes        6s ago
ceph-node01  /dev/vdd  hdd   df8a0981-87f2-4fb0-9  10.7G  Yes        6s ago
ceph-node01  /dev/vde  hdd   07526583-dc65-4d07-8  10.7G  Yes        6s ago
ceph-node02  /dev/vdc  hdd   ee4d2082-8ae1-4f6c-8  10.7G  Yes        6s ago
ceph-node02  /dev/vdd  hdd   19dadf78-4369-4bbb-8  10.7G  Yes        6s ago
ceph-node02  /dev/vde  hdd   c3972ab4-7ebb-4f22-b  10.7G  Yes        6s ago
ceph-node03  /dev/vdc  hdd   70ad190e-46a2-44df-8  10.7G  Yes        6s ago
ceph-node03  /dev/vdd  hdd   2ae6c11a-8b4b-480d-8  10.7G  Yes        6s ago
ceph-node03  /dev/vde  hdd   5a556d98-0ec5-4242-b  10.7G  Yes        6s ago
----

There are many ways in which you can define what drive will be used for each
partition, you can take a look at examples in this https://docs.ceph.com/en/quincy/cephadm/services/osd/#advanced-osd-service-specifications[link]

In our case first it would not make sense to separate in different drives our db and
wal devices, because all of our disks are the same, but just for the sake of
training let's use `vde` as our flash disk and `vdc,vdd` as the HDD drives.

To make things more complicated, we can't filter our disk for block.db using the
rotation/non-rotational flags, because all our disks are the same.

So our best option is using the direct paths filtering option. Here is an example
spec to get it working

----
# cat osd-path-spec.yaml
---
service_type: osd
service_id: osd_using_paths
placement:
  host_pattern: 'ceph-node0[1-3]'
data_devices:
  paths:
    - /dev/vdc
    - /dev/vdd
db_devices:
  paths:
    - /dev/vde
----

We can use the `dry-run` parameter to check what actions will be taken by cephadm

----
# ceph orch apply -i osd-path-spec.yaml --dry-run
Preview data is being generated.. Please re-run this command in a bit.
# ceph orch apply -i osd-path-spec.yaml --dry-run
################
OSDSPEC PREVIEWS
################
+---------+-----------------+-------------+----------+----------+-----+
|SERVICE  |NAME             |HOST         |DATA      |DB        |WAL  |
+---------+-----------------+-------------+----------+----------+-----+
|osd      |osd_using_paths  |ceph-node01  |/dev/vdc  |/dev/vde  |-    |
|osd      |osd_using_paths  |ceph-node01  |/dev/vdd  |/dev/vde  |-    |
|osd      |osd_using_paths  |ceph-node02  |/dev/vdc  |/dev/vde  |-    |
|osd      |osd_using_paths  |ceph-node02  |/dev/vdd  |/dev/vde  |-    |
|osd      |osd_using_paths  |ceph-node03  |/dev/vdc  |/dev/vde  |-    |
|osd      |osd_using_paths  |ceph-node03  |/dev/vdd  |/dev/vde  |-    |
+---------+-----------------+-------------+----------+----------+-----+
----

The actions that will take place are looking good, so we can go ahead and apply
the new OSD spec file.

----
# ceph orch apply -i osd-path-spec.yaml --dry-run
Scheduled osd.osd_using_paths update...
----

Lets, check if the OSDs are getting configured, we now have no free disks

----
# ceph orch device ls | grep Yes
#
----

With ceph orch, we can see the OSD service created and the daemons running:

----
# ceph orch ls | grep osd
osd.all-available-devices                     3  34s ago    2d   label:osd
osd.osd_using_paths                           6  34s ago    2m   ceph-node0[1-3]
# ceph orch ps | grep osd
osd.1                            ceph-node02               running (32m)    46s ago   2d     183M    4096M  16.2.10-94.el8cp  34880245f74a  da5d207fe102
osd.10                           ceph-node03               running (2m)     46s ago   2m    98.6M    4096M  16.2.10-94.el8cp  34880245f74a  b316ff73688c
osd.2                            ceph-node03               running (32m)    46s ago   2d     177M    4096M  16.2.10-94.el8cp  34880245f74a  e9323a455610
osd.4                            ceph-node01               running (31m)    46s ago  13h     107M    4096M  16.2.10-94.el8cp  34880245f74a  395c45a7aff0
osd.5                            ceph-node01               running (2m)     46s ago   2m    69.4M    4096M  16.2.10-94.el8cp  34880245f74a  a4827a6ebbc3
osd.6                            ceph-node02               running (2m)     46s ago   2m    90.0M    4096M  16.2.10-94.el8cp  34880245f74a  f45ddb8c8f4c
osd.7                            ceph-node03               running (2m)     46s ago   2m    71.3M    4096M  16.2.10-94.el8cp  34880245f74a  da22f34fecf7
osd.8                            ceph-node01               running (2m)     46s ago   2m    68.6M    4096M  16.2.10-94.el8cp  34880245f74a  fdb82f45e2d7
osd.9                            ceph-node02               running (2m)     46s ago   2m    67.5M    4096M  16.2.10-94.el8cp  34880245f74a  aeed575262aa
----

With the `ceph osd tree` command we can check the osds are up+in the cluster

----
# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME                 STATUS  REWEIGHT  PRI-AFF
-1         0.14619  root default
-3         0.14619      datacenter DC1
-2         0.06825          host ceph-node01
 4    hdd  0.00980              osd.4             up   1.00000  1.00000
 5    hdd  0.01459              osd.5             up   1.00000  1.00000
 8    hdd  0.01459              osd.8             up   1.00000  1.00000
-4         0.03897          host ceph-node02
 1    hdd  0.00980              osd.1             up   1.00000  1.00000
 6    hdd  0.01459              osd.6             up   1.00000  1.00000
 9    hdd  0.01459              osd.9             up   1.00000  1.00000
-5         0.03897          host ceph-node03
 2    hdd  0.00980              osd.2             up   1.00000  1.00000
 7    hdd  0.01459              osd.7             up   1.00000  1.00000
10    hdd  0.01459              osd.10            up   1.00000  1.00000
----

We can check with lvm and `ceph-volume lvm list` that we are actually using different
devices for each new OSD that we just deployed

----
# podman exec -it ceph-910c8bb8-95bc-11ed-b7d6-2cc26078e4ef-osd-8 ceph-volume lvm list
====== osd.4 =======

  [block]       /dev/ceph-45cf10cf-2149-4517-bb8d-25809fb29bbd/osd-block-319ded07-60bd-4715-9f1a-50cdf1872cb6

      block device              /dev/ceph-45cf10cf-2149-4517-bb8d-25809fb29bbd/osd-block-319ded07-60bd-4715-9f1a-50cdf1872cb6
      block uuid                X1ascR-kGpJ-cdAG-VDjm-JYVu-Bghz-6Ie5NK
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      encrypted                 0
      osd fsid                  319ded07-60bd-4715-9f1a-50cdf1872cb6
      osd id                    4
      osdspec affinity          all-available-devices
      type                      block
      vdo                       0
      devices                   /dev/vdb

====== osd.5 =======

  [block]       /dev/ceph-6b54b025-2567-4197-b4a9-42d85cfb80b9/osd-block-c40763ca-486a-437f-b881-a49d622c69a8

      block device              /dev/ceph-6b54b025-2567-4197-b4a9-42d85cfb80b9/osd-block-c40763ca-486a-437f-b881-a49d622c69a8
      block uuid                1WfFtJ-8lAO-SCWC-Q9JD-FDeD-O4iy-fox5N4
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      db device                 /dev/ceph-84eee3e3-e8cb-46b1-a980-63c91906cc80/osd-db-e2e64b9d-eeb9-4513-93cf-f6cb0c2e97e6
      db uuid                   4wbS1C-mAy1-E8NZ-X38T-nSe7-RU5F-BNia2w
      encrypted                 0
      osd fsid                  c40763ca-486a-437f-b881-a49d622c69a8
      osd id                    5
      osdspec affinity          osd_using_paths
      type                      block
      vdo                       0
      devices                   /dev/vdc

  [db]          /dev/ceph-84eee3e3-e8cb-46b1-a980-63c91906cc80/osd-db-e2e64b9d-eeb9-4513-93cf-f6cb0c2e97e6

      block device              /dev/ceph-6b54b025-2567-4197-b4a9-42d85cfb80b9/osd-block-c40763ca-486a-437f-b881-a49d622c69a8
      block uuid                1WfFtJ-8lAO-SCWC-Q9JD-FDeD-O4iy-fox5N4
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      db device                 /dev/ceph-84eee3e3-e8cb-46b1-a980-63c91906cc80/osd-db-e2e64b9d-eeb9-4513-93cf-f6cb0c2e97e6
      db uuid                   4wbS1C-mAy1-E8NZ-X38T-nSe7-RU5F-BNia2w
      encrypted                 0
      osd fsid                  c40763ca-486a-437f-b881-a49d622c69a8
      osd id                    5
      osdspec affinity          osd_using_paths
      type                      db
      vdo                       0
      devices                   /dev/vde

====== osd.8 =======

  [db]          /dev/ceph-84eee3e3-e8cb-46b1-a980-63c91906cc80/osd-db-bcf8b6e9-17ec-48ef-ab77-3d14ab47e51c

      block device              /dev/ceph-930f86f3-2810-44b9-9fee-c550446084cc/osd-block-e0dff673-f64f-4f36-9077-d32ec755a911
      block uuid                FguIB1-Eo48-WNFM-RnPr-05XW-uxKO-bE1BMk
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      db device                 /dev/ceph-84eee3e3-e8cb-46b1-a980-63c91906cc80/osd-db-bcf8b6e9-17ec-48ef-ab77-3d14ab47e51c
      db uuid                   CXUPGU-gek0-Kncw-bkWe-MqXo-YJcN-qMd812
      encrypted                 0
      osd fsid                  e0dff673-f64f-4f36-9077-d32ec755a911
      osd id                    8
      osdspec affinity          osd_using_paths
      type                      db
      vdo                       0
      devices                   /dev/vde

  [block]       /dev/ceph-930f86f3-2810-44b9-9fee-c550446084cc/osd-block-e0dff673-f64f-4f36-9077-d32ec755a911

      block device              /dev/ceph-930f86f3-2810-44b9-9fee-c550446084cc/osd-block-e0dff673-f64f-4f36-9077-d32ec755a911
      block uuid                FguIB1-Eo48-WNFM-RnPr-05XW-uxKO-bE1BMk
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      db device                 /dev/ceph-84eee3e3-e8cb-46b1-a980-63c91906cc80/osd-db-bcf8b6e9-17ec-48ef-ab77-3d14ab47e51c
      db uuid                   CXUPGU-gek0-Kncw-bkWe-MqXo-YJcN-qMd812
      encrypted                 0
      osd fsid                  e0dff673-f64f-4f36-9077-d32ec755a911
      osd id                    8
      osdspec affinity          osd_using_paths
      type                      block
      vdo                       0
      devices                   /dev/vdd

# lvs
  LV                                             VG                                        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  osd-block-319ded07-60bd-4715-9f1a-50cdf1872cb6 ceph-45cf10cf-2149-4517-bb8d-25809fb29bbd -wi-ao---- <10.00g                                                    
  osd-block-c40763ca-486a-437f-b881-a49d622c69a8 ceph-6b54b025-2567-4197-b4a9-42d85cfb80b9 -wi-ao---- <10.00g                                                    
  osd-db-bcf8b6e9-17ec-48ef-ab77-3d14ab47e51c    ceph-84eee3e3-e8cb-46b1-a980-63c91906cc80 -wi-ao----  <5.00g                                                    
  osd-db-e2e64b9d-eeb9-4513-93cf-f6cb0c2e97e6    ceph-84eee3e3-e8cb-46b1-a980-63c91906cc80 -wi-ao----  <5.00g                                                    
  osd-block-e0dff673-f64f-4f36-9077-d32ec755a911 ceph-930f86f3-2810-44b9-9fee-c550446084cc -wi-ao---- <10.00g   
----

As a final exercise, how would you remove the osds we just created?. We need
the OSDs out of the ceph-cluster and all the devices in a clean state to be
re-used.

[TIP]
====
First, get rid of the OSDs at the Ceph Level, make sure you don't lose data!

The Cephadm approach to deleting OSDs
----
# ceph orch osd rm 5
Scheduled OSD(s) for removal
# ceph orch osd rm status
OSD  HOST         STATE     PGS  REPLACE  FORCE  ZAP    DRAIN STARTED AT
5    ceph-node01  draining    1  False    False  False  2023-01-19 08:19:27.551848
# ceph orch osd rm status
No OSD remove/replace operations reported
# ceph orch device zap ceph-node01 /dev/vdc --force
zap successful for /dev/vdc on ceph-node01
----

Example of deleting the osd without the cephadm orchestrator(only use if cephadm leaves
things behind)
----
ceph osd out osd.11
ceph osd down osd.11
ceph osd rm osd.11
ceph osd crush rm osd.11
ceph auth del osd.11
----
====

[TIP]
====
If the unmanaged flag is unset, cephadm automatically deploys drives that match the OSDSpec. For example, if you use the all-available-devices option when creating OSDs, when you zap a device, the cephadm orchestrator automatically creates a new OSD in the device.

it’s best to modify the drive group spec before removal. Setting the unmanaged:
true to the OSD spec.
====


