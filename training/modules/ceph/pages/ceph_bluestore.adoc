= Bluestore

//++++
//<link rel="stylesheet"  href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/3.1.0/css/font-awesome.min.css">
//++++
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:sectnums:
:sectnumlevels: 6
:toc: left
:toclevels: 4


== Introduction

BlueStore is the default backend object store for the Ceph OSD daemons. The original object store, FileStore, requires a file system on top of raw block devices. Objects are then written to the file system. Unlike the original FileStore back end, BlueStore stores object directly on the block devices without any file system interface, which improves the performance of the cluster.

NOTE: Filestore has been Deprecated.

== Bluestore Architecture

Data is directly written to the raw block device and all metadata operations are managed by RocksDB. The device containing the OSD is divided between RocksDB metadata and the actual user data stored in the cluster.  User data objects are stored as blobs directly on the raw block device, once the data has been written to the block device, RocksDB metadata gets updated with the required details about the new data blobs.

image:::filestore-vs-bluestore-2.png[Filestore vs Bluestore]

RocksDB is an embedded high-performance key-value store that excels with flash-storage, RocksDB can’t directly write to the raw disk device, it needs and underlying filesystem to store it’s persistent data, this is where BlueFS comes in, BlueFS is a Filesystem developed with the minimal feature set needed by RocksDB to store its sst files.

RocksDB uses WAL as a transaction log on persistent storage, unlike Filestore where all the writes went first to the journal, in bluestore we have two different datapaths for writes, one were data is written directly to the block device and the other were we use deferred writes, with deferred writes data gets written to the WAL device and later asynchronously flushed to disk.

New Bluestore features:
* it enables compression of data at the lowest level.
** if compression is enabled data blobs allocated on the raw device will be compressed. This means that any data written into RH Ceph Storage, no matter the client used(rbd,rados, etc), can benefit from this feature.
* An additional benefit of BlueStore is that it stores data and meta-data in the cluster with checksums for increased integrity.
** Whenever data is read from persistent storage its checksum is verified

== Bluestore improvements

* Direct management of storage devices
BlueStore consumes raw block devices or partitions. This avoids any intervening layers of abstraction, such as local file systems like XFS, that might limit performance or add complexity.

* Metadata management with RocksDB
BlueStore uses the RocksDB’ key-value database to manage internal metadata, such as the mapping from object names to block locations on a disk.

* Full data and metadata checksumming
By default all data and metadata written to BlueStore is protected by one or more checksums. No data or metadata are read from disk or returned to the user without verification.
Efficient copy-on-write

* The Ceph Block Device and Ceph File System snapshots rely on a copy-on-write clone mechanism that is implemented efficiently in BlueStore. This results in efficient I/O both for regular snapshots and for erasure coded pools which rely on cloning to implement efficient two-phase commits.
No large double-writes

* BlueStore first writes any new data to unallocated space on a block device, and then commits a RocksDB transaction that updates the object metadata to reference the new region of the disk. Only when the write operation is below a configurable size threshold, it falls back to a write-ahead journaling scheme, similar to how FileStore operates.

== Bluestore Layout

Bluestore has a number of possible storage layout configurations:

* The main device that stores the object data.
* An optional RocksDB device that stores the metadata
** A DB device (identified as block.db in the data directory) can be used for storing BlueStore’s internal metadata. BlueStore (or rather, the embedded RocksDB) will put as much metadata as it can on the DB device to improve performance. If the DB device fills up, metadata will spill back onto the primary device (where it would have been otherwise). Again, it is only helpful to provision a DB device if it is faster than the primary device.
* An optional WAL device that stores the transaction logs.
** A write-ahead log (WAL) device (identified as block.wal in the data directory) can be used for BlueStore’s internal journal or write-ahead log. It is only useful to use a WAL device if the device is faster than the primary device (e.g., when it is on an SSD and the primary device is an HDD).

For example you could configure the main device on the an HDD drive , this is
where the user data will be stored. We then configured the WAL and RocksDB
devices on a faster Flash NVMe drive, having WAL and RocksDB on a high performing drive will give us more IOPS with lower latency.

We can check inside an OSD container
`/var/lib/ceph/910c8bb8-95bc-11ed-b7d6-2cc26078e4ef/osd.3/` dir to get the osd
layout, this OSD has been configured specifiying the 3 devices block,block.db
and block.wal that is why we have 3 different devices for each

----
# ls -ltra /var/lib/ceph/910c8bb8-95bc-11ed-b7d6-2cc26078e4ef/osd.3/ | grep block
lrwxrwxrwx 1 ceph ceph  111 Jan 16 11:57 block -> /dev/mapper/ceph--f5a5ccbd--5dc2--44d3--9ea1--031153783317-osd--block--2f3561bc--81c5--483e--9cc2--cb1e3bb69dc8
lrwxrwxrwx 1 ceph ceph  108 Jan 16 11:57 block.db -> /dev/mapper/ceph--b6c2103e--6cc0--4040--b117--a5acfa756bc9-osd--db--987930d8--0aaa--4c8f--812b--edc12edb2160
lrwxrwxrwx 1 ceph ceph  109 Jan 16 11:57 block.wal -> /dev/mapper/ceph--d71f301f--cdd5--40f9--b7aa--8ee84c092d81-osd--wal--5de39adc--5a3e--419e--ab3f--3ae239dd2f78
----

The devices point to LVM device mappers on the Node, so each device is a
dedicated LVM logical volume, each lvol is using a separe VG with a dedicated
drive

----
# lvs
  LV                                             VG                                        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  osd-db-987930d8-0aaa-4c8f-812b-edc12edb2160    ceph-b6c2103e-6cc0-4040-b117-a5acfa756bc9 -wi-ao---- <10.00g
  osd-wal-5de39adc-5a3e-419e-ab3f-3ae239dd2f78   ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81 -wi-ao---- <10.00g
  osd-block-2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8 ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317 -wi-ao---- <10.00g

# pvs
  PV         VG                                        Fmt  Attr PSize   PFree
  /dev/vdc   ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317 lvm2 a--  <10.00g    0
  /dev/vdd   ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81 lvm2 a--  <10.00g    0
  /dev/vde   ceph-b6c2103e-6cc0-4040-b117-a5acfa756bc9 lvm2 a--  <10.00g    0
----

From inside the OSD container we can also use the `ceph-volume` command to
check-out the OSD layout details

----
# ceph-volume lvm list

====== osd.3 =======

  [db]          /dev/ceph-b6c2103e-6cc0-4040-b117-a5acfa756bc9/osd-db-987930d8-0aaa-4c8f-812b-edc12edb2160

      block device              /dev/ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317/osd-block-2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      block uuid                bP7bb6-RmSZ-2Un8-bj63-yJbR-RK21-KdYcDZ
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      db device                 /dev/ceph-b6c2103e-6cc0-4040-b117-a5acfa756bc9/osd-db-987930d8-0aaa-4c8f-812b-edc12edb2160
      db uuid                   2Lk2gH-Ud0M-SH2J-GNGE-4DfM-0xIN-CvJ8E0
      encrypted                 0
      osd fsid                  2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      osd id                    3
      osdspec affinity          osd_using_paths
      type                      db
      vdo                       0
      wal device                /dev/ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81/osd-wal-5de39adc-5a3e-419e-ab3f-3ae239dd2f78
      wal uuid                  VufD8W-g9uX-FIDd-2p6G-90hJ-nBYt-HZG5X7
      devices                   /dev/vde

  [wal]         /dev/ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81/osd-wal-5de39adc-5a3e-419e-ab3f-3ae239dd2f78

      block device              /dev/ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317/osd-block-2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      block uuid                bP7bb6-RmSZ-2Un8-bj63-yJbR-RK21-KdYcDZ
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      encrypted                 0
      osd fsid                  2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      osd id                    3
      osdspec affinity          osd_using_paths
      type                      wal
      vdo                       0
      wal device                /dev/ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81/osd-wal-5de39adc-5a3e-419e-ab3f-3ae239dd2f78
      wal uuid                  VufD8W-g9uX-FIDd-2p6G-90hJ-nBYt-HZG5X7
      devices                   /dev/vdd

  [block]       /dev/ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317/osd-block-2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8

      block device              /dev/ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317/osd-block-2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      block uuid                bP7bb6-RmSZ-2Un8-bj63-yJbR-RK21-KdYcDZ
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      db device                 /dev/ceph-b6c2103e-6cc0-4040-b117-a5acfa756bc9/osd-db-987930d8-0aaa-4c8f-812b-edc12edb2160
      db uuid                   2Lk2gH-Ud0M-SH2J-GNGE-4DfM-0xIN-CvJ8E0
      encrypted                 0
      osd fsid                  2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      osd id                    3
      osdspec affinity          osd_using_paths
      type                      block
      vdo                       0
      wal device                /dev/ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81/osd-wal-5de39adc-5a3e-419e-ab3f-3ae239dd2f78
      wal uuid                  VufD8W-g9uX-FIDd-2p6G-90hJ-nBYt-HZG5X7
      devices                   /dev/vdc
----

You can also use the `ceph-bluestore-tool` to check the labels

WARNING: The OSD process can't be running when using the  `ceph-bluestore-tool`
or the ceph-objectstore-tool tool, check steps on how to start an OSD container
without the OSD process for debugging xref:troubleshooting_bluestore.adoc[here].

----
# ceph-bluestore-tool show-label --dev /dev/mapper/ceph--f5a5ccbd--5dc2--44d3--9ea1--031153783317-osd--block--2f3561bc--81c5--483e--9cc2--cb1e3bb69dc8
{
    "/dev/mapper/ceph--f5a5ccbd--5dc2--44d3--9ea1--031153783317-osd--block--2f3561bc--81c5--483e--9cc2--cb1e3bb69dc8": {
        "osd_uuid": "2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8",
        "size": 10733223936,
        "btime": "2023-01-16T16:57:43.638530+0000",
        "description": "main",
        "bfm_blocks": "2620416",
        "bfm_blocks_per_key": "128",
        "bfm_bytes_per_block": "4096",
        "bfm_size": "10733223936",
        "bluefs": "1",
        "ceph_fsid": "910c8bb8-95bc-11ed-b7d6-2cc26078e4ef",
        "kv_backend": "rocksdb",
        "magic": "ceph osd volume v026",
        "mkfs_done": "yes",
        "osd_key": "AQAFgsVjR+/KKRAA8QmuVH4WJ9mlDaVcO91xVg==",
        "osdspec_affinity": "osd_using_paths",
        "ready": "ready",
        "require_osd_release": "16",
        "whoami": "3"
    }
}
----


== Bluestore Spill Over

Spill Over happens when RocksDB starts using the `slow` block device affecting
performance.

BlueFS wraps RocksDB understanding of filesystem and transforms into structure well suited for block devices.

. *db is* allocated from device options.bluestore_block_db_path, (informal block.db)
. *db.slow* is allocated from device options.bluestore_block_path, (informal block)
. *db.wal* is allocated from device options.bluestore_wal_path, (informal block.wal)

During processing, RocksDB has a temporary higher demand for space. It asks to create a file on "/db/xxxx" but exhausts space on block.db and starts consuming block space. This redirection of allocation is done internally in BlueFS and RocksDB is unaware that relocation occurred. This means that file with name "/db/xxxxx" is located on slow block device, so RocksDB still thinks it is fast. After the peak is gone, data allocated on block remains there, with a lot of space free on block.db.

*How to check?*

If you have spillover you can expect the "slow_bytes" values to be > 0.

----
# ceph daemon osd.0 perf dump bluefs | grep -E "db_|slow_"
         "db_total_bytes": 21470642176,
         "db_used_bytes": 179699712,
         "slow_total_bytes": 0,
         "slow_used_bytes": 0,
----

== Bluestore deployment Strategy

It's recommended to follow these guidelines:

* If all devices are the same type, for example all rotational drives, and there are no fast devices to use for metadata, it makes sense to specify the block device only and to not separate block.db or block.wal.
* If you have a mix of fast and slow devices (SSD / NVMe and rotational), it is recommended to place block.db on the faster device while block (data) lives on the slower (spinning drive).

When using a mixed spinning and solid drive setup it is important to make a large enough block.db logical volume for BlueStore. Generally, block.db should have as large as possible logical volumes.

The general recommendation is to have block.db size in between 1% to 4% of block size. For RGW workloads, it is recommended that the block.db size isn’t smaller than 4% of block, because RGW heavily uses it to store metadata (omap keys). For example, if the block size is 1TB, then block.db shouldn’t be less than 40GB. For RBD workloads, 1% to 2% of block size is usually enough.

You can check detailed information on the OSD layout with the `ceph osd metadata command` 

----
# ceph osd metadata osd.0 | grep blue
    "bluefs": "1",
    "bluefs_dedicated_db": "0",
    "bluefs_dedicated_wal": "0",
    "bluefs_single_shared_device": "1",
    "bluestore_bdev_access_mode": "blk",
    "bluestore_bdev_block_size": "4096",
    "bluestore_bdev_dev_node": "/dev/dm-0",
    "bluestore_bdev_devices": "vdb",
    "bluestore_bdev_driver": "KernelDevice",
    "bluestore_bdev_partition_path": "/dev/dm-0",
    "bluestore_bdev_rotational": "1",
    "bluestore_bdev_size": "10733223936",
    "bluestore_bdev_support_discard": "0",
    "bluestore_bdev_type": "hdd",
    "osd_objectstore": "bluestore",
----

[TIP]
====
In older releases, internal level sizes mean that the DB can fully utilize only
specific partition / LV sizes that correspond to sums of L0, L0+L1, L1+L2, etc.
sizes, which with default settings means roughly 3 GB, 30 GB, 300 GB, and so
forth. Most deployments will not substantially benefit from sizing to
accommodate L3 and higher.

Improvements in the latest releases beginning with Nautilus 14.2.12 and Octopus 15.2.6 enable better utilization of arbitrary DB device sizes, and the Pacific release brings experimental dynamic level support.
====

== Bluestore Cache

The BlueStore cache is a collection of buffers that, depending on configuration, can be populated with data as the OSD daemon does reading from or writing to the disk.

=== Bluestore Automatic Cache Sizing

BlueStore can be configured to automatically resize its caches when TCMalloc is configured as the memory allocator and the bluestore_cache_autotune setting is enabled. This option is currently enabled by default. BlueStore will attempt to keep OSD heap memory usage under a designated target size via the osd_memory_target configuration option. This is a best effort algorithm and caches will not shrink smaller than the amount specified by osd_memory_cache_min.

Automatic Cache sized works great with most workloads, as such we recommend to
using it.

=== Bluestore Manual Cache Sizing

When bluestore_cache_autotune is disabled and bluestore_cache_size_ssd parameter is set, BlueStore cache gets subdivided into 3 different caches:

* *cache_meta:* used for BlueStore Onode and associated data.
* *cache_kv:* used for RocksDB block cache including indexes/bloom-filters
* *data cache:* used for BlueStore cache for data buffers.

The amount of space that goes to each cache is configurable using ratios, just
an example

----
bluestore_cache_autotune = 0
bluestore_cache_kv_ratio = 0.2
bluestore_cache_meta_ratio = 0.8
----

TIP: you can check per OSD memory usage details with the following command `ceph daemon osd.$OSD_ID dump_mempools"`

=== The importance of the Bluestore Onode Cache.

With all NVMe deployments, and specially with RBD workloads the size of the
bluestore cache can have a huge impact on performance. Onode caching in bluestore is hierarchical.  If a onode is not cached, it will be read from the DB disk, populated into the KV cache, and finally populated into the bluestore onode cache, as you can imagine having a direct hit in the Onode cache is much faster than reading from disk or from the KV cache.

When all onodes in a data set fit into bluestore's block cache, the onodes are never read from disk and thus onodes never have to be populated into the KV cache at all.  This is the optimal scenario for RBD. On the other hand, a worst case scenario is were you end up needing to read onodes from disk, you'll end up populating both the rocksdb KV cache and the bluestore onode cache with fresh data and force out older onodes, which may be read back in again from disk later.

== Bluestore Database Sharding

BlueStore can divide this data into multiple RocksDB column families. When keys have similar access frequency, modification frequency and lifetime, BlueStore benefits from better caching and more precise compaction. This improves performance, and also requires less disk space during compaction, since each column family is smaller and can compact independent of others.

OSDs deployed in Pacific or later use RocksDB sharding by default. If Ceph is upgraded to Pacific from a previous version, sharding is off.

To check if sharding is enabled on your cluster

----
# ceph config get osd.1 bluestore_rocksdb_cf
true
# ceph config get osd.0 bluestore_rocksdb_cfs
m(3) p(3,0-12) O(3,0-13)=block_cache={type=binned_lru} L P
----

== Minimum Allocation Size.

There is a configured minimum amount of storage that BlueStore will allocate on an OSD. In practice, this is the least amount of capacity that a RADOS object can consume. The value of bluestore_min_alloc_size is derived from the value of bluestore_min_alloc_size_hdd or bluestore_min_alloc_size_ssd depending on the OSD’s rotational attribute.

Through the Mimic release, the default values were 64KB and 16KB for rotational (HDD) and non-rotational (SSD) media respectively. Octopus changed the default for SSD (non-rotational) media to 4KB, and Pacific changed the default for HDD (rotational) media to 4KB as well.

For example, when an RGW client stores a 1KB S3 object, it is written to a single RADOS object. With the default min_alloc_size value, 4KB of underlying drive space is allocated. This means that roughly (4KB - 1KB) == 3KB is allocated but never used, which corresponds to 300% overhead or 25% efficiency

This happens for each replica. So when using the default three copies of data (3R), a 1KB S3 object actually consumes roughly 9KB of storage device capacity. If erasure coding (EC) is used instead of replication, the amplification may be even higher: for a k=4,m=2 pool, our 1KB S3 object will allocate (6 * 4KB) = 24KB of device capacity.

[IMPORTANT]
====
Note that this BlueStore attribute takes effect only at OSD creation; if changed later, a given OSD’s behavior will not change unless / until it is destroyed and redeployed with the appropriate option value(s). Upgrading to a later Ceph release will not change the value used by OSDs deployed under older releases or with other settings.
====
