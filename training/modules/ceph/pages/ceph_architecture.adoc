= Ceph Architecture
//++++
//<link rel="stylesheet"  href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/3.1.0/css/font-awesome.min.css">
//++++
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:sectnums:
:sectnumlevels: 6
:toc: left
:toclevels: 4

== Ceph architecture

The Ceph cluster provides a scalable storage solution while providing
multiple access methods to enable the different types of
clients present within the IT infrastructure to get access to the data.

.Ceph Architecture
image::./images/ceph101-overview.png[Ceph From Above, align="center"]

The entire Ceph architecture is resilient, does not present any single point
of failure (SPOF) and introduces a specific data placement algorithm known as CRUSH (Controlled
Replication Under Scalable Hashing) that does not require the use of a central look-up server.

=== RADOS and its components

The heart of Ceph is an object store known as RADOS (Reliable Autonomic
Distributed Object Store) bottom layer on the screen. This layer provides the
Ceph software defined storage with the ability to store data (serve IO
requests, to protect the data, to check the consistency and the integrity of
the data through built-in mechanisms. The RADOS layer is composed of the
following daemons:

* MONs or Monitors
* OSDs or Object Storage Daemons
* MGRs or Managers
* MDSs or Meta Data Servers

.*_Monitors_*
The Monitors maintain the cluster map and state and provide distributed
decision-making while configured in an odd number, 3 or 5 depending on the
size and the topology of the cluster, to prevent split-brain situations. The
Monitors are not in the data-path and do not serve IO requests to and from
the clients.

The Monitors use the Paxos algorithm to maintain map consistency. It requires
a majority (50% + 1) of the Monitors to validate each map update event. This is why the Monitors
must be deployed as an odd number. All Monitors maintain an identical copy of the
cluster map which contains the following maps:

* `MONMap` - Map of the Monitors
* `MGRMap` - Map of the Managers
* `MDSMap` - Map of the MDSs
* `OSDMap` - Map of the OSDs
* `PGMap`  - Map of the Placement Groups
* `SVCMap` - Map of the services offered by the Ceph cluster

.*_OSDs_*
One Object Storage Daemon is typically deployed for each local block devices and the native
scalable nature of Ceph allows for thousands of OSDs to be part of the
cluster. The OSDs are serving IO requests from the clients while guaranteeing
the protection of the data (replication or erasure coding), the rebalancing
of the data in case of an OSD or a node failure, the coherence of the data
(scrubbing and deep-scrubbing of the existing data).

.OSD Layout
image::./images/ceph101-osdlayout.png[alt=Layout,scaledwidth="66%",align="center"]

NOTE: Originally the OSD backend was using a filesystem and a transaction journal to
guarantee the atomicity of the write transactions. This implementation, known
as the FileStore, has been sunset and replaced by a new backend architecture
based on RocksDB and known as BlueStore (`Jewel` introduction).

The OSDs can have two (2) roles assigned to them. The role depends on what an OSD
performs as functions for a given Placement Group. Therefor a given OSD can be assigned
the two (2) roles at the same time but for different Placement Groups (`PGs`).

*Primary OSD* for a specific `PG`:

* Serves `PG` IO requests
* Replicates and protects `PG`
* Checks `PG` coherence
* Rebalances `PG`
* Recovers `PG`

*Secondary OSD* for a specific `PG`:

* Always controlled by the Primary OSD for the `PG`
* Can be promoted Primary OSD

.*_MGRs_*
The Managers are tightly integrated with the Monitors and collect the
statistics within the cluster. Additionally they provide an extensible
framework for the cluster through a pluggable Python interface aimed at
expanding the Ceph existing capabilities. The current list of modules
developed around the Manager framework are:

* Balancer module
* Placement Group auto-scaler module
* Dashboard module
* RESTful module
* Prometheus module
* Zabbix module
* Rook module

.*_MDSs_*
The Meta Data Servers manage the metadata for the POSIX compliant shared
filesystem such as the directory hierarchy and the file metadata (ownership,
timestamps, mode, ...). All the metadata is stored with RADOS and they do not
server any data to the clients. MDSs are only deployed when a shared
filesystem is configured in the Ceph cluster.

.*_General Layout_*
If we look at the Ceph cluster foundation layer, the full picture with the
different types of daemons or containers looks like this.

.RADOS as it stands
image::./images/ceph101-rados.png[RADOS Overview, align="center"]

The circle represent the MONs, the 'M' represent the MGRs and the squares
with the bars represent the OSDs. In the diagram above, the cluster operates
with 3 Monitors, 2 Managers and 23 OSDs.

=== RADOS access methods 

Ceph was designed to provides the IT environment with all the necessary
access methods so that any application can use what is the best solution for
its use-case.

.Different Storage Types Supported
image::./images/ceph101-differentstoragetypes.png[Ceph Access Modes, align="center"]

Ceph supports block storage through the RADOS Block Device (aka RBD) access
method, file storage through the Ceph Filesystem (aka CephFS) access method
and object storage through its native `librados` API or through the RADOS
Gateway (aka RADOSGW or RGW) for compatibility with the S3 and Swift
protocols.

*Librados*

Librados allows developers to code natively against the native Ceph cluster
API for maximum efficiency combined with a small footprint.

.Application Native Object API
image::./images/ceph101-librados.png[librados, align="center"]

The Ceph native API offers different wrappers such as C, C++, Python, Java,
Ruby, Erlang, Go and Rust.

*RADOS Block Device (RBD)*

This access method is used with Linux distributions and Kubernetes.
RBDs can be accessed either through a kernel module (Linux, Kubernetes)
or through the `librbd` API (OpenStack, Proxmox). In the Kubernetes world,
RBDs are designed to address the need for RWO PVCs. `Pacific` provides a
major update to `librbd` aimed at reducing the performance gap with `kRBD`.

*_Kernel Module (kRBD)_*

The kernel RBD driver offers superior performance while not providing the
same level of functionality. e.g., no RBD Mirroring until `Pacific`.

.kRBD Diagram
image::./images/ceph101-krbd.png[Kernel based RADOS Block Device, align="center"]

*_Userspace RBD (librbd)_*

This access method leverages all existing RBD features such as RBD Mirroring.

.librbd Diagram
image::./images/ceph101-librbd.png[Userspace RADOS Block Device, align="center"]

*_Shared filesystem (CephFS)_*

This method allows clients to jointly access a shared POSIX compliant
filesystem. The client initially contacts the Meta Data Server to obtain the
location of the object(s) for a given inode and then communicates directly
with an OSD to perform the final IO request.

.File Access (Ceph Filesystem or CephFS)
image::./images/ceph101-cephfs.png[Kernel Based CephFS Client, align="center"]

In Kubernetes environments, CephFS is typically used for RWX claims and can
also be used to support RWO claims.

*_Object storage, S3 and Swift (Ceph RADOS Gateway)_*

This access method offers support for the Amazon S3 and OpenStack Swift
support on top of a Ceph cluster.

.Amazon S3 or OpenStack Swift (Ceph RADOS Gateway)
image::./images/ceph101-rgw.png[S3 and Swift Support, align="center"]

=== Controlled replication under scalable hashing

The Ceph cluster being a distributed architecture some solution had to be
designed to provide an efficient way to distribute the data across the
multiple OSDs in the cluster. The technique used is called CRUSH or
Controlled Replication Under Scalable Hashing. With CRUSH, every object is
assigned to one and only one hash bucket known as a Placement Group (PG).

image::./images/ceph101-crushfromobjecttoosd-new.png[From Object to OSD, align="center"]

CRUSH is the central point of configuration for the topology of the cluster.
It offers a pseudo-random placement algorithm to distribute the objects
across the PGs and uses rules to determine the mapping of the PGs to the
OSDs. In essence, the PGs are an abstraction layer between the objects
(application layer) and the OSDs (physical layer). In case of failure, the
PGs will be remapped to different physical devices (OSDs) and eventually see
their content resynchronized to match the protection rules selected by the
storage administrator.

NOTE: The `PG` acts as a floating abstraction layer between the object (logical layer)
and the OSD (physical layer). The mapping of a `PG` is linked to the actual cluster 
state (Cluster Map). As a result the mapping of a `PG` will always be
identical for a given cluster state.

