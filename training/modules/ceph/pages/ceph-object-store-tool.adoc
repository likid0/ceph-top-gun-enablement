== Internal bluestore tools in Ceph containers

When troubleshooting Ceph, sometimes we need to use some tools such as
`ceph-objectstore-tool` or `ceph-bluestore-tool`. When working in
containerized environments this is more complicated as there are a set
of internal scripts inside the container that automatically start the
OSD service and the OSD is automatically included in the cluster upon
start.

We have received a procedure from
https://access.redhat.com/support/cases/#/case/02801980[Red Hat Support]
where there is no need to modify the official RHCS image.

*1. Set `noout` flag on Ceph cluster*

....
$ podman exec ceph-mon-$(hostname -s) ceph osd set noout
noout is set
....

*2. Login to the node hosting the OSD container*

In our case we are going to act in the node `ceph1` and the `osd.0`:

....
$ podman exec ceph-mon-$(hostname -s) ceph osd tree
ID CLASS WEIGHT  TYPE NAME      STATUS REWEIGHT PRI-AFF 
-1       0.17578 root default                           
-5       0.05859     host ceph1                         
 0   hdd 0.02930         osd.0      up  1.00000 1.00000 
 4   hdd 0.02930         osd.4      up  1.00000 1.00000 
-7       0.05859     host ceph2                         
 1   hdd 0.02930         osd.1      up  1.00000 1.00000 
 3   hdd 0.02930         osd.3      up  1.00000 1.00000 
-3       0.05859     host ceph3                         
 2   hdd 0.02930         osd.2      up  1.00000 1.00000 
 5   hdd 0.02930         osd.5      up  1.00000 1.00000
....

*3. Backup `/etc/systemd/system/ceph-osd@.service` unit file to `/root`
directory*

....
$ cp /etc/systemd/system/ceph-osd@.service /root/ceph-osd@.service.backup
....

*4. Move `/run/ceph-osd@osd_id.service-cid` file to `/root`*

....
$ mv /run/ceph-osd@0.service-cid /root
....

*5. Edit `/etc/systemd/system/ceph-osd@.service` unit file*

Add `-it --entrypoint /bin/bash` option to podman command.

....
$ vim /etc/systemd/system/ceph-osd@.service
# Please do not change this file directly since it is managed by Ansible and will be overwritten
[Unit]
Description=Ceph OSD
After=network.target

[Service]
EnvironmentFile=-/etc/environment
ExecStartPre=-/usr/bin/rm -f /%t/%n-pid /%t/%n-cid
ExecStartPre=-/usr/bin/podman rm -f ceph-osd-%i
<ExecStart=/usr/bin/podman run \
>ExecStart=/usr/bin/podman run -it --entrypoint /bin/bash \
  -d --conmon-pidfile /%t/%n-pid --cidfile /%t/%n-cid \
  --rm \
  --net=host \
  --privileged=true \
  --pid=host \
  --ipc=host \
  --cpus=4 \
  -v /dev:/dev \
  -v /etc/localtime:/etc/localtime:ro \
  -v /var/lib/ceph:/var/lib/ceph:z \
  -v /etc/ceph:/etc/ceph:z \
  -v /var/run/ceph:/var/run/ceph:z \
  -v /var/run/udev/:/var/run/udev/ \
  -v /var/log/ceph:/var/log/ceph:z \
  -e OSD_BLUESTORE=1 -e OSD_FILESTORE=0 -e OSD_DMCRYPT=0 \
  -e CLUSTER=ceph \
  -v /run/lvm/:/run/lvm/ \
  -e CEPH_DAEMON=OSD_CEPH_VOLUME_ACTIVATE \
  -e CONTAINER_IMAGE=bastion:5000/rhceph/rhceph-4-rhel8:latest \
  -e OSD_ID=%i \
>  -e DEBUG=stayalive \
  --name=ceph-osd-%i \
  --log-opt=path="/dev/null" \
  bastion:5000/rhceph/rhceph-4-rhel8:latest
ExecStop=-/usr/bin/sh -c "/usr/bin/podman rm -f `cat /%t/%n-cid`"
KillMode=none
Restart=always
RestartSec=10s
TimeoutStartSec=120
TimeoutStopSec=15
Type=forking
PIDFile=/%t/%n-pid

[Install]
WantedBy=multi-user.target
....

*6. Reload systemd manager configuration*

....
$ systemctl daemon-reload
....

*7. Re-start the OSD service associated with our OSD*

....
$ systemctl restart ceph-osd@0.service
....

*8. Login to container associated with our OSD*

....
$  podman exec -it ceph-osd-0 /bin/bash
....

*9. Get osd fsid and activate the OSD to mount OSD LV*

....
$ osd_fsid=$(ceph-volume lvm list |grep -A15 "osd\.0"|grep "osd fsid" | awk '{print $3}')
$ echo $osd_fsid 
8a398b35-aa13-4363-9652-e2aa6dd1fb8d
$ ceph-volume lvm activate --bluestore 0 $osd_fsid
Running command: /usr/bin/mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-0
Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-0
Running command: /usr/bin/ceph-bluestore-tool --cluster=ceph prime-osd-dir --dev /dev/ceph-a54942f0-a335-4a5f-9028-27825e779605/osd-data-a4a8734e-0222-4c7c-933e-73da01938c59 --path /var/lib/ceph/osd/ceph-0 --no-mon-config
Running command: /usr/bin/ln -snf /dev/ceph-a54942f0-a335-4a5f-9028-27825e779605/osd-data-a4a8734e-0222-4c7c-933e-73da01938c59 /var/lib/ceph/osd/ceph-0/block
Running command: /usr/bin/chown -h ceph:ceph /var/lib/ceph/osd/ceph-0/block
Running command: /usr/bin/chown -R ceph:ceph /dev/dm-0
Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-0
Running command: /usr/bin/systemctl enable ceph-volume@lvm-0-8a398b35-aa13-4363-9652-e2aa6dd1fb8d
 stderr: Created symlink /etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-0-8a398b35-aa13-4363-9652-e2aa6dd1fb8d.service → /usr/lib/systemd/system/ceph-volume@.service.
Running command: /usr/bin/systemctl enable --runtime ceph-osd@0
 stderr: Created symlink /run/systemd/system/ceph-osd.target.wants/ceph-osd@0.service → /usr/lib/systemd/system/ceph-osd@.service.
Running command: /usr/bin/systemctl start ceph-osd@0
 stderr: Running in chroot, ignoring request: start
--> ceph-volume lvm activate successful for osd ID: 0
....

*10. Run `ceph-objectstore-tool` or `ceph-bluestore-tool` commands*

....
$ ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-0 --op list
$ ceph-bluestore-tool show-label --dev /dev/ceph-a54942f0-a335-4a5f-9028-27825e779605/osd-data-a4a8734e-0222-4c7c-933e-73da01938c59 
{
    "/dev/ceph-a54942f0-a335-4a5f-9028-27825e779605/osd-data-a4a8734e-0222-4c7c-933e-73da01938c59": {
        "osd_uuid": "8a398b35-aa13-4363-9652-e2aa6dd1fb8d",
        "size": 32208060416,
        "btime": "2021-01-25 07:02:17.277679",
        "description": "main",
        "bluefs": "1",
        "ceph_fsid": "d8a34408-3b5a-4ee3-bd12-b32f6f466c60",
        "kv_backend": "rocksdb",
        "magic": "ceph osd volume v026",
        "mkfs_done": "yes",
        "osd_key": "AQBHsw5glhbhNhAAXGDN2BdMkXan71OFduMrRA==",
        "ready": "ready",
        "require_osd_release": "14",
        "whoami": "0"
    }
}
$ exit
....

=== Restore steps

*1. Copy `/etc/systemd/system/ceph-osd@.service` unit file from `/root`
directory*

....
$ cp /etc/systemd/system/ceph-osd@.service /root/ceph-osd@.service.modified
$ cp /root/ceph-osd@.service.backup /etc/systemd/system/ceph-osd@.service
....

*2. Reload systemd manager configuration*

....
$ systemctl daemon-reload
....

*3. Move `/run/ceph-osd@osd_id.service-cid` file to `/tmp` and restart
the OSD service associated with our OSD*

....
$ mv /run/ceph-osd@0.service-cid /tmp
$ systemctl restart ceph-osd@0.service
....

*4. Unset `noout` flag on Ceph cluster*

....
$ podman exec ceph-mon-$(hostname -s) ceph osd unset noout
noout is unset
....

*5. Ensure Ceph status is in `HEALTH_OK`*

....
$ podman exec ceph-mon-$(hostname -s) ceph -s
  cluster:
    id:     d8a34408-3b5a-4ee3-bd12-b32f6f466c60
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 27h)
    mgr: ceph2(active, since 27h), standbys: ceph1, ceph3
    osd: 6 osds: 6 up (since 80s), 6 in (since 80s)
    rgw: 3 daemons active (ceph1.rgw0, ceph2.rgw0, ceph3.rgw0)
 
  task status:
 
  data:
    pools:   18 pools, 264 pgs
    objects: 1.24k objects, 56 KiB
    usage:   6.4 GiB used, 174 GiB / 180 GiB avail
    pgs:     264 active+clean
 
  io:
    client:   341 B/s rd, 0 op/s rd, 0 op/s wr
....
