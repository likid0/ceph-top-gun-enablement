== Troubleshooting nearfull and full OSDs scenarios

This document describes the procedure to handle the scenarios when a
RHCS 4 cluster reaches either the nearfull ratio or the full ratio.

=== Thresholds to monitor OSDs capacity

A Ceph cluster will prevent writing operations to a full OSD in order to
avoid losing data. Some warnings are triggered when cluster's OSDs and
pools reach some thresholds.

The following is a RHCS cluster with `full`, `nearfull` and
`backfillfull` OSDs:

....
ceph health detail
HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
osd.X is full at 97%
osd.Y is backfill full at 91%
osd.Z is near full at 87%
....

The three parameters that monitor OSD's capacity with their
corresponding default values can be checked in the following table:

[cols=",,",options="header",]
|===
|Parameter |Description |Default value
|`mon_osd_full_ratio` |The percentage of disk space used before an OSD
is considered full |0.95

|`mon_osd_nearfull_ratio` |The percentage of disk space used before an
OSD is considered nearfull |0.85

|`osd_backfill_full_ratio` |The RHCS cluster refuses to accept backfill
requests when the OSD's full ratio is above this value |0.90
|===

The parameters shown above cannot be changed after creating the RHCS
cluster. However, the parameters: `full_ratio`, `nearfull_ratio` and
`backfillfull_ratio` can be adjusted temporarily as follows:

....
ceph osd set-nearfull-ratio <float[0.0-1.0]>
ceph osd set-full-ratio <float[0.0-1.0]>
ceph osd set-backfillfull-ratio <float[0.0-1.0]>
....

In a later section we will see when these parameters can be changed to
handle a `full` or `nearfull` OSD scenarios.

=== Checking OSDs utilization

The `ceph df` command can be used to check the overall cluster or pools
fullness. This command shows a similar output as follows:

....
# ceph df
RAW STORAGE:
    CLASS           SIZE            AVAIL           USED         RAW USED          %RAW USED 
    <CLASS>      <TOTAL_SIZE>      <AVAILABLE_SPACE> <USED_SPACE> <RAW_USED_SPACE> <%_RAW_USED_SPACE> 
    TOTAL     
 
POOLS:
    POOL                         ID        STORED           OBJECTS       USED            %USED         MAX AVAIL 
 <POOL_NAME>                 <POOL_ID>  <BYTES_STORED>   <NUM_OBJECTS> <USED_SPACE>  <%_USED_SPACE>   <AVAILABLE_SPACE> 

....

Another utility to check the OSDs utilization is the `ceph osd df`
command which shows the use of each OSD within a RHCS cluster. The
following is the output of this command:

....
# ceph osd df
   ID       CLASS    WEIGHT  REWEIGHT SIZE    RAW USE DATA    OMAP   META     AVAIL   %USE VAR  PGS STATUS 
                    TOTAL 
....

Both commands shown above will help in order to handle the `nearfull`
and `full` OSDs scenarios as it explained below.

=== Troubleshooting a `Nearfull` OSDs scenario

When a RHCS cluster reaches the capacity set by the
`mon_osd_nearfull_ratio` parameter the cluster will be in
`HEALTH_WARN # nearfull osds` state. The main causes of this situation
are as follows:

* The OSDs are not balanced among the OSD nodes in the cluster. It could
be due to unequal number of OSD daemons in some OSD nodes, or the weight
of some OSDs in the CRUSH map is not adequate to ther capacity.
* The Placement Group count is not proper as per the number of the OSDs,
use case, target PGs per OSD, and OSD utilization.
* The cluster uses inappropiate CRUSH tunables.
* The back-end storage for OSDs is almost full.

In order to solve this situation please execute the following steps in
order:

. Verify that the PG count is sufficient and increase it if needed.

With the help of pg-autoscaling recommendations it is easy to check if
the PG count is sufficient. By default in RHCS 4, the pg-autoscaling is
in `warn` mode so it will not automatically adjust the PG count, it will
just recommend the expected PG count of each pool.

....
# podman exec ceph-mon-$(hostname -s) ceph osd pool autoscale-status
POOL                       SIZE TARGET SIZE RATE RAW CAPACITY  RATIO TARGET RATIO EFFECTIVE RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE
....

Before making any changes related to the PG count, please be sure the
`TARGET RATIO` of each pool is set accordinly to your use cases.

Then ensure the column `NEW_PG_NUM` is empty. Otherwise, adjust each
pool with the `NEW_PG_NUM` recommended value:

....
# podman exec ceph-mon-$(hostname -s) ceph osd pool set {pool-name} pg_num [pg_num]
# podman exec ceph-mon-$(hostname -s) ceph osd pool set {pool-name} pgp_num [pgp_num]
....

If the RHCS cluster keeps in `HEALTH_WARN # nearfull osds` state execute
the next step.

[start=2]
. Verify that the RHCS cluster uses CRUSH tunables optimal for its
version.

From a MON node:

* Extract and decompile the CRUSH map from the RHCS cluster:

....
crushmapfile=$(date +%Y%m%d)
# podman exec ceph-mon-$(hostname -s) ceph osd getcrushmap -o /etc/ceph/${crushmapfile}
# podman exec ceph-mon-$(hostname -s) crushtool -d /etc/ceph/${crushmapfile} -o /etc/ceph/${crushmapfile}.txt
....

* Open the CRUSH map and look for the tunable values. They are set at
the begining of the CRUSH map, under the `# begin crush map` comment.

For RHCS 4 the optimal tunables are as follows:

....
tunable choose_local_tries 0
tunable choose_local_fallback_tries 0
tunable choose_total_tries 50
tunable chooseleaf_descend_once 1
tunable chooseleaf_vary_r 1
tunable chooseleaf_stable 1
tunable straw_calc_version 1
....

If the tunables shown above differ from the values set in the RHCS
cluster then they need to be set to `optimal`.

* Setting the tunables to the `optimal` profile.

From a MON node:

....
# podman exec ceph-mon-$(hostname -s) ceph osd crush tunables optimal
....

*NOTE*: Changing the CRUSH tunables will result in data movement.

Please for further details check the
https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html-single/storage_strategies_guide/#crush_tunables[CRUSH
tunables] section in the Storage Stategies guide for RHCS 4.
Furthermore, you could test the impact of tunable modifications into the
RHCS cluster following this
https://access.redhat.com/solutions/2159151[article] on the Red Hat
Customer Portal.

If the RHCS cluster keeps in `HEALTH_WARN # nearfull osds` state
continue executing the next step.

[start=3]
. Change the weight of OSDs by utilization.

A RHCS cluster may become imbalanced even though CRUSH is an algorithm
continuosly looking for a uniform probability distribution for write
requests. In this situation OSDs can be reweight by utilization by
executing the `reweight-by-utilization` command.

However, this step can be skipped since RHCS 4 includes the *Ceph
Manager balancer module* which optimizes the placement of PGs across
OSDs in order to achieve a balanced distribution. Enabling this module
is covered in the next step, but in order to have a complete procedure,
below you will find how to reweight OSDs by utilization manually.

From a MON node:

* Test the reweight operation to determine which and how many PGs and
OSDs will be affected:

....
# podman exec ceph-mon-$(hostname -s) test-reweight-by-utilization [threshold] [weight_change_amount] [number_of_OSDs] --no-increasing

....

*NOTE:* Limiting the number of OSDs to reweight prevents significant
rebalancing.

Let's review the optional parameters this command allows in the
following table:

[cols=",,,",options="header",]
|===
|Parameter |Description |Default Value |Valid Values
|`threshold` |It is a percentage of OSD utilization |120 |> 100

|`weight_change_amount` |It is the amount to change the weight |0.05
|0.0 - 1.0

|`number_of_OSDs` |The maximum number of OSDs to reweight |n/a |n/a
|===

* Reweight OSDs by utilization:

....
# podman exec ceph-mon-$(hostname -s) reweight-by-utilization [threshold] [weight_change_amount] [number_of_OSDs] --no-increasing
....

This will generate data movement. But after the rebalance if the RHCS
cluster keeps in `HEALTH_WARN # nearfull osds` state continue executing
the next step.

[start=4]
. Enable the *Ceph Manager balancer module*:

The *Ceph Manager balancer module* is the recommended approach to
achieve a balanced distribution instead of using the
`reweight-by-utilization` command. This module optimizes the placement
of PGs accross OSDs.

From a MON node:

....
# podman exec ceph-mon-$(hostname -s) ceph mgr module enable balancer
# podman exec ceph-mon-$(hostname -s) ceph balancer on

....

This will generate data movement into the RHCS cluster. Wait until it
finishes but if the RHCS cluster keeps in `HEALTH_WARN # nearfull osds`
state then either scale the cluster adding a new OSD node or delete
unnecessary data.

Scaling the cluster by adding a new OSD node is out of the scope of this
document. Please refer to the
https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html/operations_guide[RHCS
Operations Guide] which provides a step by step procedure about adding a
new OSD node to the cluster.

=== Troubleshooting a `Full` OSDs scenario

When a RHCS cluster reaches the capacity set by the `mon_osd_full_ratio`
parameter the cluster will be in `HEALTH_ERR # full osds` state to
prevent clients from performing I/O operations and to avoid losing data.

To handle this situation Red Hat recommends to execute the following
steps:

. Check the `%RAW USED` through `ceph df` command.

If the `%RAW USED` value is above 70% there are two options:

* Scale the cluster by adding a new OSD node. This is the best way to
deal with this situation since it is a long-term solution. This is the
option recommended by Red Hat.
* Delete unnecessary data. This is a short-term solution just to avoid
production downtime.

Scaling the cluster by adding a new OSD node is out of the scope of this
document. Please refer to the
https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html/operations_guide[RHCS
Operations Guide] which provides a step by step procedure about adding a
new OSD node to the cluster.

This procedure will cover the removal of the unnecessary data.

[start=2]
. Check the current value of `full_ratio` (0.95 by default) from a MON
node:

....
# podman exec ceph-mon-$(hostname -s) ceph osd dump | grep -i ^full_ratio
full_ratio 0.95
....

[start=3]
. Increase the value of `full_ratio` to *0.97*:

....
# podman exec ceph-mon-$(hostname -s) ceph osd set-full-ratio 0.97
osd set-full-ratio 0.97
....

*NOTE*: Setting `set-full-ratio` higher than 0.97 makes the recovery
process extremelly difficult and the RHCS cluster might not be able to
recover full OSDs at all.

[start=4]
. Verify the parameter was successfully set to *0.97*:

....
# podman exec ceph-mon-$(hostname -s) ceph osd dump | grep -i ^full_ratio
full_ratio 0.97
....

[start=5]
. Monitor the RHCS cluster:

....
# watch podman exec ceph-mon-$(hostname -s) ceph -s
....

As soon as the cluster changes its state from `full` to `nearfull`,
please proceed deleting any unnecessary data.

After deleting any unnecessary data, check the `%RAW_USED` through
`ceph df` command. Also check the cluster status through `ceph -s`. If
the RHCS cluster state is `HEALTH OK` and the `%RAW_USED` has an
acceptable value then carry on rolling back the `full_ratio` parameter.

[start=6]
. Rollback `full_ratio` parameter to its default value (0.95):

....
# podman exec ceph-mon-$(hostname -s) ceph osd set-full-ratio 0.95
osd set-full-ratio 0.95
....

[start=7]
. Verify the parameter was successfully set to *0.95*:

....
# podman exec ceph-mon-$(hostname -s) ceph osd dump | grep -i ^full_ratio
full_ratio 0.95
....
